% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
  \usepackage{amssymb}
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Quantitative Methods in Plant Breeding: The Digital Tutorials.},
  pdfauthor={NIAB 2021},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage[bf,singlelinecheck=off]{caption}

\usepackage{Alegreya}
\usepackage[scale=.7]{sourcecodepro}

\usepackage{framed,color}
\definecolor{shadecolor}{RGB}{248,248,248}

\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.8}
\renewcommand{\bottomfraction}{0.8}
\renewcommand{\floatpagefraction}{0.75}

\renewenvironment{quote}{\begin{VF}}{\end{VF}}
\let\oldhref\href
\renewcommand{\href}[2]{#2\footnote{\url{#1}}}

\ifxetex
  \usepackage{letltxmacro}
  \setlength{\XeTeXLinkMargin}{1pt}
  \LetLtxMacro\SavedIncludeGraphics\includegraphics
  \def\includegraphics#1#{% #1 catches optional stuff (star/opt. arg.)
    \IncludeGraphicsAux{#1}%
  }%
  \newcommand*{\IncludeGraphicsAux}[2]{%
    \XeTeXLinkBox{%
      \SavedIncludeGraphics#1{#2}%
    }%
  }%
\fi

\makeatletter
\newenvironment{kframe}{%
\medskip{}
\setlength{\fboxsep}{.8em}
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\makeatletter
\@ifundefined{Shaded}{
}{\renewenvironment{Shaded}{\begin{kframe}}{\end{kframe}}}
\makeatother

\newenvironment{rmdblock}[1]
  {
  \begin{itemize}
  \renewcommand{\labelitemi}{
    \raisebox{-.7\height}[0pt][0pt]{
      {\setkeys{Gin}{width=3em,keepaspectratio}\includegraphics{images/#1}}
    }
  }
  \setlength{\fboxsep}{1em}
  \begin{kframe}
  \item
  }
  {
  \end{kframe}
  \end{itemize}
  }
\newenvironment{rmdnote}
  {\begin{rmdblock}{note}}
  {\end{rmdblock}}
\newenvironment{rmdquiz}
  {\begin{rmdblock}{quiz}}
  {\end{rmdblock}}
\newenvironment{rmdcaution}
  {\begin{rmdblock}{caution}}
  {\end{rmdblock}}
\newenvironment{rmdimportant}
  {\begin{rmdblock}{important}}
  {\end{rmdblock}}
\newenvironment{rmdtip}
  {\begin{rmdblock}{tip}}
  {\end{rmdblock}}
\newenvironment{rmdwarning}
  {\begin{rmdblock}{warning}}
  {\end{rmdblock}}

\usepackage{makeidx}
\makeindex

\usepackage{longtable}

\urlstyle{tt}

\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother


\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{Quantitative Methods in Plant Breeding: The Digital Tutorials.}
\author{NIAB 2021}
\date{2021-02-27}

\begin{document}
\maketitle

%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage
\thispagestyle{empty}
\begin{center}
\includegraphics{images/dedication.pdf}
\end{center}

\setlength{\abovedisplayskip}{-5pt}
\setlength{\abovedisplayshortskip}{-5pt}

{
\setcounter{tocdepth}{2}
\tableofcontents
}
\hypertarget{Introduction}{%
\chapter{Introduction}\label{Introduction}}

\includegraphics{images/cover.jpg}
Welcome to the online tutorial booklet written for the QMPB course at NIAB. The book has been written using \emph{markdown} and formed using a R package called \textbf{Bookdown}. You will be prompted to a chapter/section in this book for each tutorial.

This is our first year using digital tutorials, while we happy with the end product, if you spot or experience any \emph{teething issues} please let us know and we'll work to resolve the problem.

For each tutorial, you will typically need the corresponding dataset which will enable you to work alongside the online tutorials on your own personal computers. To download the datasets, please follow this link:
\url{https://tallywright.github.io/jekyll/update/2021/01/05/QMPB-data-repository.html}

Via the link, download the .zip folder containing the data files. The folder should be organised within chapters and each tutorial will prompt you towards which dataset to use and also how to use it.

Please don't be afraid to ask for help where needed throughout these tutorials. We're here to help!

\hypertarget{Acknow}{%
\chapter{Acknowledgements and Contents}\label{Acknow}}

\hypertarget{acknowledgements}{%
\section{Acknowledgements}\label{acknowledgements}}

This is the 13th time NIAB has run its Quantitative Methods in Plant Breeding course. The course was devised and created at NIAB by Professor Ian Mackay (now of Implant Consultancy Ltd.~and SRUC, \href{mailto:i.j.mackay@gmail.com}{\nolinkurl{i.j.mackay@gmail.com}}) from discussions with Mike Kearsey, Noel Ellis, Wayne Powell and Andy Greenland about the need for training for plant breeders and what and how NIAB could contribute. Ian also wrote this course manual.

This is the 7th time in which I have taught on the NIAB QMPB course. Like so many participants over the years, Ian has stimulated my thinking on all aspects of plant breeding and I have learnt a great deal from him. I hope I can continue to add to the wealth of knowledge and practical learning that he has developed in the course.

Over the years, the course has also benefitted from the input of several guest lecturers, as well as assistance in lecturing and demonstrating from colleagues in the John Bingham Laboratory at NIAB. In particular, we would like to recognise the enthusiastic contributions of our former NIAB colleague, Greg Mellers, who sadly passed away in late 2019.

\includegraphics{images/Greg.png}

All the course administration has been handled by NIAB's Mary MacPhee. I would also like to thank Tally Wright, Nick Fradgley, Camila Zanella, Joachim Nwezeobi and Yeorgia Argirou for their roles in preparing and teaching the course and Joseph Amosu for managing the IT support.

We would like to thank the authors of the \texttt{bookdown} package \citep{R-bookdown} which has been used to form this online book, which was built with R Markdown and \texttt{knitr} \citep{xie2015}.

Please feel free to contact me with any questions: \href{mailto:keith.gardner@niab.com}{\nolinkurl{keith.gardner@niab.com}}. For more details about Genetics and Breeding at NIAB please see: \url{https://www.niab.com/research/agricultural-crop-research-0}

\emph{Keith Gardner}

NIAB

\hypertarget{digital-tutorial-chapters}{%
\section{Digital Tutorial Chapters:}\label{digital-tutorial-chapters}}

\begin{itemize}
\item
  Chapter \ref{Introduction}: Introduction
\item
  Chapter \ref{Acknow}: Acknowledgements and Contents
\item
  Chapter \ref{R-and-stats}: R and Rstudio: a general introduction with statistics.
\item
  Chapter \ref{Trial-design}: Trial design
\item
  Chapter \ref{Trial-analysis}: Trial analysis
\item
  Chapter \ref{Cross-site-analysis}: Cross site analysis
\item
  Chapter \ref{Population-genetics}: Population genetics
\item
  Chapter \ref{Imputation}: Imputation
\item
  Chapter \ref{Association-mapping}: Association Mapping.
\item
  Chapter \ref{Mapping-Markers}: Mapping Markers and Quantitative Trait Loci.
\end{itemize}

\hypertarget{R-and-stats}{%
\chapter{R and Rstudio: a general introduction with statistics.}\label{R-and-stats}}

\hypertarget{introduction-section-on-r}{%
\section{Introduction section on R}\label{introduction-section-on-r}}

R is free software. It can be used as an overpowered calculator, to carry out many standard statistical analyses, and to develop applications carrying out complex analyses in specific subject areas. Many of these applications and much other useful information are freely available from the Comprehensive R Archive network (CRAN) \href{http://cran.r-project.org}{website}. In particular, the guide ``An Introduction to R'' is available under the documentation section and can be used to supplement the outline given here. Another excellent source is ``Introductory Statistics with R'' by Peter Dalgaard.

R is just one among several excellent statistical packages. Its major strength is that it is free and this has led to an ever expanding user base and to the development of more and better applications. R is commonly used in many commercial and academic establishments: knowledge of R is a useful and transferable skill to acquire.

For many users, the major disadvantage of R is that commands need to be typed at a prompt, rather than selected from a menu. However, most commands are short, and as we shall see, much repetitive typing can be avoided. Over the last few years, RStudio, an interface to R, has become so popular that it is now the standard way by which users are introduced to the package. RStudio retains the look and feel of R: it still requires commands to be typed but is more accessible. In this tutorial, we shall use Rstudio, but if you can use R you can use Rstudio, and vice-versa.

The intention of this guide is twofold. Firstly, it is to help the beginner to get started. It will show you how to get data into RStudio, carry out basic analyses, produce graphs, and save results. It is hoped that this will allow users to dip directly into more complete guides as required. This guide is a long way off being thorough and many of the definitions provided here are not perfectly precise. The hope is that this guide will allow you to rapidly carry out useful analyses of importance in plant breeding and provide a base from which additional knowledge can be acquired without too much extra effort.

Secondly, the guide is intended to act as revision of some basic statistical methods. For this purpose we shall be studying some properties of a panel of European winter wheat lines; the TriteceaeGenome panel which was collected as part of a European collaborative project in association mapping, the results of which have been described by \citep{Bentley2014}. A copy of the paper has been supplied in the references folder on the course \href{https://tallywright.github.io/}{website}. We shall use this dataset for various analyses throughout the course.

The subset of the TG panel data which we shall use today are in ``TG data for day 1.xlsx''. Open the file and have a look and I shall describe the data. We can discuss what sorts of hypotheses we would be interested in testing with these data.

\hypertarget{getting-the-data-ready}{%
\subsection{Getting the data ready}\label{getting-the-data-ready}}

The first step of this tutorial will be to open this excel sheet (in excel) and save a copy of it as a comma-separated values (CSV) file. This can be completed in excel via the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Opening the file called: ``TG data for day 1.xlsx''
\item
  Going to `save as' and then changing the file type to: CSV (Comma delimited) (.csv).
\item
  Save this file in the same folder as the ``.xlsx'' file. You can keep the same name, as you are creating a different file type. You can only save a single excel sheet as a CSV file and that file won't contain any formulas or formatting, only values. Now that file is converted to a CSV file, we can move on to using RStudio and come back to it later.
\end{enumerate}

\begin{rmdcaution}
You will need to complete the above section before moving on to the rest of the tutorial.
\end{rmdcaution}

\hypertarget{the-rstudio-layout-workspace-and-environment.}{%
\section{The Rstudio layout, workspace and environment.}\label{the-rstudio-layout-workspace-and-environment.}}

Next open Rstudio which should be installed on your computer. Typically Rstudio will open with three windows (possibly four if your script from your last session opens automatically).

If you only have three windows, open a new script by pressing: \emph{Ctl + Shift + N}

You should now see four screens:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  A console or command window. This is on the bottom left in the screen-shot. In this window we type commands and the result of those commands is listed. If you were to work in R, rather than R studio, this is the only window you would see.
\item
  A script editing window: for writing scripts. This is shown in the top left. Commands can also be run from here.
\item
  An environment / workspace and history window (top right). Within the environment window you will see user defined objects (data frames, lists, vectors ect.) This workspace can be saved for later use.
\item
  A window for files, plots, packages and help (bottom right).
\end{enumerate}

You can re-size these windows, move about between them and essentially do everything that is possible in an ordinary R session, including developing your own programs and scripts. The idea is that this is easier and more convenient than working in R directly.

\begin{figure}
\centering
\includegraphics{images/Structure_of_Rstudio.png}
\caption{The structure if RStudio}
\end{figure}

\hypertarget{working-directory}{%
\subsection{Working directory}\label{working-directory}}

The current working directory can be shown by the \texttt{getwd()} function. This is ultimately the folder on your computer that your current R session is working from. You can type this in the script window and then press \textbf{Ctrl + Return} to run, the cursor needs to be on the line you want to run. In the working directory, files will be loaded when called automatically and files will be automatically saved to this folder. You can change your working directory manually using the function \texttt{setwd()}. Use the folder path to prompt where the working directory is set. However, sometimes it's easiest just to click on the tab at the top of the window and use the route of the drop-down menu: \emph{Session / Set Working Directory / Choose Directory}. Which works just as well.

\begin{rmdquiz}
\textbf{1}: You want to be working from a folder where the data you will be using is saved in. You could create a new folder for this tutorial and then add the CSV file ``TG data for day 1.csv'' that you created earlier. You can do this outside of RStudio.

Next, with RStudio, change the working directory to the folder containing the CSV file ``TG data for day 1.csv'' that you created earlier. Then you can run the line \texttt{list.files()} in the script window to ensure that the file is present in your current working directory.
\end{rmdquiz}

\hypertarget{basic-r-syntx}{%
\section{Basic R Syntx}\label{basic-r-syntx}}

Write your commands into the \emph{script editing window}, to then run a command you can leave the cursor on the line you wish to run and press \emph{Ctrl + Return}. You can also press \emph{Run} at the top right of the window. If you want to run multiple lines at once, highlight the desired lines and click \emph{Run} or press \emph{Ctrl + Return}.

When you run the line in the \emph{script editing window} you will see your command appear in the console and the result of what you have run.

For example entering 9+1 will return:

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{9}\SpecialCharTok{+}\DecValTok{1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 10
\end{verbatim}

You are now fully trained to use R as a calculator! The syntax is essentially the same as in Excel. Thus \texttt{\^{}2} will square numbers, \texttt{sqrt()} will take the square root and so on. For example:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sqrt}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.414214
\end{verbatim}

Returns the square root of 2. The \texttt{{[}1{]}} is just a line number.

Note that R is case sensitive, running \texttt{LOG10(2)}. Will produce an error.

Whereas:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{log10}\NormalTok{((}\DecValTok{9}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2
\end{verbatim}

Does not produce any errors.

If you make a mistake, you can easily edit the line that contains the error in the \emph{script editing window}. Some of the new versions of Rstudio will flag up mistakes in a line in the \emph{script editing window}.

Alternatively if you are working directly in the \emph{console or command window} and you make a mistake, it can be corrected by tapping the up arrow on the keyboard to bring back the last typed command, then moving along the command line using the left and right arrows, deleting characters using the backspace or delete key, inserting correct numbers or text and hitting the carriage return key. Multiple hits of the up arrow will bring back successively earlier commands. However, an easier way of finding and re-entering commands used previously is to click on the ``history'' tab at the top right. This shows the previous commands you have run. If you select one and press \emph{Ret}, then that command will be passed to the console window. Press \emph{Ret} again and the command will be executed. It is even possible to select several non-contiguous commands in the history window, pass them to the console and execute them all as group.

To avoid this slightly laborious approach, just write and run your scripts through the \emph{script editing window}. This way you will have a complete record of your code preserved.

\begin{rmdnote}
Note also that it is possible to cut from windows outside RStudio and paste into it. So R commands listed in another document, for example this one, can be copied and pasted into the \emph{console or command window} or the \emph{script editing window} and will run (after hitting carriage return). You will find this useful during the course.
\end{rmdnote}

\hypertarget{brackets}{%
\subsection{Brackets}\label{brackets}}

We have seen brackets used above in simple formulae - just as in Excel formula. However, brackets are used more extensively in R: all commands include brackets. For example:

Running \texttt{quit()} will end your R session. However:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{quit}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## function (save = "default", status = 0, runLast = TRUE) 
## .Internal(quit(save, status, runLast))
## <bytecode: 0x000000001560ff08>
## <environment: namespace:base>
\end{verbatim}

outputs the above. Note how the use of the brackets changes the outcome. The above is the section of computer code which is run when you type \texttt{quit()}. Unless you are an enthusiast, it is not necessary to know what this means.

\hypertarget{equals-symbols}{%
\subsection{Equals symbols}\label{equals-symbols}}

R has four sets of symbols all of which loosely mean ``equals'' but which have differences in use. This can take some getting used to.

The most commonly used set is

\texttt{\textless{}-}

or sometimes

\texttt{-\textgreater{}}

These symbols are typed using the dash (the minus sign) and the left or right arrow. They are used to assign results from one side of the arrow to the other. For example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_first\_result }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{log10}\NormalTok{((}\DecValTok{9}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Won't return a result and just returns the prompt in the command window. But then typing:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_first\_result}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.414214
\end{verbatim}

Returns the desired result. We have created a variable my\_first\_result and assigned the result of our calculation to it, using \texttt{\textless{}-}. The same result would be achieved by: \texttt{sqrt(log10((9+1)\^{}2))-\textgreater{}\ my\_first\_result}

The entry stored in \texttt{my\_first\_result} is now available for additional manipulation. For example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_first\_result}\SpecialCharTok{\^{}}\DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2
\end{verbatim}

There are historical reasons why \texttt{\textless{}-} was introduced for this purpose, but R also allows the use of ``='' in addition to \texttt{\textless{}-} in assigning values to variables. \texttt{x\textless{}-3} and \texttt{x=3} both return the value of 3 if you run \texttt{x}. Note however that \texttt{2=x} will return an error. Because of its lack of ambiguity with the other use of \texttt{=} (see below), I prefer to stick to \texttt{\textless{}-} and \texttt{-\textgreater{}} in assigning values, though this can seem artificial and unnecessarily geeky at first.

The second equals symbol is the tilde \texttt{\textasciitilde{}} (above the \# on my keyboard). This is used in R in statistical analysis to distinguish between x and y variables; that is to say between what is being analysed (e.g.~the phenotype) and what factors and variates it is being analysed with (eg marker data and environmental factors or variates). So for example:

\texttt{P\textasciitilde{}G+E}

is R syntax to state that a variate P is to be explained by two variates or factors G and E. If you try this in R now nothing will happen. The exact context in which this syntax is used will be given later. For now, note that

\texttt{P\textasciitilde{}G}

would represents linear regression of P on G Interactions can also be included:

\texttt{P\textasciitilde{}G+E+G:E}

includes an interaction term between G and E (represented by G:E). This can be abbreviated to:

\texttt{P\textasciitilde{}G*E}

Yet more complex models, for example with nested factors, can be described by including bracketed terms.

The third equals symbol is \texttt{=}. This is most often used within commands to provide information about specific parameters. For example we shall come across:

\texttt{xlab=\ "what\ you\ put\ here\ is\ used\ to\ label\ the\ x\ axis\ in\ a\ graph"}

used in commands to generate graphs.

The final equals description is \texttt{==}. This truly means ``is equal to'', and is used to test relationships: is A equal to B is written as

\texttt{A==B}

Try this sequence of commands

\texttt{A\textless{}-2}

\texttt{B\textless{}-3}

\texttt{A==B}

Related to == are:

\texttt{!=} not equal to

\texttt{\textgreater{}} greater than

\texttt{\textless{}} less than

\texttt{\textgreater{}=} greater than or equal to

\texttt{\textless{}=} less than or equal to.

These, together with \texttt{==} itself, are entered here for completeness. Try \texttt{A!=B}.

\hypertarget{continuation-character.}{%
\subsection{Continuation character.}\label{continuation-character.}}

When the \texttt{+} symbol is shown rather than the \texttt{\textgreater{}} symbol in the console window, it means that your previous command was `syntactically incomplete'. If an R command is incomplete when the carriage return is depressed, you are prompted with a \texttt{+} to continue the command on the next line. Typing \texttt{sqrt(log10} returns: \texttt{+}.

The rest of the command can be entered after the \texttt{+}:

\texttt{sqrt(log10}

\texttt{+\ ((9+1)\^{}2))}

\texttt{{[}1{]}\ 1.414214}

\begin{rmdnote}
You might find it hard to generate these errors in Rstudio, as it helpfully inserts a \texttt{)} as soon as you type a \texttt{(}.
\end{rmdnote}

\hypertarget{text-and-numbers}{%
\subsection{Text and numbers}\label{text-and-numbers}}

Text is distinguished from numbers and from R commands by the use of quotes. Either single quotes `text' or double quotes ``text'' will do, but the quotes must match. Again, RStudio will help you with this.

\hypertarget{summary-of-syntax}{%
\subsection{Summary of syntax}\label{summary-of-syntax}}

Arithmetic: \texttt{+\ -\ /\ *\ \^{}\ (\ )} just like Excel

Commands use brackets: \texttt{quit()} is correct. \texttt{quit} is wrong.

Equals:

\begin{itemize}
\tightlist
\item
  \texttt{\textless{}-} stores a result
\item
  \texttt{\textasciitilde{}} is described by
\item
  \texttt{=} options in commands
\item
  \texttt{==} logical equivalence
\end{itemize}

For text use quotes: \texttt{"hello"} or \texttt{\textquotesingle{}hello\textquotesingle{}}.

This introduction should provide sufficient syntax to get you going. Additional syntax is introduced, as required, in the discussion of specific commands and operations in the remainder of this document.

\begin{rmdquiz}
\textbf{2} Assign the result of \texttt{(9.75\ *\ 80)\ -\ 34} to an object called: \texttt{my\_second\_result}. Then print out \texttt{my\_second\_result}. Lastly divide \texttt{my\_second\_result} by 2.

Assign the word \texttt{harvester} to an object called: \texttt{Combine}. Then run \texttt{Combine}.
\end{rmdquiz}

\hypertarget{reading-in-data}{%
\section{Reading in data}\label{reading-in-data}}

There are many ways of entering the data you wish to analyse. For example, R can read data from the clipboard, so you can copy data directly from Excel. It will read tab separated files (files in which different columns of data are separated by tabs, and it will read comma separated files (csv files). There are more advanced methods to interface directly with databases, to capture data from the web, and so on. There are also several different formats for reading data in.

Earlier we changed our working directory to the location of our TriticeaeGenome data files, which we saved as a CSV file. If you missed this, do it now:

From the top of the Rstudio window select:

\emph{Session / Set Working Directory / Choose Directory}

Then browse to locate your directory (choose the folder where you saved the data as a CSV file). When we exit RStudio, your work will be saved here (as long as you remember to actually save it), making it easier to return later and pick up where you left of. We shall explain this later.

If you follow the above process, in the Console window, you will see echoed the command that you could have typed to change directories. For me, this was:

\texttt{setwd("C:/Users/x991625/OneDrive\ -\ NIAB/a).\ NIAB\ WORK/QMPB\ 2021/R\_bookdown\_local/bookdown-demo-master")}

\begin{rmdtip}
If you copy this line to the top of your \emph{script editing window} and if you had to re-run the script this line would set the working directory for you. It can also serve as a reminder on where your files are saved. There may be times when you wish to change directories as part of a larger sequence of commands that you want to run, in which case you would use the typed form. Note - A sequence of commands is commonly called a ``script'': in essence it is a small computer program.
\end{rmdtip}

We may not cover Rprojects over these days but an alternative available within RStudio, but not R, is to create a project. From the menu at the top, select:

\emph{File / New Project / Existing Directory /}

Then browse to the directory you want to work in and \emph{Create project}

Later, when you quit RStudio, you will find a file with the extension .Rproj has been created in the selected directory. If you click on that, then RStudio will open with all your previous work saved.

Now we have our working directory set we can read in the data. Personally I always think it's safest to read in the data as a comma-separated values (CSV) file. Earlier I asked you to save the data as a CSV file and that file should be in our working directory. We can check what files are in our working directory by running:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{list.files}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "_book"                                              
##  [2] "_bookdown.yml"                                      
##  [3] "_bookdown_files"                                    
##  [4] "_build.sh"                                          
##  [5] "_deploy.sh"                                         
##  [6] "_output.yml"                                        
##  [7] "_render.R"                                          
##  [8] "02-Acknow.Rmd"                                      
##  [9] "03-R_and_stats.Rmd"                                 
## [10] "04-Trial_design.Rmd"                                
## [11] "05-Trial_analysis.Rmd"                              
## [12] "06-Cross_site_analysis.Rmd"                         
## [13] "07-Population_genetics.Rmd"                         
## [14] "08-Imputation.Rmd"                                  
## [15] "09-Assocation_mapping.Rmd"                          
## [16] "10-Mapping_Markers_and_Quantitative_Trait_Loci.Rmd" 
## [17] "11-Data_visualization.Rmd"                          
## [18] "12-Selection_theory_and_breeding_program_design.Rmd"
## [19] "13-Genomic_prediction.Rmd"                          
## [20] "14-Breeding_simulations.Rmd"                        
## [21] "15-references.Rmd"                                  
## [22] "aug_blocks.csv"                                     
## [23] "AxCmapped.csv"                                      
## [24] "book.bib"                                           
## [25] "bookdown-demo.aux"                                  
## [26] "bookdown-demo.bbl"                                  
## [27] "bookdown-demo.blg"                                  
## [28] "bookdown-demo.idx"                                  
## [29] "bookdown-demo.ilg"                                  
## [30] "bookdown-demo.ind"                                  
## [31] "bookdown-demo.log"                                  
## [32] "bookdown-demo.out"                                  
## [33] "bookdown-demo.pdf"                                  
## [34] "bookdown-demo.Rmd"                                  
## [35] "bookdown-demo.Rproj"                                
## [36] "bookdown-demo.tex"                                  
## [37] "bookdown-demo.toc"                                  
## [38] "bookdown-demo_files"                                
## [39] "cover.jpg"                                          
## [40] "data"                                               
## [41] "delete.csv"                                         
## [42] "DESCRIPTION"                                        
## [43] "Dockerfile"                                         
## [44] "images"                                             
## [45] "index.Rmd"                                          
## [46] "journals.bib"                                       
## [47] "latex"                                              
## [48] "lattice_25.csv"                                     
## [49] "lattice_9.csv"                                      
## [50] "LICENSE"                                            
## [51] "now.json"                                           
## [52] "packages.bib"                                       
## [53] "preamble.tex"                                       
## [54] "README.md"                                          
## [55] "style.css"                                          
## [56] "tables"                                             
## [57] "TG data for day 1.csv"                              
## [58] "this will be the file name.csv"                     
## [59] "toc.css"
\end{verbatim}

In my example there are a number of files in my working directory. That's because my working directory contains all the files used to build this book. We are interested in reading in the file: \texttt{TG\ data\ for\ day\ 1.csv}. To read the data in we would use a command like:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TG\_data\_for\_day\_1 }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"TG data for day 1.csv"}\NormalTok{,}\AttributeTok{header=}\NormalTok{T)}
\end{Highlighting}
\end{Shaded}

Looks complicated? It's not really. We are only using the function \texttt{read.csv} which takes the the name of the csv file you want to read in as the first argument. Note this name has to be in speech marks and requires the .csv at the end. The second argument is \texttt{header=T}, this ensures that the top row in the excel file is read in as the column headers in R. The other part of the command was to assign the data to an R object called \texttt{TG\_data\_for\_day\_1}.

A key point is to consider how any program you are using handles \texttt{NA} (missing data). For R to treat excel cells with no data as \texttt{NA} those cells should contain \texttt{NA} and not be left blank. This was done for you with this data. However, that won't always be the case.

It's always good to make sure that the data was read in correctly and there are no obvious issues. This can be done with some simple functions. I always like to make sure the headers (column headers were read in correctly):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(TG\_data\_for\_day\_1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "variety"     "year"        "ORIGIN"      "Rht2"        "PpdD1"      
##  [6] "yield"       "CALLOW_2011" "FRANCE_2010" "FRANCE_2011" "LGE_2010"   
## [11] "LGE_2011"    "NIAB_2011"   "FT"          "HT"          "AWNS"
\end{verbatim}

Looks good, we have 15 distinct headers. One of the many strengths of R is the help documentation. Throughout this tutorial and your own research, if you come across a function you are unsure about, or want to know what arguments the function uses or want some examples of using the function, you can just enter the function name into \textbf{help()}. For example try running:

\texttt{help("names")}

This opens a help window in the Rstudio pane on your right, where you should see all the required information.

We should also check the dimensions of the data, which can often indicate if there's an issue. From the excel sheet we have 377 rows and 15 columns. We check the dimensions of our object in R like this:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dim}\NormalTok{(TG\_data\_for\_day\_1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 376  15
\end{verbatim}

Rows are shown first, followed by columns.

\begin{rmdcaution}
The rows are one less than expected. Why? Because we read in the first row as a column headers using \texttt{headers=T} in the \texttt{read.csv()} function.
\end{rmdcaution}

Additionally, you should also see the data in the \emph{Environment Window} in the top right pane of RStudio. You can see the number of rows (obs.) and columns (variables) there too. You can click on this (or run \texttt{View(TG\_data\_for\_day\_1)}) to view the data displayed in a spreadsheet format in the editing window. You can apply filters to the various columns to see subsets of the data if desired. Don't do this if you are working with large datasets! It will freeze RStudio or takes a very long time to load. R is not designed for viewing large data files in this way.

We now have our data ready and waiting for analysis. To display a variable (a column), say yield, we should type \texttt{TG\_data\_for\_day\_1\$yield}. However, this would print out 376 numbers and to save paper, we can only request the first five values using \texttt{{[}1:5{]}} after our command, like so:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TG\_data\_for\_day\_1}\SpecialCharTok{$}\NormalTok{yield [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 103.23  88.55  90.76  82.24  96.66
\end{verbatim}

Notice the use of the \texttt{\$} to access a variable from your data frame.

Writing out the name of the data object each time is tedious. To speed things up, we can \texttt{attach} our data:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{attach}\NormalTok{(TG\_data\_for\_day\_1)}
\end{Highlighting}
\end{Shaded}

R will now assume that when you type yield, you mean \texttt{TG\_data\_for\_day\_1\$yield}.

\begin{rmdwarning}
This can sometimes be dangerous: if you have another dataset you are working with at the same time which also has a variable called yield, then it can be confusing to know which you are working with.
\end{rmdwarning}

To stop working by default with the TG\_data\_for\_day\_1 dataset you could type: \texttt{detach(TG\_data\_for\_day\_1)}. If you do this now, don't forget to attach it again!

We could had read our data in using \texttt{read.table()}, this is how we would read in data saved as a .txt file. We will use this function throughout the course. It is very similar to \texttt{read.csv}. Use the help documentation in R, to the read further information on the \texttt{read.table()} command, i.e.~run: \texttt{help(read.table)}.

You can also use the \texttt{read.table()} function to read from your clipboard. This is achieved by copying cells in excel (your data table) and then running:

\texttt{read.table(file\ =\ "clipboard",\ header=TRUE)}

Note that this is not a reproducible method and can lead to some downstream issues if you forget where your data came from.

\begin{rmdquiz}
\textbf{3} You can also read data from a .txt file using the function \texttt{read.table()}. The function will take the same arguments as \texttt{read.csv()}. Save the data in excel as a tab delimited file (.txt) and then read this file in using \texttt{read.table()}, save it as: my\_data2.

Use the functions I showed above to ensure \texttt{read.table()} was read in properly.
\end{rmdquiz}

\hypertarget{understanding-data-types-and-r-objects}{%
\section{Understanding data types and R objects}\label{understanding-data-types-and-r-objects}}

Now that we have imported our data into R, it becomes important to understand some of the basic data types.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(TG\_data\_for\_day\_1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "data.frame"
\end{verbatim}

If we run the function \texttt{class()}, R will tell us what type of data object we are working on. The data.frame is a common form of data for plotting, analysis and statistics. The \texttt{read.csv} function will form a data.frame automatically. Within a data.frame the columns are vectors. A vector contains a series of numbers (or characters). A vector is the most common and simple data form of R. You can assign a vector manually, using the function \texttt{c()}. This is a generic function which combines its arguments (`c' is short for `concatenate'). For example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_first\_vector }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{167}\NormalTok{,}\DecValTok{212}\NormalTok{,}\DecValTok{358}\NormalTok{,}\DecValTok{973}\NormalTok{,}\DecValTok{214}\NormalTok{,}\DecValTok{558}\NormalTok{)}
\NormalTok{my\_first\_vector}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 167 212 358 973 214 558
\end{verbatim}

Above we have produced a numeric vector of 6 numbers. We can confirm this by using \texttt{class()} function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(my\_first\_vector)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "numeric"
\end{verbatim}

We could also make a vector containing just characters:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_second\_vector}\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Tios"}\NormalTok{, }\StringTok{"Paragon"}\NormalTok{, }\StringTok{"Skyfall"}\NormalTok{, }\StringTok{"Grafton"}\NormalTok{, }\StringTok{"Graham"}\NormalTok{, }\StringTok{"Robigus"}\NormalTok{ )}
\NormalTok{my\_second\_vector}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Tios"    "Paragon" "Skyfall" "Grafton" "Graham"  "Robigus"
\end{verbatim}

We've produced a character vector that contains 6 names.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(my\_second\_vector)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "character"
\end{verbatim}

A Vector can also be logical, as in it contains either TRUE or FALSE:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_third\_vector }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\ConstantTok{FALSE}\NormalTok{, }\ConstantTok{FALSE}\NormalTok{, }\ConstantTok{TRUE}\NormalTok{, }\ConstantTok{TRUE}\NormalTok{ , }\ConstantTok{TRUE}\NormalTok{ , }\ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{class}\NormalTok{(my\_third\_vector)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "logical"
\end{verbatim}

We can now actually combine these vectors into a data.frame. A data.frame can handle a combination of variables which are of different modes(numeric, logical, ect.).For example if we use the \texttt{str()} function on our previously loaded data.frame, we can see the different types of each variable

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(TG\_data\_for\_day\_1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    376 obs. of  15 variables:
##  $ variety    : chr  "AARDEN" "AARDVARK" "ABELE" "ABO" ...
##  $ year       : int  2003 1997 1970 1977 1998 2007 2004 2007 2006 1992 ...
##  $ ORIGIN     : chr  "DEU" "GBR" "GBR" "FRA" ...
##  $ Rht2       : int  1 NA 0 0 1 1 1 0 1 1 ...
##  $ PpdD1      : int  0 1 0 1 0 1 1 1 0 0 ...
##  $ yield      : num  103.2 88.5 90.8 82.2 96.7 ...
##  $ CALLOW_2011: num  114.2 96.7 95.3 87.1 102.9 ...
##  $ FRANCE_2010: num  136 122 124 115 132 ...
##  $ FRANCE_2011: num  100.2 96.4 92.9 85.8 101.5 ...
##  $ LGE_2010   : num  93.5 75 82.8 77.5 84.7 ...
##  $ LGE_2011   : num  96.2 75.7 83.2 68.4 87.5 ...
##  $ NIAB_2011  : num  79.4 65.3 66.1 59.5 71.3 ...
##  $ FT         : num  156 153 154 149 157 ...
##  $ HT         : num  67.6 66.3 81.1 69.6 66.7 ...
##  $ AWNS       : int  0 0 0 0 0 1 0 0 0 0 ...
\end{verbatim}

It seems that the variables (columns in the data.frame) are either in the form of characters, numeric or integers, depending on their content. Let's form a data.frame from our three vectors:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_DF}\OtherTok{\textless{}{-}}\FunctionTok{data.frame}\NormalTok{(my\_second\_vector, my\_first\_vector, my\_third\_vector)}
\NormalTok{my\_DF}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   my_second_vector my_first_vector my_third_vector
## 1             Tios             167           FALSE
## 2          Paragon             212           FALSE
## 3          Skyfall             358            TRUE
## 4          Grafton             973            TRUE
## 5           Graham             214            TRUE
## 6          Robigus             558            TRUE
\end{verbatim}

Note the length of the variables need to be the same. We can now use \texttt{class()}, \texttt{str()} and \texttt{dim()} to inspect the \texttt{data.frame}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(my\_DF)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "data.frame"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(my\_DF)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    6 obs. of  3 variables:
##  $ my_second_vector: chr  "Tios" "Paragon" "Skyfall" "Grafton" ...
##  $ my_first_vector : num  167 212 358 973 214 558
##  $ my_third_vector : logi  FALSE FALSE TRUE TRUE TRUE TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dim}\NormalTok{(my\_DF)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 6 3
\end{verbatim}

So we have a new data.frame, with 6 rows and 3 columns. The first column contains factors (which is typically how text is handled) and the second is numeric. The third is logical. Factors are categorical data and are stored as integer vectors.

We can use a combination of \texttt{names()} and \texttt{c()} to change our column names:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(my\_DF)}\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Line"}\NormalTok{,}\StringTok{"Block"}\NormalTok{,}\StringTok{"Winter\_Habit"}\NormalTok{)}
\NormalTok{my\_DF}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Line Block Winter_Habit
## 1    Tios   167        FALSE
## 2 Paragon   212        FALSE
## 3 Skyfall   358         TRUE
## 4 Grafton   973         TRUE
## 5  Graham   214         TRUE
## 6 Robigus   558         TRUE
\end{verbatim}

We can force R to change the type of a column. For instance if we wanted ``Block'' to be treated as a factor rather then numeric, we could use:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_DF}\SpecialCharTok{$}\NormalTok{Block}\OtherTok{\textless{}{-}}\FunctionTok{as.factor}\NormalTok{(my\_DF}\SpecialCharTok{$}\NormalTok{Block)}
\FunctionTok{str}\NormalTok{(my\_DF)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    6 obs. of  3 variables:
##  $ Line        : chr  "Tios" "Paragon" "Skyfall" "Grafton" ...
##  $ Block       : Factor w/ 6 levels "167","212","214",..: 1 2 4 6 3 5
##  $ Winter_Habit: logi  FALSE FALSE TRUE TRUE TRUE TRUE
\end{verbatim}

Note the use of the \texttt{\$} sign to specify a column (variable) of a data.frame. We will use this a lot throughout. We could work in reverse now and use this to extract a column from a data.frame and save it as a vector:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_V}\OtherTok{\textless{}{-}}\NormalTok{TG\_data\_for\_day\_1}\SpecialCharTok{$}\NormalTok{yield}
\FunctionTok{length}\NormalTok{(my\_V)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 376
\end{verbatim}

We have extracted the yield column from the data.frame we loaded earlier and saved it as a variable consisting of 376 numbers. If we run \texttt{my\_V} we would see all of these numbers in the console.

To save space, let's output only the 5th number from the 376 numbers, using \texttt{{[}{]}} brackets like so:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_V[}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 96.66
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_V      [}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 96.66
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_V            [}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 96.66
\end{verbatim}

Note how the variable uses of spaces makes no difference, that's a nice element of R.

Let's say we only wanted the first five elements:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_V[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 103.23  88.55  90.76  82.24  96.66
\end{verbatim}

We can reverse the output like so:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_V[}\DecValTok{5}\SpecialCharTok{:}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  96.66  82.24  90.76  88.55 103.23
\end{verbatim}

We can also pick and choose using \texttt{c()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_V[}\FunctionTok{c}\NormalTok{(}\DecValTok{121}\NormalTok{, }\DecValTok{200}\NormalTok{, }\DecValTok{345}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  98.83 102.75 100.24
\end{verbatim}

The use of square brackets is the same for a data.frame, except a data.frame consists of rows and columns:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dim}\NormalTok{(TG\_data\_for\_day\_1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 376  15
\end{verbatim}

We have 376 rows and 15 columns and with this any element of a data.frame can be called using: \texttt{data.frame{[}row,\ column{]}}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TG\_data\_for\_day\_1[}\DecValTok{5}\NormalTok{,}\DecValTok{6}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 96.66
\end{verbatim}

That's the data point in the 5th row and the 6th column. We could look at the whole of the 5th row like so:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TG\_data\_for\_day\_1[}\DecValTok{5}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   variety year ORIGIN Rht2 PpdD1 yield CALLOW_2011 FRANCE_2010 FRANCE_2011
## 5  ACCESS 1998    GBR    1     0 96.66      102.86      132.06      101.49
##   LGE_2010 LGE_2011 NIAB_2011    FT    HT AWNS
## 5    84.68     87.5     71.34 156.6 66.69    0
\end{verbatim}

We can't save this as a vector as there are different data types present (factor and numeric), so assigning this row from our data.frame to a new object would create a new data.frame, with 1 row and 15 columns.

\hypertarget{summary}{%
\subsection{Summary}\label{summary}}

I've only focused on a subset of data types and classes. Here are some more examples and formal definitions that I have copied from a book called ``R in action'' by Robert Kabacoff. You can find a free PDF online.

In R, an object is anything that can be assigned to a variable. This includes constants, data structures, functions, and even graphs. An object has a mode (which describes how the object is stored) and a class (which tells generic functions like print how to handle it).

A data frame is a structure in R that holds data and is similar to the datasets found in standard statistical packages (for example, SAS, SPSS, and Stata). The columns are variables, and the rows are observations. You can have variables of different types (for example, numeric or character) in the same data frame. Data frames are the main structures you use to store datasets.

\textbf{Factors} are nominal or ordinal variables. They're stored and treated specially in R.

\textbf{Vectors} are one-dimensional arrays that can hold numeric data, character data, or logical data.

\textbf{Matrix} is a two-dimensional array in which each element has the same mode
(numeric, character, or logical). Matrices are created with the matrix()function.

\textbf{Arrays} are similar to matrices but can have more than two dimensions.

\textbf{Data frame} is more general than a matrix in that different columns can contain different modes of data (numeric, character, and so on). It's similar to the dataset you'd typically see in SAS, SPSS, and Stata. Data frames are the most common data structure you'll deal with in R.

\textbf{Lists} are the most complex of the R data types. Basically, a list is an ordered collection of objects (components). A list allows you to gather a variety of (possibly unrelated) objects under one name. For example, a list may contain a combination of vectors, matrices, data frames, and even other lists. You create a list using the list() function.

\begin{rmdquiz}
\textbf{4} Form three vectors. The first, a character vector containing the names of each day of the week. The second, a numeric vector containing 7 random numbers (hint: try using \texttt{runif(7,\ 1,\ 10)}). The third, a logical vector corresponding to whether you would typically visit work on each day of the week (TRUE or FALSE).

Save the above as a data.frame and change the column names to: day\_of\_week, coffee\_consumed and attendance.

Change the mode of the data in the coffee\_consumed from numeric to factor.\\
\end{rmdquiz}

\hypertarget{summarising-and-displaying-data}{%
\section{Summarising and displaying data}\label{summarising-and-displaying-data}}

\emph{`Time spent in reconnaissance is never wasted' - Napoleon (attrib).}

It is always worthwhile to spend time scanning and summarising new datasets before starting formal statistical analysis. Simple methods such as studying the range, the distribution and the relationships between variables can often reveal unexpected structure or the presence of errors in a dataset.

Let's look at the TriticeaeGenome data. Try some or all of the commands below.

\texttt{hist(yield)}

\texttt{plot(yield\textasciitilde{}year)}

\texttt{plot(yield\textasciitilde{}Rht2)}

It is very easy to produce basic informative graphs in R. Titles, font sizes, colours and so on can all be changed by adding additional options to the basic commands above, but we shall introduce those as we go along.

\texttt{plot(yield\textasciitilde{}Rht2)} is not great. Try \texttt{boxplot(yield\textasciitilde{}Rht2)} which produces a box plot, or box-and-whisker plot. The central bold line is the median. The boxes show, approximately, the first and third quartiles. The lines extending from the boxes to the horizontal bars then show the distance to the maximum and minimum observations. However, any data viewed as outliers are plotted separately. These plots must not be used as statistical test for outliers. The definition of what is plotted as an outlier can be changed with additional arguments. The default is 1.5 times the interquartile range (the width of the middle 50\% of observations).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{boxplot}\NormalTok{(yield}\SpecialCharTok{\textasciitilde{}}\NormalTok{PpdD1}\SpecialCharTok{+}\NormalTok{Rht2, }\AttributeTok{varwidth=}\ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-44-1.pdf}

We now have boxplots for the four classes of +/- ppd and +/- rht2 alleles. We added \texttt{varwidth=TRUE} to scale the width of each boxplot to the square root of the different samples sizes. The group ``0.1'' carries the wild-type ppdD1 allele for late flowering and the dwarfing allele at rht2, and would appear to be the highest yielding, and largest, group of lines. Substitute FT (flowering time) for yield in the boxplot to get something a little more exciting.

To find out what options are available for commands, switch from the ``Plots'' tab to ``Help'', then type in the name of the command you are interested in. The help is often cryptic, but with practice, its interpretation becomes easier.

A useful command for displaying relations between all variables in a data set is ``pairs'', we'll introduce the command and then refine it. Try running:

\texttt{pairs(TG\_data\_for\_day\_1)}

It produces an error message? Some of the data are non-numeric. This is being caused by the names in the ``variety'' column. We'll remind ourselves what the names of the data columns are:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(TG\_data\_for\_day\_1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "variety"     "year"        "ORIGIN"      "Rht2"        "PpdD1"      
##  [6] "yield"       "CALLOW_2011" "FRANCE_2010" "FRANCE_2011" "LGE_2010"   
## [11] "LGE_2011"    "NIAB_2011"   "FT"          "HT"          "AWNS"
\end{verbatim}

We are only interested, in plotting the yield data, ignoring the other traits for now. We'll also select the subset of lines which were registered in Germany. These are labeled DEU in the origin column. Finally we'll add a title:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pairs}\NormalTok{(TG\_data\_for\_day\_1[(ORIGIN}\SpecialCharTok{==}\StringTok{"DEU"}\NormalTok{),}\DecValTok{6}\SpecialCharTok{:}\DecValTok{12}\NormalTok{],}\AttributeTok{main=}\StringTok{"German varieties only"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-46-1.pdf}

This is easier to interpret and we can see that the yield of German varieties is strongly correlated across all the sites. Note the use of the square brackets \texttt{{[}\ {]}} to reference rows and columns of our data set. We have seen this in the last section too. As revision:

\begin{itemize}
\tightlist
\item
  \texttt{{[}1,2{]}} refers to a single cell - row one, column 2.
\item
  \texttt{{[}2:5,{]}} refers to rows 2 to 5 inclusive, and to all columns.
\item
  \texttt{{[},-3{]}} refers to all rows and all columns except column 3.
\item
  \texttt{{[},c(1,4:6){]}} refers to all rows and to columns 1, 4, 5, and 6.
\end{itemize}

You cannot mix omission of rows or columns by using ``-'' with inclusion using anything else, but otherwise this provides you with a flexible way of selecting subsets of data.

\texttt{c(...\ ,\ ...\ ,\ ...\ ,)} as used above, is a method of concatenating data into a single entity and is used quite extensively. It can also be used as a method of entering small amounts of data directly into R:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fred}\OtherTok{\textless{}{-}}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{, }\StringTok{"Buckle my shoe"}\NormalTok{)}
\NormalTok{fred}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "1"              "2"              "Buckle my shoe"
\end{verbatim}

Another way of selecting a subset of data is to use the subset command

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fred}\OtherTok{\textless{}{-}}\FunctionTok{subset}\NormalTok{(TG\_data\_for\_day\_1,}\AttributeTok{select=}\FunctionTok{c}\NormalTok{(variety,ORIGIN,yield))}
\FunctionTok{head}\NormalTok{(fred)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    variety ORIGIN  yield
## 1   AARDEN    DEU 103.23
## 2 AARDVARK    GBR  88.55
## 3    ABELE    GBR  90.76
## 4      ABO    FRA  82.24
## 5   ACCESS    GBR  96.66
## 6    ACCOR    FRA  90.05
\end{verbatim}

The new command \texttt{head}, gives the same result as: \texttt{fred{[}1:6,{]}}. It prints the first 6 rows.

In R, the simplest non-graphical method of generating a summary of data is:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(TG\_data\_for\_day\_1)[,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    variety               year         ORIGIN               Rht2       
##  Length:376         Min.   :1946   Length:376         Min.   :0.0000  
##  Class :character   1st Qu.:1988   Class :character   1st Qu.:0.0000  
##  Mode  :character   Median :1997   Mode  :character   Median :1.0000  
##                     Mean   :1994                      Mean   :0.5508  
##                     3rd Qu.:2003                      3rd Qu.:1.0000  
##                     Max.   :2007                      Max.   :1.0000  
##                                                       NA's   :22      
##      PpdD1            yield       
##  Min.   :0.0000   Min.   : 67.79  
##  1st Qu.:0.0000   1st Qu.: 88.60  
##  Median :0.0000   Median : 92.63  
##  Mean   :0.2261   Mean   : 91.83  
##  3rd Qu.:0.0000   3rd Qu.: 96.32  
##  Max.   :1.0000   Max.   :105.58  
## 
\end{verbatim}

Again, to save space, I've limited this command to columns 1 to 6, by using: \texttt{{[},1:6{]}}.

For a numeric column such as yield, the output typically shows:

\begin{itemize}
\tightlist
\item
  Min. : the minimum value
\item
  1st Qu.: the first quantile
\item
  Median : the median
\item
  Mean : the sample average
\item
  3rd Qu.: the third quantile
\item
  Max. : the maximum value.
\item
  NA's : the number of missing values
\end{itemize}

The first quartile, the median and the third quartile give the values of the observations, 1/4, 1/2, and 3/4 of the way down a sorted list of each variable. These values, together with the minimum, maximum and average, give a simple assessment of the distribution of the traits.
You can also just get the summary information for one variable by using:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(HT)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   57.52   69.77   73.79   75.25   80.12   97.15
\end{verbatim}

The summary statistics given collectively by summary are also available as separate commands, listed below (substitute the variable name of your choice for ``x''):

\texttt{mean(x)}

\texttt{median(x)}

\texttt{quantile(x)}

\texttt{min(x)}

\texttt{max(x)}

Note that \texttt{quantile} returns the quartiles. This is the default for the command, which can be altered.

This is an important quirk of R:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(Rht2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] NA
\end{verbatim}

R does not ignore missing values by default (although some functions do). Because Rht2 has some missing data (the NAs) R returns ``NA'' rather than the mean of the available data. This is often irritating, but it is safe: you will not forget that you have missing data. The rather longwinded way to cope with this is as follows:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(Rht2,}\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.5508475
\end{verbatim}

The additional logical variable \texttt{na.rm} (meaning: not available, remove) is set to the value TRUE. In English: remove the NA values before calculating the mean. A number of other commands require this option too.

One dangerous exception to the default behavior of R is the command length, which counts the number of entries in a vector:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{length}\NormalTok{(Rht2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 376
\end{verbatim}

The value of 376 is returned: this includes the missing values, which is logical but often misleading. To count the number of observations, excluding the NAs:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sum}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(Rht2))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 354
\end{verbatim}

Breaking this down, \texttt{is.na()} is the command to return the logical value TRUE or FALSE depending on whether each value of a variate exists or not.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{is.na}\NormalTok{(Rht2)[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE  TRUE FALSE FALSE FALSE
\end{verbatim}

I've only shown the first five logical values, but this would continue for every row. The use of \texttt{!is.na()} switches this around to return TRUE if the value is NA (ie does not exist). \texttt{!} is the R symbol to negate the following argument. (We introduced \texttt{!=} to mean ``not equal to'' earlier.)

\begin{Shaded}
\begin{Highlighting}[]
\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(Rht2)[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  TRUE FALSE  TRUE  TRUE  TRUE
\end{verbatim}

For arithmetic purposes, the logical TRUE has a value of 1 and FALSE has a value of 0, \texttt{sum()} returns the sum or total, so \texttt{sum(!is.na(Rht2))} returns the value of 354.

Some other useful summary commands are listed below:

\texttt{sum(yield,na.rm=T)} returns the total yield

\texttt{sum(yield,na.rm=T)} returns the of yield variance

\texttt{sd(yield,na.rm=T)} returns the standard deviation.

\texttt{rowMeans(dataset,na.rm=T)} returns means across rows of a dataset

\texttt{colMeans(dataset,na.rm=T)} returns means across columns of a dataset

\texttt{rowSums(dataset,na.rm=T)} returns totals across rows of a dataset

\texttt{colSums(dataset,na.rm=T)} returns totals across columns of a dataset

These commands have all been listed including the \texttt{na.rm=T} option. If the dataset is complete this option need not be included.

\begin{rmdquiz}
\textbf{5} Let's say we measured leaf length from 10 plants from one of our plots. For whatever reason one leaf was missing from the measurements:

Create this vector: \texttt{leaf\_sizes\textless{}-c(30.1,\ 32.3,\ 36.8,\ 34.1,\ 32.0,\ NA,\ 32.4,\ 30.1,\ 29.9,\ 32.6)}

Plot a histogram showing the distribution of our leaves. Then add a title and figure legends (measurements are in cm), see \texttt{help(hist)} if you get stuck.

What's the mean of the 9 leaves?

It's pointless really to keep the NA in the vector here, produce a vector of 9 numbers without the NA.\\
\end{rmdquiz}

\hypertarget{calculations-and-data-manipulation}{%
\section{Calculations and data manipulation}\label{calculations-and-data-manipulation}}

Once data have been read into R, the vectors (columns of data) which contain each variable can be manipulated in the same manner as individual numbers. For example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{harvest\_index}\OtherTok{\textless{}{-}}\NormalTok{yield}\SpecialCharTok{/}\NormalTok{HT}
\FunctionTok{boxplot}\NormalTok{(harvest\_index}\SpecialCharTok{\textasciitilde{}}\NormalTok{PpdD1)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-58-1.pdf}

The late flowering allele (coded 1) seems to have a slightly higher harvest index. Whether this difference is likely genuine or just due to chance is something shall see how to test shortly.

\hypertarget{sorting-data}{%
\subsection{Sorting data}\label{sorting-data}}

Sorting data and inspection of high and low values is also of assistance in detecting errors. In R this is carried out using the command sort:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sort}\NormalTok{(yield) [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 67.79 71.12 73.67 74.12 74.38
\end{verbatim}

To save paper I've added the \texttt{{[}1:5{]}} to only produce the first 5 values, or in this case the lowest 5 values.

More generally, it is usual to sort a block of data with respect to one or more columns. Routinely, this may be more easily achieved in Excel. In R: First select yield and some descriptors for ease of display:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TG.sub}\OtherTok{\textless{}{-}}\FunctionTok{subset}\NormalTok{(TG\_data\_for\_day\_1,}\AttributeTok{select=}\FunctionTok{c}\NormalTok{(variety,ORIGIN,year,yield))}
\end{Highlighting}
\end{Shaded}

Then display the top few lines of the new dataframe:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(TG.sub[}\FunctionTok{order}\NormalTok{(ORIGIN,year),])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          variety ORIGIN year yield
## 190         IBIS    DEU 1963 89.61
## 234 MIRONOVSKAJA    DEU 1963 84.43
## 45        ASTRON    DEU 1965 92.19
## 97        CARIBO    DEU 1968 88.29
## 310      SENATOR    DEU 1970 95.76
## 229       MERKUR    DEU 1973 86.37
\end{verbatim}

We used \texttt{order(ORIGIN,year)} to sort the whole dataframe, indexed by country of origin and then year of release. Alphabetically, the German varieties are first.

\hypertarget{relationships-between-variates}{%
\subsection{Relationships between variates}\label{relationships-between-variates}}

The correlation coefficient ranges from zero to one and measures the strength of the relationship between two variables:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(yield,year)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.6218021
\end{verbatim}

We can enter a block of data and generate a table of correlation coefficients in a single command.
There are some traps for the unwary however:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(TG\_data\_for\_day\_1[,}\DecValTok{4}\SpecialCharTok{:}\DecValTok{6}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       Rht2      PpdD1      yield
## Rht2     1         NA         NA
## PpdD1   NA  1.0000000 -0.1556021
## yield   NA -0.1556021  1.0000000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(TG\_data\_for\_day\_1[,}\DecValTok{4}\SpecialCharTok{:}\DecValTok{6}\NormalTok{],}\AttributeTok{use=}\StringTok{"complete"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             Rht2      PpdD1      yield
## Rht2   1.0000000 -0.2461724  0.5021948
## PpdD1 -0.2461724  1.0000000 -0.1728991
## yield  0.5021948 -0.1728991  1.0000000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(TG\_data\_for\_day\_1[,}\DecValTok{4}\SpecialCharTok{:}\DecValTok{6}\NormalTok{],}\AttributeTok{use=}\StringTok{"pairwise.complete"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             Rht2      PpdD1      yield
## Rht2   1.0000000 -0.2461724  0.5021948
## PpdD1 -0.2461724  1.0000000 -0.1556021
## yield  0.5021948 -0.1556021  1.0000000
\end{verbatim}

The first call gives no results for Rht2 because it has missing data. The second method uses ``complete observations'', that is to say only records with no missing data for any field. This is the default method for some commercial statistical software systems. In the example here, 22 records of data are discarded. The third method does not discard complete records. It excludes from the analysis only those pairs of observations in which at least one of the pair is NA. In this example, correlations among PpdD1 and yield will be based on all varieties as there are no missing data for these two traits. This option can be particularly useful with extensive sets of genotype data: even if marker calling rates are high, with multiple markers it may be rare for a single individual or line to have no missing data.

In passing, we note that the calculation of a correlation coefficient is not usually a particularly sensible or conventional way to study the relationship between two binary variables (here Rht2 and PpdD1). More conventional would be to tabulate the data in a 2 x 2 contingency table. However, for marker data, the squared correlation coefficient is very commonly used as a measure of linkage disequilibrium between two loci, so the example given here has some justification (although a 2 x 2 table of observations is still informative and is demonstrated later on).

Correlation coefficients are a simple way of quantifying relationships between two variables. However, as we have already commented, it is often better to visualise the data in a scatter plot:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(year,yield)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-66-1.pdf}

R also provides other methods of displaying these correlations graphically. \texttt{Image} creates a coloured grid of a matrix with colours depending on the values in the matrix. First we shall save the correlation matrix, working only on the trait data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TG.cor}\OtherTok{\textless{}{-}}\FunctionTok{cor}\NormalTok{(TG\_data\_for\_day\_1[,}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\SpecialCharTok{:{-}}\DecValTok{3}\NormalTok{)],}\AttributeTok{use=}\StringTok{"pairwise.complete"}\NormalTok{)}
\FunctionTok{image}\NormalTok{(TG.cor}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-67-1.pdf}

Various colours are schemes are possible: type help(image)for more detail. Note we have squared the correlation coefficient - so all values are positive - otherwise the interpretation with both large negative and large positive correlations present can become difficult. Here, the block of pale colours in the centre of the plot indicates very high correlations among the yield measurements, with lower correlations among the other traits. The function \texttt{heatmap()} is a useful extension to this which also plots a dendogram showing estimated relationships between entries.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{heatmap}\NormalTok{(TG.cor}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{,}\AttributeTok{symm=}\NormalTok{T)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-68-1.pdf}
This plot alters the row and column order. \texttt{symm=T} is required for heatmap to recognize that it is working on a symmetrical matrix.

The interpretation seems clear. The yield measurements correlate highly. PpdD1 and flowering time are associated, as are Rht2 and height. The relationships show up even better using a different colour scheme (which I found someone else using and copied):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{heatmap}\NormalTok{(TG.cor}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{,}\AttributeTok{symm=}\NormalTok{T,}\AttributeTok{col=}\FunctionTok{c}\NormalTok{(}\FunctionTok{grey}\NormalTok{(}\FloatTok{0.4}\NormalTok{), }\StringTok{"blue"}\NormalTok{, }\StringTok{"green"}\NormalTok{, }\StringTok{"orange"}\NormalTok{,}\StringTok{"red"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-69-1.pdf}

This highlights a weak relationship between height and yield, at least in some environments.

\hypertarget{exporting-your-script-and-workspace}{%
\subsection{Exporting your script and workspace}\label{exporting-your-script-and-workspace}}

This is not as straightforward as you would think as the script and workspace (your data in the Environment window) need to be saved separately.

To save your workspace (the working environment you have created) in your working directory, you can just click:

\emph{Session / Save Workspace As}

Or you could run:

\texttt{save.image(file="File\_name\_for\_your\_data.RData")}

Note that this will save an \texttt{.RData} file.

Next save the script you have created in the \emph{script editing window} as an R file. You can then come back to your results by loading both the R file and the saved workspace. You simply save the script by clicking on \emph{File / Save}.
Or you could press: \textbf{Ctrl + S}

To reload your workspace you can go to: \emph{Session / Load Workspace}. Or use \texttt{load("myfile.RData")} if you are in the correct working directory. To open your script file, you would simple click on it and it will open RStudio (or open in RStudio if you already have the program open).

\hypertarget{saving-a-data-table}{%
\subsection{Saving a data table}\label{saving-a-data-table}}

There are two simple methods for this, the simplest is to cut and paste from R to Word or Excel, exactly as you would for any other Windows application.

However, a better method would be to use R to do it. If you want to export a table you have created as a csv file (other formats are available):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#create a subset of the main data, in this case we just take varieties with a GBR origin}
\NormalTok{GBRorigin}\OtherTok{\textless{}{-}}\FunctionTok{subset}\NormalTok{(TG\_data\_for\_day\_1, ORIGIN}\SpecialCharTok{==}\StringTok{"GBR"}\NormalTok{)}

\CommentTok{\#save this as a new csv file}
\FunctionTok{write.csv}\NormalTok{(GBRorigin, }\StringTok{"this will be the file name.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{rmdtip}
That file will be saved in your directory now. Notice my use of \# here. You will see this a lot in R. Putting a \# at the start of a line means that R won't try to run that line, which means that you can add notes to yourself within your scripts.
\end{rmdtip}

\begin{rmdquiz}
\textbf{6} You now should of seen the use of \texttt{subset()} several times. This is a very useful function which subsets large data.frames to smaller ones based on criteria. Firstly use \texttt{subset()} to form a new data.frame of only the unawned lines. Ask for help if you are stuck.

Re-order your new data.frame by origin.

Using your new data, form a plot showing the relationship between height (HT) and yield.
Use \texttt{cor()} to test the correlation between the two variables.
\end{rmdquiz}

\hypertarget{basic-statistical-analysis}{%
\section{Basic statistical analysis}\label{basic-statistical-analysis}}

\hypertarget{the-t-test}{%
\subsection{The t-test}\label{the-t-test}}

The t-test is a simple and robust method to test if the difference in means between two samples, or the difference between the mean of a sample and a known constant, is statistically significant. In other words, does the difference look too large to have occurred as a result of bad luck in selecting the samples for analysis?

t = difference / standard error of the difference.

For large sample sizes, a value of t \textgreater{} 1.96 will only occur by chance, in the absence of any genuine difference, in about 5\% of experiments. A value \textgreater{} 1.96 is therefore judged to be improbably large: the difference in means is declared to be statistically significant at the 5\% level.

The t-test assumes that the sampling error of the difference being tested is normally distributed. In real data sets, this condition is often met. Firstly, the trait being measured is itself often normally distributed, and secondly, even if the trait has a non-normal distribution, mean trait values will be close to normal provided the sample size is moderately large (greater than about 10).

For example, the plot below shows the distribution of 1000 numbers. Each number was generated by taking the mean of 10 uniformly distributed random numbers. It is clear that although the distribution of original random numbers was very non-normal, the mean of a sample of 10 such numbers is pretty close to normal. In fact, in the early days of computing, normally distributed random numbers were often generated in this way. The tendency for the distribution of means to be normally distributed is called the Central Limit Theorem. It explains the popularity of the normal distribution in statistics and also the tendency for many traits in nature to be roughly normally distributed - for example if variation in a phenotype results from variation at multiple genes, the phenotype itself will often inevitably be normally distributed.

\begin{figure}
\centering
\includegraphics{images/Distribution Plot.png}
\caption{Illustration of the central limit theorem}
\end{figure}

The t-test is very simply invoked in R. To test the difference in means between yield in Germany in the two years of testing:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(LGE\_2010,LGE\_2011)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Welch Two Sample t-test
## 
## data:  LGE_2010 and LGE_2011
## t = 9.8055, df = 745.63, p-value < 2.2e-16
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  4.019585 6.032010
## sample estimates:
## mean of x mean of y 
##  85.79497  80.76918
\end{verbatim}

The output provides a value for t and its associated p-value to test the significance of the difference in means, and also the means themselves. In addition, 95\% confidence intervals are provided. These refer to the difference between the two means. Statisticians can get quite hot under the collar about what, exactly, 95\% confidence intervals actually are. We can state that over a long lifetime of calculating 95 \% confidence intervals for parameter estimates, they will have included the true parameter value in 95 \% of cases. It is best not to worry too much about this.

Note that the degrees of freedom (df) is 745.63 and not a whole number. This is because the default setting for R is to assume that, whether or not the means of the two groups being tested are different, the variances themselves are different. In accounting for this we end up with fractional degrees of freedom. This is the Welch variant of the t-test - stated in the first line of the output.

To test whether the variances in the two groups are similar, we can use a variance ratio test, or F test - dividing one variance by the other and estimating whether the deviation from the expected value of 1 is attributable to chance or is indicative of something else:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{var.test}\NormalTok{(LGE\_2010,LGE\_2011)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  F test to compare two variances
## 
## data:  LGE_2010 and LGE_2011
## F = 0.85785, num df = 375, denom df = 375, p-value = 0.1381
## alternative hypothesis: true ratio of variances is not equal to 1
## 95 percent confidence interval:
##  0.700429 1.050643
## sample estimates:
## ratio of variances 
##          0.8578466
\end{verbatim}

The F-ratio of 0.86, with 375 and 375 degrees of freedom is not significant (p-value 0.1381).

If var.test(LGE\_2011,LGE\_2010) was called (the order of the two trials is reversed), the F-ratio would be 1.17 but the p-value would be unchanged. When testing the ratios of two variances in an F test, care is required because we are not testing whether the numerator variance is significantly greater than the denominator, which is the usual use of the F-ratio in ANOVA. Here we are carrying out a two-tailed test of significance rather than a one-tailed. Fortunately using var.test() takes care of this for you. If you were to do this by hand; dividing the larger variance by the smaller, then looking up the probability associated with that value of F in tables (or you could use an R function, p(f), then the probability would need to be doubled.

Since the variances are not significantly different (i.e.~they are homogeneous), \texttt{t.test} can be called in a form to take this into account:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(LGE\_2010,LGE\_2011,}\AttributeTok{var.equal =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Two Sample t-test
## 
## data:  LGE_2010 and LGE_2011
## t = 9.8055, df = 750, p-value < 2.2e-16
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  4.019595 6.032001
## sample estimates:
## mean of x mean of y 
##  85.79497  80.76918
\end{verbatim}

This is the more usual form for the t test - as given in most text books. It is more powerful test than Welch's variant, provided the variances are homogeneous. Note that the degrees of freedom is now an integer.

A more biologically interesting comparison is whether Rht2 and PpdD1 have an effect on yield. The problem here is that data for the two groups to be compared (yields for one allele and yields for the other) are no longer in separate variables. To tell R that data to be analysed are in one variate but are described by data in another, we use the tilde operator (\textasciitilde) introduced in the section on syntax:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(yield}\SpecialCharTok{\textasciitilde{}}\NormalTok{PpdD1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Welch Two Sample t-test
## 
## data:  yield by PpdD1
## t = 3.0435, df = 136.67, p-value = 0.002806
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  0.8154943 3.8410701
## sample estimates:
## mean in group 0 mean in group 1 
##        92.35522        90.02694
\end{verbatim}

The difference in mean yield between the two PpdD1 alleles is significant, but quite small. PpdD1 could have a direct effect on yield or it could lie close to a QTL for yield, or the significant result could be due to something we don't know about concerning the origins of the varieties under test. Disentangling trait-marker associations due to the presence of a closely linked QTL from other spurious causes of association is the challenge of association genetics. We can also see if the effect of PpdD1 is consistent across all sites. There are six sites, in columns 6-12 of the dataset. We could analyse each site by editing the name of the variable before running the command, but we can automate this procedure using a loop.

This would be achieved by running:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{6}\SpecialCharTok{:}\DecValTok{12}\NormalTok{) \{}
   \FunctionTok{print}\NormalTok{(}\FunctionTok{t.test}\NormalTok{(TG\_data\_for\_day\_1[,i]}\SpecialCharTok{\textasciitilde{}}\NormalTok{ PpdD1))}
\NormalTok{       \}}
\end{Highlighting}
\end{Shaded}

Give this a go on your computers.

What do you make of the output? It's possible to select individual components of the output (such as the means and p-values) so we can summarise many analyses concisely.

\texttt{for(i\ in\ 6:12)} means repeat the commands which follow a number of times by varying an index variable, here ``i'', from 6 to 12 (seven times in total), incrementing it by 1 each repeat. Then, because we are analysing have TG\_data\_for\_day\_1{[},i{]}, on each iteration {[},i{]} is substituted with {[},6{]} then {[},7{]} \ldots{} {[},12{]} and all our analyses are done. We are beginning to write scripts!

The commands we want to loop over do not have to be restricted to a single line, provided they are enclosed in curly brackets. Note that as these commands are run within a loop, the output does not get echoed to the screen unless we include the ``print'' statement as above.

\hypertarget{linear-regression}{%
\subsection{Linear Regression}\label{linear-regression}}

We have already come across the command to correlate two traits: \texttt{cor()}. To fit a straight line to a data set we use the R command \texttt{lm()}; for linear model. Suppose we want to study the effect of year on yield:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lm}\NormalTok{(yield}\SpecialCharTok{\textasciitilde{}}\NormalTok{year)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = yield ~ year)
## 
## Coefficients:
## (Intercept)         year  
##   -583.2088       0.3386
\end{verbatim}

The output is somewhat sparse. A feature of R, in contrast to many statistical packages is that by default it does not deliver multiple pages of output by default, from which you may only wish to extract a single figure. Here the output gives you the best fitting straight line:

\(yield = -583.4 + (0.3386 * year)\)

More output is available but we need to be explicit that we wish R to produce it. First we shall rerun the analysis, but save the results:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{yield\_year\_regression}\OtherTok{\textless{}{-}}\FunctionTok{lm}\NormalTok{(yield}\SpecialCharTok{\textasciitilde{}}\NormalTok{year)}
\end{Highlighting}
\end{Shaded}

No output is generated. We can produce some using \texttt{summary()}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(yield\_year\_regression)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = yield ~ year)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -18.1783  -3.2680   0.7123   3.1559  12.6978 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(>|t|)    
## (Intercept) -583.20878   43.96487  -13.27   <2e-16 ***
## year           0.33855    0.02205   15.35   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 4.915 on 374 degrees of freedom
## Multiple R-squared:  0.3866, Adjusted R-squared:  0.385 
## F-statistic: 235.8 on 1 and 374 DF,  p-value: < 2.2e-16
\end{verbatim}

The most interesting part of the output is given at the end: the F-statistic and p-value for the significance of the regression - hugely significant in this example. The Multiple R-Squared is the proportion of the total sum of squares accounted for by the regression. It is also the square of the correlation coefficient between yield and year of origin. The Adjusted R-squared is the proportional reduction in variance after fitting the regression (i.e.~1-residual variance/total variance). Both these figures give an indication of how effective the regression has been in accounting for the observed variation: a significant regression does not imply that a relationship is particularly important (although it is here). Equally, with very small experiments, large proportions of variation may be accounted for, but the regression is still non-significant. This is generally an indication that you should have designed a larger experiment.

A more conventional display of the regression analysis is given as:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(yield\_year\_regression)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Analysis of Variance Table
## 
## Response: yield
##            Df Sum Sq Mean Sq F value    Pr(>F)    
## year        1 5694.7  5694.7  235.75 < 2.2e-16 ***
## Residuals 374 9034.0    24.2                      
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

This uses the \texttt{anova()} command. To R, anova is the name given to a form of tabular output. Formally, the analysis of variance itself is just a particular type of multiple regression analysis, and that is exactly how R treats it, as we shall see shortly.

We can look at a plot of the data with our fitted line a follows:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(year,yield)}
\FunctionTok{abline}\NormalTok{(yield\_year\_regression)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-82-1.pdf}

When using plot, the name of the variate you want plotted on the x axis comes first. The additional command, \texttt{abline()}, adds the best fitting straight line. There is a lot of variation around the best fitting line, but as we have varieties from three different countries of origin, and we have tested them in three different countries, this is not surprising.

Finally, we look at how to extract residual values and fitted values from a regression. Large residual values for particular observations are often of use in searching for errors in data. Also, identification of the individuals or varieties responsible for large residuals may sometimes suggest some other factor which needs to be considered in the analysis. Fitted values and residuals are extracted as:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{fitted}\NormalTok{(yield\_year\_regression)[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        1        2        3        4        5 
## 94.90825 92.87695 83.73608 86.10594 93.21550
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{resid}\NormalTok{(yield\_year\_regression)[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         1         2         3         4         5 
##  8.321746 -4.326949  7.023919 -3.865936  3.444500
\end{verbatim}

Note I've only printed values 1 to 5 (\texttt{{[}1:5{]}}) to save space. These could be saved to other variables, or plotted:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(}\FunctionTok{fitted}\NormalTok{(yield\_year\_regression), }\FunctionTok{resid}\NormalTok{(yield\_year\_regression))}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-85-1.pdf}

A plot of residuals against fitted values is often informative. If large residuals tend to be associated with large fitted values, for example, this indicates that the error variances are not homogenous and we treat our results with more caution. Residuals which increase in magnitude with increasing fitted values are often an indication that transforming the data to logarithms before analysis may be warranted.

For linear regression in R, however, we can produce four diagnostic plots very easily:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(yield\_year\_regression)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-86-1.pdf} \includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-86-2.pdf} \includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-86-3.pdf} \includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-86-4.pdf}

The first two plots are the most important and the easiest to interpret:

The first plot is of residuals against fitted values as previously described. There is nothing untoward here.

The QQ plot takes the residuals, sorts them and standardises them by dividing them by the standard deviation, so they have a mean of zero and a variance of 1. It is possible to work out what the expected value of these sorted normal deviates should be if the error distribution was truly normal. Plotting the observed against the expected gives you a QQ plot (QQ for quantile-quantile). For well behaved data, the points should lie on a straight line with a gradient of one. The data here look acceptable.

The Scale-Location plot is essentially looking at the same thing as the plot of residuals against fitted values.

The Residuals vs Leverage shows the strength of data points on the parameters of the regression. Data points with very high or low values tend to have more effect (leverage) on the regression equation. This can be quantified. If a data point with high leverage also has a big residual (positive or negative) it is an indication that the data for that observation wants checking for errors, and maybe even deleting. Cook's distance, which is also mentioned in this plot, is a measure of the improvement in error sum of squares if a particular data point were dropped. In the plot above, there is nothing to be checked: the data are good.

\hypertarget{multiple-regression}{%
\subsection{Multiple regression}\label{multiple-regression}}

Multiple regression in R requires little more than simple linear regression. The \texttt{lm()} command is still used. All that is required is to specify a more complex model using the syntax described in the Basic R Syntax section of this guide:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{yield\_mult\_reg}\OtherTok{\textless{}{-}}\FunctionTok{lm}\NormalTok{(yield}\SpecialCharTok{\textasciitilde{}}\NormalTok{year}\SpecialCharTok{+}\NormalTok{ORIGIN}\SpecialCharTok{+}\NormalTok{PpdD1}\SpecialCharTok{+}\NormalTok{Rht2}\SpecialCharTok{+}\NormalTok{FT}\SpecialCharTok{+}\NormalTok{HT}\SpecialCharTok{+}\NormalTok{AWNS)}
\NormalTok{yield\_mult\_reg}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = yield ~ year + ORIGIN + PpdD1 + Rht2 + FT + HT + 
##     AWNS)
## 
## Coefficients:
## (Intercept)         year    ORIGINFRA    ORIGINGBR        PpdD1         Rht2  
##   -460.4045       0.2639      -0.5753       0.9456      -0.2138       2.2100  
##          FT           HT         AWNS  
##      0.2335      -0.1403      -0.4815
\end{verbatim}

Thus yield is predicted as:

-461.2409\\
+ 0.2645 x year\\
+ -0.5637 if French origin\\
+ 0.9510 if UK origin\\
+ -0.2157 x Pppd1\\
+ 2.2112 x Rht2\\
+ 0.2311 x FT\\
+ -0.1402 x HT\\
+ -0.6402 x Awns

Note this form of analysis is acceptable for PpdD1 and Rht2, SNPs and other binary markers provided the markers are coded numerically. 0 and 1 are ideal codes, but 0 can sometimes be confused with a missing value, especially if data are to be analysed in packages other than R. 1, 2 coding is also acceptable. Numeric coding cannot be used at all for multiallelic markers since it implies that alleles coded with a higher number have a higher value than those with a lower number.

Note the order in which the variates are supplied to lm does not affect the estimated coefficients:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lm}\NormalTok{(yield}\SpecialCharTok{\textasciitilde{}}\NormalTok{ year}\SpecialCharTok{+}\NormalTok{ORIGIN}\SpecialCharTok{+}\NormalTok{FT}\SpecialCharTok{+}\NormalTok{Rht2}\SpecialCharTok{+}\NormalTok{PpdD1}\SpecialCharTok{+}\NormalTok{HT}\SpecialCharTok{+}\NormalTok{AWNS)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = yield ~ year + ORIGIN + FT + Rht2 + PpdD1 + HT + 
##     AWNS)
## 
## Coefficients:
## (Intercept)         year    ORIGINFRA    ORIGINGBR           FT         Rht2  
##   -460.4045       0.2639      -0.5753       0.9456       0.2335       2.2100  
##       PpdD1           HT         AWNS  
##     -0.2138      -0.1403      -0.4815
\end{verbatim}

Will give the same answer. However, this is not the case for estimates of significance:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(yield\_mult\_reg)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Analysis of Variance Table
## 
## Response: yield
##            Df Sum Sq Mean Sq  F value    Pr(>F)    
## year        1 5177.9  5177.9 253.1041 < 2.2e-16 ***
## ORIGIN      2  663.2   331.6  16.2102 1.879e-07 ***
## PpdD1       1  121.4   121.4   5.9356  0.015348 *  
## Rht2        1  554.0   554.0  27.0786 3.375e-07 ***
## FT          1   33.0    33.0   1.6122  0.205051    
## HT          1  172.5   172.5   8.4344  0.003921 ** 
## AWNS        1    5.1     5.1   0.2475  0.619158    
## Residuals 342 6996.4    20.5                       
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(}\FunctionTok{lm}\NormalTok{(yield}\SpecialCharTok{\textasciitilde{}}\NormalTok{ year}\SpecialCharTok{+}\NormalTok{ORIGIN}\SpecialCharTok{+}\NormalTok{FT}\SpecialCharTok{+}\NormalTok{Rht2}\SpecialCharTok{+}\NormalTok{PpdD1}\SpecialCharTok{+}\NormalTok{HT}\SpecialCharTok{+}\NormalTok{AWNS))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Analysis of Variance Table
## 
## Response: yield
##            Df Sum Sq Mean Sq  F value    Pr(>F)    
## year        1 5177.9  5177.9 253.1041 < 2.2e-16 ***
## ORIGIN      2  663.2   331.6  16.2102 1.879e-07 ***
## FT          1  164.7   164.7   8.0498  0.004822 ** 
## Rht2        1  543.2   543.2  26.5504 4.349e-07 ***
## PpdD1       1    0.5     0.5   0.0261  0.871836    
## HT          1  172.5   172.5   8.4344  0.003921 ** 
## AWNS        1    5.1     5.1   0.2475  0.619158    
## Residuals 342 6996.4    20.5                       
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

In the two analyses we've swapped the order in which PpdD1 and FT were fitted. In the first anova table, the sum of squares for PpdD1 is 121.4 and 33.0 for FT. In the second analysis, the sum of squares is 0.5 for PpdD1 and 164.7 for FT. The sums of squares for Rht2 has changed a bit but all the others, and most importantly the Residual SS, are unchanged. In unbalanced experiments such as this, where combinations of PpdD1 and height (and most other things too) are not all equally represented, the results from the analysis of variance depend on the order in which the terms are represented in the model. However, there is an easy way to interpret this table. Taking output from the last analysis a line at a time:

The effect for year is fitted first, and is found to be statistically significant:

year \texttt{P\ \textless{}\ 2.2e-16\ ***}

After fitting the year, origin it is fitted next,

ORIGIN \texttt{P\ =\ \ 1.867e-07\ ***}

then FT:

FT \texttt{P\ =\ \ 0.004813\ **}

The p-value of 0.0049 for FT is the significance for FT, \emph{after} accounting for any effect of year and origin. By the time we get to PpdD1, its effect after fitting everything above it in the table is non-significant:

PpdD1 \texttt{P\ =\ 0.871809}

In designed experiments, where different combinations of treatments and factors are usually equally represented, or balanced, the order in which terms are fitted makes no difference; the terms are said to be \emph{orthogonal}. Balance not only has the property of making the terms orthogonal, it also makes the arithmetic very much easier. This was very important before the advent of readily available computers. However, the requirement for balance, solely from the point of view of data analysis, is now no longer required and many contemporary designs for variety trials (for example alpha-designs) are not balanced and would be impossible to analyse without a computer. A readable account of a contemporary approach to experimental design is given in the book Mead, Gilmour and Mead: ``Statistical Principles for the Design of Experiments''.

In this example, the interpretation is clear: PpdD1 is known to be the major locus determining flowering time in wheat, but it doesn't control all the variation. If we had to choose between Ppd or FT in the model, we would chose FT because its sum of squares is bigger: we account for more of the variation in the data. But here it makes sense to fit Ppd first and then to test if there is a residual effect of FT on yield. If you have time, try the same thing for Rht2 and HT.

Our prior knowledge isn't always so good and the interpretation and order of fitting isn't always so clear. Selecting the order in which terms are fitted, and selecting which terms to include in the final model and which to exclude is something of an art, which we shall not develop here. There are formal methods to assist in this process. These too are not covered here, but are available within R. Generally, with genetic analysis, it is usual to account for variation attributable to causes other than genes or markers first, and then fit the genetic effects. This is a conservative approach. If we had followed it here we would not have detected an effect for PpdD1.

Note that \texttt{resid()}, \texttt{fitted()}, \texttt{plot()} and \texttt{summary()} work for multiple regression exactly as for simple linear regression.
Before we leave regression, compare these two simple analyses of the relationship between yield and PpdD1

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(yield}\SpecialCharTok{\textasciitilde{}}\NormalTok{PpdD1,}\AttributeTok{var.equal=}\NormalTok{T)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Two Sample t-test
## 
## data:  yield by PpdD1
## t = 3.0463, df = 374, p-value = 0.002481
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  0.8254243 3.8311401
## sample estimates:
## mean in group 0 mean in group 1 
##        92.35522        90.02694
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(}\FunctionTok{lm}\NormalTok{(yield}\SpecialCharTok{\textasciitilde{}}\NormalTok{ PpdD1))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Analysis of Variance Table
## 
## Response: yield
##            Df  Sum Sq Mean Sq F value   Pr(>F)   
## PpdD1       1   356.6  356.61    9.28 0.002481 **
## Residuals 374 14372.1   38.43                    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

The p-values are identical. For a regression analysis on a single variate with only two values or classes, the two tests are equivalent. In fact:

\(t^2 = F\)

In this case

\(3.0463^2 = 9.280\).

\hypertarget{the-analysis-of-variance}{%
\subsection{The analysis of variance}\label{the-analysis-of-variance}}

In the t-test, we test if the difference between two treatment means is statistically significant. This is a special case of the Analysis of Variance in which we test if differences among multiple treatments are jointly statistically significant. We could compare multiple treatments by carrying out multiple t tests, but this increases the risk that at least one test will be declared significant by chance alone - the so called problem of multiple testing. In addition the interpretation of results becomes increasingly complex. (There is, in fact, an R command that automates this procedure and includes an adjustment for multiple testing: pairwise.t.test). The omnibus test for significance of all means, considered together, that the Analysis of Variance offers is therefore of great value.

The principal of the Analysis of Variance is that, in the absence of any genuine difference among means, the variability among those means can be predicted from the variability from observation to observation within each treatment. This argument is little more than saying that the variance of a mean is just the variance among the observations that contribute to that mean divided by the number of observations contributing to the mean:

\(V\hat{x} = Vx/n\)

We therefore have independent estimates of \(\hat{x}\), one directly from the means, and one from the within treatments variance (/n). If there are no true differences in the means of the treatments, these two estimates of variance are expected to be the same though they will still differ through sampling variation. However, if the means differ, in addition, through differential responses to the treatments we no longer expect them to be the same. Rather:

\(V\hat{x} > Vx/n\)

The test for statistically significant differences among the means is therefore a variance ratio, or F test: the variance among treatment means is divided by the expected variance calculated within treatments. This is an oversimplification: differences in the number of observations within treatments must also be taken into account and with more complicated experimental designs the analysis is also more complicated, but the basic principal remains the same: differences in means inflate the estimate of variance between treatment means compared to the independent estimate of variance from observations within treatments.

We'll start with a simple example. Does height differ significantly between countries of origin?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{boxplot}\NormalTok{(HT}\SpecialCharTok{\textasciitilde{}}\NormalTok{ORIGIN,}\AttributeTok{main=}\StringTok{"Height by country of origin"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-93-1.pdf}

It's clear from this that there is a difference - German lines tend to be taller. The command for an analysis of variance is just like that for linear regression:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(}\FunctionTok{lm}\NormalTok{(HT}\SpecialCharTok{\textasciitilde{}}\NormalTok{ORIGIN))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Analysis of Variance Table
## 
## Response: HT
##            Df  Sum Sq Mean Sq F value    Pr(>F)    
## ORIGIN      2  4274.3 2137.17  47.307 < 2.2e-16 ***
## Residuals 371 16760.6   45.18                      
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

There are three countries of origin: so we have a test with 2 degrees of freedom. That is because if we know a line didn't come from France or Germany, then we know (for this dataset) that it must have come from the UK: there are only two independent countries of origin. We've seen this before: in the earlier multiple regression analysis of yield we included origin and saw that it gave an effect for the UK and an effect for France but not for Germany. The German effect was the default - the mean - to be adjusted by the appropriate effect if a line wasn't German. In fact if we had two extra columns of data, one with zeros everywhere except if a variety was from the UK, when we had a 1, and the second with zeros except if a variety was French, when we had a 1, then including these two columns in the analysis instead of ``origin'' would give exactly the same answer. And if we swapped out the ``UK column'' and inserted a new ``German'' column, we would still get exactly the same answer. Try it if you don't believe me!

To get more information from our analysis of variance, we can use all the methods introduced earlier for linear and multiple regressions: as stated, the analysis of variance is just a special case of multiple regression.

Let's finish off the analysis of variance with something more interesting. We know that the three countries tend to have different requirements for optimum flowering times and height. France tends to require earlier flowering lines and many varieties carry the PpdD1 early allele to achieve this. German lines tend to be taller because German wheat is often treated wheat with a growth regulator which reduces height, so there is less requirement for varieties to be bred for reduced height, commonly by using a dwarfing allele at one of the rht loci. After adjusting HT for country of origin, can we detect the effects of these major phenological loci? Do they interact?

Let's have a look at the data first:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{boxplot}\NormalTok{(HT}\SpecialCharTok{\textasciitilde{}}\NormalTok{PpdD1}\SpecialCharTok{*}\NormalTok{Rht2,}\AttributeTok{main=}\StringTok{"HT by PpdD1 and rht2"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-95-1.pdf}

Note the coding within the command to indicate we wanted to see four classes. 0.0 represents the late Ppd allele and the tall Rht2 allele. 1.0 is the Ppd early and Rht2 tall allele, and so on. It's clear that both loci have a big effect on height, but you can see also that although 1.1 is the shortest class, there is a bigger drop from 0.0 to 0.1 and 1.0 than there is from 1.0 and 0.1 to 1.1

Analysing the data, while taking into account country of origin?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(}\FunctionTok{lm}\NormalTok{(HT}\SpecialCharTok{\textasciitilde{}}\NormalTok{ORIGIN}\SpecialCharTok{+}\NormalTok{Rht2}\SpecialCharTok{*}\NormalTok{PpdD1))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Analysis of Variance Table
## 
## Response: HT
##             Df  Sum Sq Mean Sq  F value    Pr(>F)    
## ORIGIN       2  4141.7  2070.8  68.2521 < 2.2e-16 ***
## Rht2         1  3691.8  3691.8 121.6777 < 2.2e-16 ***
## PpdD1        1  1576.7  1576.7  51.9647 3.575e-12 ***
## Rht2:PpdD1   1   119.8   119.8   3.9501   0.04766 *  
## Residuals  346 10498.0    30.3                       
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

It appears there is some weak evidence that Rht2 and PpdD1are interacting. But we are ignoring other confounding factors such as flowering time itself, and many possible interactions. We can fit a complete model:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(}\FunctionTok{lm}\NormalTok{(HT}\SpecialCharTok{\textasciitilde{}}\NormalTok{ORIGIN}\SpecialCharTok{*}\NormalTok{FT}\SpecialCharTok{*}\NormalTok{Rht2}\SpecialCharTok{*}\NormalTok{PpdD1))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Analysis of Variance Table
## 
## Response: HT
##                 Df Sum Sq Mean Sq  F value    Pr(>F)    
## ORIGIN           2 4141.7  2070.8  76.8732 < 2.2e-16 ***
## FT               1 1049.5  1049.5  38.9589 1.312e-09 ***
## Rht2             1 4681.8  4681.8 173.7978 < 2.2e-16 ***
## PpdD1            1  152.2   152.2   5.6487 0.0180309 *  
## ORIGIN:FT        2  307.4   153.7   5.7049 0.0036625 ** 
## ORIGIN:Rht2      2   76.2    38.1   1.4146 0.2444857    
## FT:Rht2          1  308.9   308.9  11.4673 0.0007928 ***
## ORIGIN:PpdD1     2  133.7    66.8   2.4813 0.0851744 .  
## FT:PpdD1         1   24.3    24.3   0.9025 0.3428055    
## Rht2:PpdD1       1    5.5     5.5   0.2033 0.6523338    
## ORIGIN:FT:Rht2   2  113.2    56.6   2.1016 0.1238698    
## FT:Rht2:PpdD1    1   36.2    36.2   1.3455 0.2469017    
## Residuals      334 8997.4    26.9                       
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\hypertarget{categorical-data---the-chi-squared-test}{%
\section{\texorpdfstring{Categorical data - the \emph{chi}-squared test}{Categorical data - the chi-squared test}}\label{categorical-data---the-chi-squared-test}}

Suppose we wish to test if there is an association between Rht2 and PpdD1. They are not linked, but perhaps breeders have favoured particular combinations of +/- at the two loci. Or an association might arise because of the way the varieties have been selected for inclusion in the panel of lines. The standard method of analysis of such data is the contingency chi-squared test.
First we need to format our data into a table:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rht.ppd.table}\OtherTok{\textless{}{-}}\FunctionTok{table}\NormalTok{(Rht2,PpdD1) }
\NormalTok{rht.ppd.table}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     PpdD1
## Rht2   0   1
##    0 106  53
##    1 170  25
\end{verbatim}

From this table, we can see that the allele frequency of allele 1 at PpdD1 is:

\textbf{(53+25)/(106+170+53+25) or 0.22}

and similarly the allele frequency of allele 1 at Rht2 is 0.55

If these loci are independent of each other in this sample, the allele carried by a variety at Rht2 will be independent of the allele carried by the same variety at PpdD1. In this case, the expected frequency of (Rht2 allele 1 + PpdD1 allele 1) individuals will be 0.22 x 0.55 or 0.12. The predicted number of ``11'' haplotypes is therefore 0.12 x (106+170+53+25) or 42. We have observed 25. This same exercise can be carried out for each of the other three combinations of alleles at the two loci. (These combinations are commonly called haplotypes, though this term strictly only applies if the two loci are linked). If deviations between observed and expected numbers are sufficiently large, we draw the conclusion that genotypes at the two loci are not independent of each other. The statistical test is a chi-squared with 1 degree of freedom, calculated as:

\[\chi^2 = \sum \frac {(O - E)^2}{E}\]
O represents the observed numbers and E the expected.

This chi-squared test is simply carried out as:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{chisq.test}\NormalTok{(rht.ppd.table)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Pearson's Chi-squared test with Yates' continuity correction
## 
## data:  rht.ppd.table
## X-squared = 20.275, df = 1, p-value = 6.706e-06
\end{verbatim}

The result is obviously hugely significant; the p-value is vanishingly small. What might you conclude about how breeders have selected on these two loci?

Chi squared tests can have a problem with low numbers of observations in some cells. Try
this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rht.ppd.table.small}\OtherTok{\textless{}{-}}\FunctionTok{round}\NormalTok{(rht.ppd.table}\SpecialCharTok{/}\DecValTok{10}\NormalTok{)}
\NormalTok{rht.ppd.table.small}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     PpdD1
## Rht2  0  1
##    0 11  5
##    1 17  2
\end{verbatim}

We've created a table 1/10 of the size of the original. \texttt{round()} rounds real numbers to integers in the standard manner (with 1/2 always rounded down).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{chisq.test}\NormalTok{(rht.ppd.table.small)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in chisq.test(rht.ppd.table.small): Chi-squared approximation may be
## incorrect
\end{verbatim}

\begin{verbatim}
## 
##  Pearson's Chi-squared test with Yates' continuity correction
## 
## data:  rht.ppd.table.small
## X-squared = 1.2161, df = 1, p-value = 0.2701
\end{verbatim}

If you run this, you should see this warning message:

\textbf{Chi-squared approximation may be incorrect}

The warning message is given because some cells have expected counts less than 5. Under this threshold, there is a chance that the chi-squared test will give misleading results. Note it is the \emph{expected} count that matters, not the \emph{observed} - which can legitimately be zero and often is when we are considering closely linked SNPs for example, where we may observe only two or three of the four possible haplotypes. There will be more on this later in the course when we come to discuss linkage disequilibrium. We can extract and examine the expected values:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{chisq.test}\NormalTok{(rht.ppd.table.small)}\SpecialCharTok{$}\NormalTok{expected}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in chisq.test(rht.ppd.table.small): Chi-squared approximation may be
## incorrect
\end{verbatim}

\begin{verbatim}
##     PpdD1
## Rht2    0   1
##    0 12.8 3.2
##    1 15.2 3.8
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{chisq.test}\NormalTok{(rht.ppd.table.small)}\SpecialCharTok{$}\NormalTok{observed}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in chisq.test(rht.ppd.table.small): Chi-squared approximation may be
## incorrect
\end{verbatim}

\begin{verbatim}
##     PpdD1
## Rht2  0  1
##    0 11  5
##    1 17  2
\end{verbatim}

The expected count in two cells is less than five. There is an approximate correction, Yates's correction, that takes into account the potential failure of the test statistic to follow a chi-squared distribution when the expected numbers were low. This has been applied automatically. To see results without Yates' correction:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{chisq.test}\NormalTok{(rht.ppd.table.small,}\AttributeTok{correct=}\NormalTok{F)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in chisq.test(rht.ppd.table.small, correct = F): Chi-squared
## approximation may be incorrect
\end{verbatim}

\begin{verbatim}
## 
##  Pearson's Chi-squared test
## 
## data:  rht.ppd.table.small
## X-squared = 2.3314, df = 1, p-value = 0.1268
\end{verbatim}

The \texttt{correct\ =\ F} option turns Yates' correction off and the chi-sq test statistic is larger.

It is increasingly common, and better, to derive the distribution of the test statistic empirically, by repeated randomisation or permutation of the observed data, followed by recalculation of test statistic. The proportion of times the randomised test statistic is greater than or equal to the observed test statistic is then the empirical p-value. Here the randomisation procedure is very simple: the data for Rht2 are randomised over subjects, while the data for PpdD1 are held constant. For a chi-squared test, R automates this procedure:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{chisq.test}\NormalTok{(rht.ppd.table.small,}\AttributeTok{simulate.p.value =}\NormalTok{ T,}\AttributeTok{B=}\DecValTok{10000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Pearson's Chi-squared test with simulated p-value (based on 10000
##  replicates)
## 
## data:  rht.ppd.table.small
## X-squared = 2.3314, df = NA, p-value = 0.2081
\end{verbatim}

The value of B, the number of randomisations, can be set by the user. The default value is 2000, but I've run 10,000 which is still very quick when the number of observations is small.

The contingency chi-squared test will easily accommodate larger tables; a 10 x 10 table for example. In the context of genetic data, this could be used to test for association between genetic regions with multiple haplotype alleles.

\hypertarget{more-useful-information}{%
\section{More useful information}\label{more-useful-information}}

\hypertarget{plotting-graphs}{%
\subsection{Plotting Graphs}\label{plotting-graphs}}

Graphical methods were introduced in context in the sections on summary statistics and on basic statistical analysis. The following methods have been used:

\texttt{hist()} produces a histogram

\texttt{plot()} produces a scatter graph

\texttt{abline()} adds a line of best fit to a scatter graph

\texttt{boxplot()} produces a box-and-whisker plot.

\texttt{pairs()} produces a matrix of scatter plots.

Control of many graphics parameters - font sizes, colours and so on - is given by the command \texttt{par()}. Type \texttt{help(par)} for more details. One useful option is:

\texttt{par(mfrow=c(x,y))}

This will plot graphs successively in a window x graphs wide by y graphs deep. Once the panel of graphs is complete, the next graph will overwrite the whole window and start filling the panel again. For example, the following commands set up a panel two graphs deep by three wide and fills in the first four slots.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{))}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{) }\FunctionTok{plot}\NormalTok{(}\FunctionTok{runif}\NormalTok{(}\DecValTok{100}\NormalTok{),}\FunctionTok{runif}\NormalTok{(}\DecValTok{100}\NormalTok{))}
\FunctionTok{hist}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{1000}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-106-1.pdf}

As you can see, the graphs need not all be of the same type. The related version of the command \texttt{par(mfcol=c(x,y))} will produce a graphical window of the same dimensions but fill it in column order rather than in row order.

A benefit of RStudio over native R is that graphs are saved and you can scroll back and forwards over previous efforts and export them, either as files or to the clipboard. In native R, the export of graphs is no more complicated, but they are overwritten so you cannot scroll to inspect previous plots but must recreate them.

Although R generates graphs very quickly and simply, the labeling and formatting are often not ideal. To alter graph titles, include the options:

\texttt{main="Main\ title"}

\texttt{sub="subtitle"}

\texttt{xlab="x-label"}

\texttt{ylab="y-label"}

For example:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#switch back to plotting a single plot}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\CommentTok{\#make plot}
\FunctionTok{plot}\NormalTok{(year,yield,}\AttributeTok{main=}\StringTok{"TG panel yields over time"}\NormalTok{,}\AttributeTok{sub=}\StringTok{"example scatterplot"}\NormalTok{,}\AttributeTok{ylab=}\StringTok{"yield in quintal / ha"}\NormalTok{,}\AttributeTok{xlab=}\StringTok{"year first in trial"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-107-1.pdf}

We have only used the basic plotting commands which come with the core R package. Very highly rated, and increasingly used is a package called ``ggplot2'' \url{http://ggplot2.org/}. This is generally included as part of the tidyverse too.There is no time go into this in detail. Select ggplot2 from the ``Packages'' tab in the bottom right window to active the package. If not installed, click on the ``Packages'' tab in the bottom right window, then click ``install'' and enter ggplot2 as the package you wish to install. Alternatively, you can run this line: \texttt{install.packages("ggplot2")}, then load the package from your library:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"ggplot2"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Then:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data=}\NormalTok{TG\_data\_for\_day\_1,}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{year,}\AttributeTok{y=}\NormalTok{yield)) }\SpecialCharTok{+} \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{shape=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-109-1.pdf}

\texttt{aes} stands for aesthetics. If you think this look better and is easier, then ggplot2 is for you. See the web site for more details.

There's plenty more you can do with ggplot2. There are some good \href{https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf}{cheat sheets} available too.

\begin{rmdnote}
There are a number of other very useful cheatsheets for R and RStudio, you can take a look at many \href{https://rstudio.com/resources/cheatsheets/}{here.} It is always useful to have these handy when you are travelling through the R universe.
\end{rmdnote}

\hypertarget{probability-distributions}{%
\subsection{Probability distributions}\label{probability-distributions}}

In this section we describe how to use R to look up the p-value associated with the most
commonly used test statistics and how to look up test statistics associated with p-values.

Firstly the p-value associated with chi-squared:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pchisq}\NormalTok{(}\FloatTok{3.84}\NormalTok{,}\DecValTok{1}\NormalTok{,}\AttributeTok{lower.tail=}\NormalTok{F)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.05004352
\end{verbatim}

The parameters given to \texttt{pchisq}, in order, are:

\texttt{3.84} value of the test statistic.

\texttt{1} the degrees of freedom

\texttt{lower.tail} If set to \texttt{T} (the default) the result is the cumulative distribution up to the value of the test statistic \texttt{-\ 0.95} in the example. For significance testing we require the area of the upper tail: 1-0.95 and so set \texttt{lower.tail=F}.

To calculate a chi squared from a p-value we use the command qchisq:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qchisq}\NormalTok{(}\FloatTok{0.05}\NormalTok{,}\DecValTok{1}\NormalTok{,}\AttributeTok{lower.tail=}\NormalTok{F)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3.841459
\end{verbatim}

The syntax is identical to that for \texttt{pchisq()}, so we are required to include \texttt{lower.tail=F}.
Examples for the F distribution are shown below.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pf}\NormalTok{(}\FloatTok{3.84}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1000}\NormalTok{,}\AttributeTok{lower.tail=}\NormalTok{F)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.05032099
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qf}\NormalTok{(}\FloatTok{0.05}\NormalTok{,}\DecValTok{1}\NormalTok{,}\DecValTok{1000}\NormalTok{,}\AttributeTok{lower.tail=}\NormalTok{F)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3.850775
\end{verbatim}

The examples here are for 1 degree of freedom for the numerator and 1000 degrees of freedom for the denominator. The results are identical to those for a chi-squared test with 1 df. In fact, a chi-squared test with n degrees of freedom is identical to an F test with n degrees of freedom in the numerator and a very large number of degrees of freedom (ideally infinite) in the denominator.

Values for probabilities associated with a normal distribution are:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pnorm}\NormalTok{(}\FloatTok{1.96}\NormalTok{,}\AttributeTok{lower.tail=}\NormalTok{F)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.0249979
\end{verbatim}

This probability is for a standardised normal distribution: with a mean of zero and a variance of 1. The probability is for a single tail of the distribution. Generally, we would require the result for a two tailed test - the probability of values higher than 1.96 and lower than -1.96. This probability is just double that for a single tail: 0.05 in this case. Again, this is the same value as for chi-squared with 1 degree of freedom. If a variate has a standardised normal distribution, the variate squared has a chi-squared distribution with 1 degree of freedom:

\(1.96^2 = -1.96^2 = 3.84\)

Probabilities associated with normally distributed variables with different means and variances are produced by specifying the mean and variance.

\texttt{pnorm(q,mean=x,sd=y)} with the inverse function: \texttt{qnorm(p,mean=x,sd=y)}

\texttt{lower.tail=F} can be added if required. The values for mean and standard deviation are now user specified (substitute for x and y in pnorm and qnorm). The default values are 0 and 1.

Finally, the t-test:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pt}\NormalTok{(}\FloatTok{1.96}\NormalTok{,}\DecValTok{1000}\NormalTok{,}\AttributeTok{lower.tail=}\NormalTok{F) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.02513659
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pt}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{1.96}\NormalTok{,}\DecValTok{1000}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.02513659
\end{verbatim}

We usually carry out a two sided t test - so we require the sum of the lower and upper tail probabilities: equal to two times the single tailed probability. The inverse function follows the usual format and nomenclature:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qt}\NormalTok{(}\FloatTok{0.025}\NormalTok{,}\DecValTok{10000}\NormalTok{,}\AttributeTok{lower.tail=}\NormalTok{F)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.960201
\end{verbatim}

\hypertarget{random-number-generation}{%
\subsection{Random number generation}\label{random-number-generation}}

R has excellent random number generators for several distributions:

\texttt{rnorm(n,\ mean=0,\ sd=1)}

\texttt{runif(n,\ min=0,\ max=1)}

\texttt{rchisq(n,\ df,\ ncp=0)}

\texttt{rt(n,\ df,\ ncp)}

\texttt{rf(n,\ df1,\ df2,\ ncp)}
and there are others. These are self explanatory except for \texttt{ncp}. This is the non-centrality parameter. For most use, this will be zero. It is used in power calculations to provide the distribution of the test statistic under an alternative hypothesis. The \texttt{ncp} is the value you would get if you plug values of parameters or observations you expect under the alternative hypothesis into the formula for the test. For example, with a t test, the value under the null hypothesis is zero. If you expect the true difference between your means to be three, then this is the \texttt{ncp}. For a chi-squared test, you would create expected numbers from expected proportions from an alternative hypothesis and calculate a chi-squared value which will be the \texttt{ncp} for power calculations.

\hypertarget{loops-second-example}{%
\subsection{\texorpdfstring{Loops: \emph{second example}}{Loops: second example}}\label{loops-second-example}}

A block of commands, each on a separate line, can be saved in a file and later pasted into R where it will execute (i.e.~run as a program). There is, of course, much more to programming in R than this, but this is enough to get you started. The most common requirement is to repeat an operation multiple times. Below is the format for a simple loop which can be easily edited to fit your own purposes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{no\_times}\OtherTok{\textless{}{-}}\DecValTok{100000}
\NormalTok{diff}\OtherTok{\textless{}{-}}\ConstantTok{NA}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{no\_times) \{}
\NormalTok{diff[i]}\OtherTok{\textless{}{-}}\FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{)}\SpecialCharTok{{-}}\FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

This code plots the distribution of the difference in value between a pair of uniformly distributed values. You may prefer to reduce the number from 100,000. If you cut and paste the code into R you should get something like this:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(diff,}\AttributeTok{breaks=}\DecValTok{1000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-120-1.pdf}

Note that the beginning and end of the loop (or any R program block) is defined by curly brackets. The indentation isn't obligatory but is good programming practice. The program creates a vector, \texttt{diff} which is then indexed by \texttt{i}. If \texttt{diff} wasn't first created outside the loop, the program would fail.

Loops in R can run slowly. Often, they can be avoided by the use of the command \texttt{apply()}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{apply}\NormalTok{(TG\_data\_for\_day\_1,}\DecValTok{2}\NormalTok{,sd)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in var(if (is.vector(x) || is.factor(x)) x else as.double(x), na.rm =
## na.rm): NAs introduced by coercion

## Warning in var(if (is.vector(x) || is.factor(x)) x else as.double(x), na.rm =
## na.rm): NAs introduced by coercion
\end{verbatim}

\begin{verbatim}
##     variety        year      ORIGIN        Rht2       PpdD1       yield 
##          NA  11.5105231          NA          NA   0.4188383   6.2670999 
## CALLOW_2011 FRANCE_2010 FRANCE_2011    LGE_2010    LGE_2011   NIAB_2011 
##   7.5343555   7.6016464   8.0943785   6.7535158   7.2916383   5.1220707 
##          FT          HT        AWNS 
##   3.2931899   7.5069727          NA
\end{verbatim}

The first argument to apply is the dataset or matrix we want to work on, the second is whether we want to loop over rows (1), or columns (2 - as in this example). The third is the function you wish to loop over. So here we loop over columns to calculate the standard deviation of each.

The missing data for Rht2 has caused the standard deviation to be returned as NA. We need to include \texttt{na.rm=T} as an option. Unfortunately, there is no way I know to do this within apply. Instead we need to define our own function (which also allows us to look at how to define functions in R in general - another useful lesson in learning to program in R).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sd.na.rm.is.t}\OtherTok{\textless{}{-}}\ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{sd}\NormalTok{(x,}\AttributeTok{na.rm=}\NormalTok{T)}
\end{Highlighting}
\end{Shaded}

\texttt{function(x)} defines a set of arguments that our new function \texttt{sd.na.rm.is.t} requires. Here we only need one argument; the name of the dataset, which will be equated to x when we run the function. Following that, on the same line, is the set of commands we want to run. Here we are running an inbuilt function, \texttt{sd(x,na.rm=T)},which will take the data vector x as the source of data.

Try:
\texttt{sd(AWNS)}
\texttt{sd(AWNS,na.rm=T)}
\texttt{sd.na.rm.is.t(AWNS)}

So now:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{apply}\NormalTok{(TG\_data\_for\_day\_1,}\DecValTok{2}\NormalTok{,sd.na.rm.is.t)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in var(if (is.vector(x) || is.factor(x)) x else as.double(x), na.rm =
## na.rm): NAs introduced by coercion

## Warning in var(if (is.vector(x) || is.factor(x)) x else as.double(x), na.rm =
## na.rm): NAs introduced by coercion
\end{verbatim}

\begin{verbatim}
##     variety        year      ORIGIN        Rht2       PpdD1       yield 
##          NA  11.5105231          NA   0.4981119   0.4188383   6.2670999 
## CALLOW_2011 FRANCE_2010 FRANCE_2011    LGE_2010    LGE_2011   NIAB_2011 
##   7.5343555   7.6016464   8.0943785   6.7535158   7.2916383   5.1220707 
##          FT          HT        AWNS 
##   3.2931899   7.5069727   0.2716556
\end{verbatim}

This returns the desired standard deviation of each column of data. We still get NA's and warnings for attempting to calculate the standard deviation for non-numeric data (for Variety name and country of origin). We could specify which columns of \texttt{TG\_data\_for\_day\_1} to work on to avoid this.

\texttt{apply(TG\_data\_for\_day\_1{[},-1:-3{]},2,sd.na.rm.is.t)}

The \texttt{{[},-1:-3{]}} informs R that we need all rows but to drop the first three columns.
This is long winded, but functions are frequently longer sequences of commands rather than just one as in our example. The commands for a long function, which will run over many lines are bracketed with \texttt{\{} and \texttt{\}}, just as in loops using \texttt{for}.

\hypertarget{miscellany}{%
\subsection{Miscellany}\label{miscellany}}

Included here are some useful commands which have not so far been described and some less frequently used commands which come in handy from time to time.

\texttt{attributes(object)}

Gives information on the object which is often to help you work out what you've done.

\texttt{cbind(datasetA)} and \texttt{rbind(datasetB)}

Joins up two datasets by rows or columns. For example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{temp}\OtherTok{\textless{}{-}}\NormalTok{TG\_data\_for\_day\_1[,}\DecValTok{1}\NormalTok{] [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{]}
\FunctionTok{cbind}\NormalTok{(temp,TG\_data\_for\_day\_1[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{]  )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       temp  variety year ORIGIN
## 1   AARDEN   AARDEN 2003    DEU
## 2 AARDVARK AARDVARK 1997    GBR
## 3    ABELE    ABELE 1970    GBR
## 4      ABO      ABO 1977    FRA
## 5   ACCESS   ACCESS 1998    GBR
\end{verbatim}

We have created a subset of the data with two copies of the variety names. I've limited this to the first 5 rows to save paper. There is no particular reason we would want to do this, it's just as a demonstration.

Next, we create two copies of the first three columns of the data using rbind:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{temp}\OtherTok{\textless{}{-}}\NormalTok{TG\_data\_for\_day\_1[,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{]}
\NormalTok{temp2}\OtherTok{\textless{}{-}}\FunctionTok{rbind}\NormalTok{(temp,TG\_data\_for\_day\_1[,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{])}
\FunctionTok{head}\NormalTok{(temp2[}\FunctionTok{order}\NormalTok{(temp2}\SpecialCharTok{$}\NormalTok{variety),])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      variety year ORIGIN
## 1     AARDEN 2003    DEU
## 377   AARDEN 2003    DEU
## 2   AARDVARK 1997    GBR
## 378 AARDVARK 1997    GBR
## 3      ABELE 1970    GBR
## 379    ABELE 1970    GBR
\end{verbatim}

Note, that in ordering the data we have used \texttt{order(temp2\$variety)} rather than \texttt{order(variety)} because we require the version of ``variety''" associated with the new structure temp2, not the version of variety that we attached earlier.

\texttt{dim(x)}

Retrieve the dimensions of an array or dataset.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dim}\NormalTok{(TG\_data\_for\_day\_1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 376  15
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dim}\NormalTok{(temp2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 752   3
\end{verbatim}

\texttt{help(command)}

This opens a new window and provides help on the command. help(lm) for example, will give help on the linear modelling command that we have used extensively. The help is written in a terse and technical style however, which may be hard to understand at first but you get used to it. It is useful to see what options are available with each command - for many of the commands used in this guide more are available than have been described. Often, sufficient of the output from help makes sense to be able to get a command working by trial and error. At the bottom of the output, there are often examples of the command's use: again not always easy to follow. In Rstudio, help is available (in the same unhelpful style) directly from the windows.

\texttt{history(x)}

In native R, this opens up a window with a list of the most recently issued x commands. The default number is 25. These can be copied back into the R window and re-executed. The window with the output can be saved from the File menu to keep a record of commands issued during the R session. In RStudio, the history of commands is available directly from a tab in the top right hand window.c

\texttt{ls()\ \ or\ equivalently\ objects()}

Lists all the variables available in the current R session. This command can also be executed from the menu in native R, selecting first ``Misc'', then ``List objects''. In RStudio it is displayed using the Environment tab in the top right hand window.

\texttt{matrix(data,\ nrow\ =\ x,ncol\ =y)}

Defines a matrix and fills it with data. For example:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{matrix}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{12}\NormalTok{),}\AttributeTok{nrow=}\DecValTok{3}\NormalTok{, }\AttributeTok{ncol=}\DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            [,1]       [,2]       [,3]       [,4]
## [1,] -1.1997940  0.6565469 -1.2640155 -0.1001170
## [2,] -0.3932316  1.0922818 -0.7344448  1.3312798
## [3,] -0.4301833 -1.0324728  0.5145625  0.7772152
\end{verbatim}

The \texttt{rnorm()} part has simulated 12 random numbers which have a specified normal distribution.

Note that there is no need to define the column number; it is fixed by the data and by the row number. More commonly, we would want to name the matrix. By default, all data are initially recorded as missing:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{matrix}\NormalTok{(}\AttributeTok{nrow=}\DecValTok{3}\NormalTok{,}\AttributeTok{ncol=}\DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1] [,2] [,3] [,4]
## [1,]   NA   NA   NA   NA
## [2,]   NA   NA   NA   NA
## [3,]   NA   NA   NA   NA
\end{verbatim}

\texttt{rep(x,y)}

Generates a repeated list:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{,}\DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 1 2 3 1 2 3 1 2 3 1 2 3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rep}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}apples\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}pears\textquotesingle{}}\NormalTok{),}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "apples" "pears"  "apples" "pears"
\end{verbatim}

Remove an object:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rm}\NormalTok{(harvest\_index)}
\end{Highlighting}
\end{Shaded}

Remove those objects that are no longer required.

\texttt{rm(list=ls(all.names=TRUE))} Would removes all variables, although this can be more easily achieved from the Session tab which offers more options.

Pull row names from an object (and only show first 5):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rownames}\NormalTok{(TG\_data\_for\_day\_1)  [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "1" "2" "3" "4" "5"
\end{verbatim}

Pull column names:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{colnames}\NormalTok{(TG\_data\_for\_day\_1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "variety"     "year"        "ORIGIN"      "Rht2"        "PpdD1"      
##  [6] "yield"       "CALLOW_2011" "FRANCE_2010" "FRANCE_2011" "LGE_2010"   
## [11] "LGE_2011"    "NIAB_2011"   "FT"          "HT"          "AWNS"
\end{verbatim}

You can also write new names:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rownames}\NormalTok{(TG\_data\_for\_day\_1)[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{]}\OtherTok{\textless{}{-}}\FunctionTok{c}\NormalTok{(}\StringTok{"a"}\NormalTok{,}\StringTok{"b"}\NormalTok{,}\StringTok{"c"}\NormalTok{)}
\FunctionTok{rownames}\NormalTok{(TG\_data\_for\_day\_1)[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "a"  "b"  "c"  "4"  "5"  "6"  "7"  "8"  "9"  "10"
\end{verbatim}

\texttt{seq(x)}

Generates a sequential list.

\begin{verbatim}
`seq(10)`                 produces a list from 1:10. 
`seq(10,12,0.25)`       produces a list from 10 to 12 in steps of 1/4
\end{verbatim}

\texttt{sink(filename)}

Sends output to the given file rather than to the console. \texttt{sink()} with no filename will turn off the previous redirection, Useful if you are expecting lots of output. You can issue this before the command and then turn it off when the command has completed.

\texttt{source(filename)}

Runs commands from the filename - given as a variable containing the name or as the name itself in quotes.

\texttt{split(x,y)}

Partitions a variable or whole dataframe \texttt{x} into separate variables or dataframes on the basis of matches to \texttt{y}. Give this a go using:

\texttt{split(yield,ORIGIN)\ {[}1:2{]}}

In my hands, the following convoluted code will split our data into three separate data frames.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{DEU}\OtherTok{\textless{}{-}}\FunctionTok{data.frame}\NormalTok{(}\FunctionTok{split}\NormalTok{(TG\_data\_for\_day\_1,ORIGIN)[}\DecValTok{1}\NormalTok{])}
\NormalTok{FRA}\OtherTok{\textless{}{-}}\FunctionTok{data.frame}\NormalTok{(}\FunctionTok{split}\NormalTok{(TG\_data\_for\_day\_1, ORIGIN)[}\DecValTok{2}\NormalTok{])}
\NormalTok{UK}\OtherTok{\textless{}{-}}\FunctionTok{data.frame}\NormalTok{(}\FunctionTok{split}\NormalTok{(TG\_data\_for\_day\_1, ORIGIN)[}\DecValTok{3}\NormalTok{])}
\FunctionTok{dim}\NormalTok{(DEU)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 89 15
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dim}\NormalTok{(FRA)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 212  15
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dim}\NormalTok{(UK)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 73 15
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dim}\NormalTok{(TG\_data\_for\_day\_1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 376  15
\end{verbatim}

The row numbers don't add up because for some countries the origin was \texttt{NA}.

\hypertarget{my-number-one-piece-of-advice}{%
\subsection{My number one piece of advice}\label{my-number-one-piece-of-advice}}

If you have reached this stage of the tutorial, no doubt you have taken on board a lot of information! It's inconceivable to remember all that information. While you can refer back to this tutorial whenever you need to for help, I find the best method is usually just to \emph{`google it'}.

Whether you are a first time user, an intermediate user, or professional software developer, this is the universal solution to most programming/coding problems.

In terms of R issues, you can normally just google your problem in the following format. Let's say we wanted to change the position of a plot title for a figure we were forming for a publication/presentation, I would search for:

\emph{``How to change position of plot title in r''}

The first hit for me is \href{https://stackoverflow.com/questions/20355410/adjust-plot-title-main-position}{this}. That link shows someone asking my question and some fantastic solutions to solving the problem.

The websites \href{https://stackoverflow.com/}{stackoverflow} and \href{https://stackexchange.com/}{stackexchange} are typically the best options and have helped me a great deal. Normally, the question you are asking has been already addressed on these websites and users have provided more solutions than you need for the question at hand. I would suggest getting to grips with the format and etiquette of these website communities before posting your own questions. The moderators/users like the questions to be asked in a certain way and can be slightly hostile if you don't meet their criteria. Then your question and their slightly rude responses remain on the internet for everyone to see. I'm speaking from past experience here!

There are some good R reddit communities that accept all type of questions (e.g.~\href{https://www.reddit.com/r/Rlanguage/}{try this one}).

\hypertarget{learn-more}{%
\subsection{Learn more}\label{learn-more}}

Much useful information and documentation is available on the \href{http://www.r-project.org/}{R web site}, including the R manual ``An Introduction to R.''

\begin{rmdnote}
The manuals, including ``An introduction to R'', are available directly from the Help menu from with R.
\end{rmdnote}

The `Introductory Statistics with R' book \citep{dalgaard_introductory_2008} is an excellent introduction both to R and to statistical analysis, with many simple examples.

R in a Nutshell \citep{adler_r_2010}, by Joseph Adler in the Nutshell series from O'Reilly gives a lot of detail about what and how R does what it does. An excellent reference book, but not for beginners.

\hypertarget{packages}{%
\subsection{Packages}\label{packages}}

Many are available from the Comprehensive R Archive network (CRAN) web site \url{http://cran.r-project.org/}. They can be installed directly from this site, or downloaded as zip files and installed later (useful if you need to install onto multiple machines). We shall be using several of these later.

These packages generally come with their own manual, often detailed. That for the R/QTL package, for example, runs to 96 pages. Although the CRAN website is the first place to search for suitable packages, they are also found elsewhere and are often referred to in methodological publications or the methods sections of paper: programming in R is an expanding industry. The more popular packages (R/QTL again) may have books published about them and often course material and lecture notes can be found on the web.

\begin{rmdwarning}
It is worth reiterating that our guide to the syntax and structures used in R has been very superficial. We have mentioned the data frame and little else. Knowledge of other structures - arrays, matrices, lists - should be acquired at some stage. They are explained in ``An Introduction to R'' but this book is not a page turner.
\end{rmdwarning}

\hypertarget{the-tidyverse}{%
\subsection{The Tidyverse}\label{the-tidyverse}}

If you are new to R, it is worth putting the effort into learning about this. The website is: \url{https://www.tidyverse.org/}. The book ``R for Data Science'' \citep{wickham_r_2016}, which is free online from this website is a very good introduction, including to ggplot.

\hypertarget{other-resources}{%
\subsection{Other resources}\label{other-resources}}

For the ``Understanding data types and R objects'' section in this tutorial, inspiration was taken from \url{https://datacarpentry.org/R-ecology-lesson/}. This is an excellent resource and worth checking out. Additionally, formal definitions were taken from the book ``R in Action: Data Analysis and Graphics with R'' \citep{kabacoff_r_2011}.

\hypertarget{list-of-commands-mainly-described-in-this-guide.}{%
\section{List of commands, mainly described in this guide.}\label{list-of-commands-mainly-described-in-this-guide.}}

\hypertarget{general}{%
\subsubsection{General}\label{general}}

\texttt{apply} = fast way of looping over rows and columns

\texttt{attach} = attaches a dataset to R for subsequent analyses

\texttt{attributes} = get information on the attributes of an object

\texttt{cbind} = concatenate two tables or vectors by columns

\texttt{colnames} = adds column names to a table or reads them from a table

\texttt{detach} = attaches a dataset to R for subsequent analyses

\texttt{demo} = demonstration a command (not available for most commands)

\texttt{dim} = retrieve the dimensions of an object

\texttt{factor} = convert a variable or text into a factor

\texttt{for} = loop over a a set of commands with different input values

\texttt{function} = define a function which can then be run repeatedly

\texttt{getwd} = returns the path to the working directory

\texttt{help} = returns help on a command

\texttt{history} = lists previously issued commands

\texttt{install.packages} = select a package to install

\texttt{is.na} = returns TRUE if a value is NA.

\texttt{length} = returns the number of entries in a variate

\texttt{ls} = lists all active data structures and variates

\texttt{matrix} = defines a matrix and fills it with data.

\texttt{order} = returns the order or a variate for use in a subsequent sort

\texttt{quit} = exit R

\texttt{rbind} = concatenate two tables or vectors by rows

\texttt{read.csv} = reads in data from .csv file

\texttt{read.table} = reads in data

\texttt{rep} = generates a list with repeating elements

\texttt{rm} = delete data structures and variates from R

\texttt{rownames} = adds row names to a table or reads them from a table

\texttt{searchpaths} = lists attached packages and datasets in the order in which
searched.

\texttt{seq} = creates a sequence of values

\texttt{sink} = writes output to a file

\texttt{sort} = sorts data

\texttt{source} = reads commands from a file

\texttt{split} = splits data on a factor value

\texttt{subset} = selects a subset of data for subsequent analysis

\texttt{table} = defines a table: used for input into contingency chi sq tests.

\hypertarget{graphical}{%
\subsubsection{Graphical}\label{graphical}}

\texttt{abline} = add the best fitting straight line to a scattergram

\texttt{boxplot} = produce a Box-and-whisker plot

\texttt{heatmap} = plots a heatmap and dendogram

\texttt{curve} = add a function to a graph

\texttt{hist} = plot a histogram

\texttt{image} = colours-in a matrix according to its values.

\texttt{pairs} = plot multiple scattergrams in a matrix format

\texttt{par} = set graphical parameters

\texttt{pdf} = writes graphical output to a pdf file

\texttt{plot} = produce a scattergram

\hypertarget{statistics}{%
\subsubsection{Statistics}\label{statistics}}

\texttt{anova} = return an anova table from a linear model

\texttt{apply} = applies a function to rows and/or columns of a matrix

\texttt{chisq.test} = carry out a contingency chi-squared test

\texttt{colMeans} = returns means across columns of a dataset

\texttt{colSums} = returns totals across columns of a dataset

\texttt{cor} = returns the correlation coefficient

\texttt{fitted} = returns the fitted values from a linear model

\texttt{lm} = define and execute a linear model

\texttt{mean} = returns the mean of a variate

\texttt{median} = returns the median of a variate

\texttt{minimum} = returns the minimum of a variate

\texttt{maximum} = returns the maximum of a variate

\texttt{pchisq} = returns the p-value of a chi-squared statistic

\texttt{pf} = returns the p-value of a F (variance ratio) statistic

\texttt{pnorm} = returns the p-value for a normally distributed variate

\texttt{pt} = returns the p-value of a t-test

\texttt{qchisq} = returns a chi-squared statistic for a given probability

\texttt{qnorm} = returns a normality distributed variate for a given probability

\texttt{quantile} = returns the quantiles of a variate

\texttt{qf} = returns a F (variance ratio) statistic for a given probability

\texttt{qt} = returns the t-test statistic for a given probability

\texttt{resid} = returns the residuals from a linear model

\texttt{rchisq} = returns random numbers from the chisq distribution

\texttt{rf} = returns random numbers from the F distribution

\texttt{rnorm} = returns random numbers from the normal distribution

\texttt{rowMeans} = returns means across rows of a dataset

\texttt{rowSums} = returns totals across rows of a dataset

\texttt{runif} = returns random numbers from the uniform distribution

\texttt{rt} = returns random numbers from the t distribution

\texttt{sd} = returns the standard deviation of a variate

\texttt{sum} = returns the sum of a variate

\texttt{summary} = summarise data

\texttt{t.test} = one and two sample t-test

\texttt{var} = returns the mean of a variate

\texttt{var.test} = compare two variances by an F test

\hypertarget{Trial-design}{%
\chapter{Trial Design.}\label{Trial-design}}

\hypertarget{randomised-complete-block-designs}{%
\section{Randomised complete block designs}\label{randomised-complete-block-designs}}

Firstly, we shall use Excel to create a randomised complete block design for 10 varieties in 6 replicates. Start a new workbook. You will need three column headings: ``replicate'', ``variety code'', and ``random no''. First, we need to enter the trial design:

In the ``replicate'' column, enter 1 in the first 10 rows, 2 in the second 10 rows \ldots. 6 in rows 52-61.

In the ``variety'' column, enter 1\ldots10 in the first 10 rows, 1\ldots10 in the second 10 rows\ldots\ldots.1 to 10 in rows 52-61.

Next, we want to randomise the position of the varieties within replicates; each replicate must end up still containing every variety once only.

In the ``random no'' column, enter ``=rand()'' in each of the 60 rows below the header. Sort on replicate, then on random number within replicate.

Job done!

\hypertarget{trial-design-in-r}{%
\section{Trial design in R}\label{trial-design-in-r}}

We will use the R package \texttt{blocksdesign}\citep{R-blocksdesign} to create partially replicated and other designs. This package originated as a stand-alone program and website, a mirror of which was hosted at NIAB, but the developer, Rodney Edmondson, now only maintains the R version.

To start this tutorial, open a new RStudio screen and start a new script window. If you don't have the package already in your library you can install it by running:

\texttt{install.packages("blocksdesign")}

Regardless of if you've got the package installed in your library already, you will still need to load it to your current working session. You can do this with:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(blocksdesign)}
\end{Highlighting}
\end{Shaded}

I always start my scripts with a list of library commands, stating the packages I will want to use in each analysis.

The command blocks is used to create designs:

\texttt{blocks(treatments,\ replicates,\ blocks\ =\ NULL,\ searches\ =\ NULL,\ seed\ =\ NULL,\ jumps\ =\ 1)}

\begin{itemize}
\tightlist
\item
  \texttt{treatments} is the number of varieties
\item
  \texttt{replicates} is the number of replicates
\item
  \texttt{blocks} defines the number of (incomplete) row blocks
\end{itemize}

The other options can be ignored for now, and omitted. \texttt{blocks} can be ignored too, for simple designs:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{blocks}\NormalTok{(}\AttributeTok{treatments=}\DecValTok{10}\NormalTok{, }\AttributeTok{replicates =} \DecValTok{6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $Replication
##    Treatments Freq
## 1           1    6
## 2           2    6
## 3           3    6
## 4           4    6
## 5           5    6
## 6           6    6
## 7           7    6
## 8           8    6
## 9           9    6
## 10         10    6
## 
## $Blocks_model
##   Level Blocks D-Efficiency A-Efficiency A-Bound
## 1     1      1            1            1       1
## 
## $Design
##    Level_1 plots treatments
## 1       B1     1          7
## 2       B1     2          2
## 3       B1     3          4
## 4       B1     4          8
## 5       B1     5          2
## 6       B1     6          3
## 7       B1     7          5
## 8       B1     8          9
## 9       B1     9          8
## 10      B1    10          8
## 11      B1    11         10
## 12      B1    12          7
## 13      B1    13          6
## 14      B1    14         10
## 15      B1    15          5
## 16      B1    16          9
## 17      B1    17          3
## 18      B1    18          9
## 19      B1    19          8
## 20      B1    20          3
## 21      B1    21          3
## 22      B1    22          1
## 23      B1    23          9
## 24      B1    24         10
## 25      B1    25          7
## 26      B1    26          7
## 27      B1    27          9
## 28      B1    28         10
## 29      B1    29          8
## 30      B1    30          7
## 31      B1    31          6
## 32      B1    32          5
## 33      B1    33         10
## 34      B1    34          5
## 35      B1    35          3
## 36      B1    36          4
## 37      B1    37          8
## 38      B1    38          2
## 39      B1    39          1
## 40      B1    40          5
## 41      B1    41          9
## 42      B1    42          5
## 43      B1    43          1
## 44      B1    44          2
## 45      B1    45          1
## 46      B1    46          2
## 47      B1    47          4
## 48      B1    48          6
## 49      B1    49          2
## 50      B1    50          3
## 51      B1    51          6
## 52      B1    52         10
## 53      B1    53          4
## 54      B1    54          1
## 55      B1    55          7
## 56      B1    56          4
## 57      B1    57          6
## 58      B1    58          1
## 59      B1    59          6
## 60      B1    60          4
## 
## $Plan
##   Level_1 Blocks.Plots: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
## 1      B1               7 2 4 8 2 3 5 9 8  8 10  7  6 10  5  9  3  9  8  3  3
##   22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47
## 1  1  9 10  7  7  9 10  8  7  6  5 10  5  3  4  8  2  1  5  9  5  1  2  1  2  4
##   48 49 50 51 52 53 54 55 56 57 58 59 60
## 1  6  2  3  6 10  4  1  7  4  6  1  6  4
## 
## $seed
## NULL
## 
## $searches
## [1] 84
## 
## $jumps
## [1] 1
\end{verbatim}

This will produce a version of the randomised complete block design design we produced above. That was much faster than the excel approach! It's better for us to save the result. We will save it as \texttt{RCB.save}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{RCB.save}\OtherTok{\textless{}{-}} \FunctionTok{blocks}\NormalTok{(}\AttributeTok{treatments=}\DecValTok{10}\NormalTok{, }\AttributeTok{replicates =} \DecValTok{6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The package saves the results in a list object. You can determine what type of object you've saved by running:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(RCB.save)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "list"
\end{verbatim}

In this case, you can use \texttt{\$} to access the useful output you saw printed when we ran the command before saving.

\begin{rmdquiz}
\textbf{1} Try running these on your own computer:

\texttt{RCB.save\$Treatments}

\texttt{RCB.save\$Design}

\texttt{RCB.save\$Plan}
\end{rmdquiz}

\hypertarget{partially-replicated-designs}{%
\subsection{Partially replicated designs}\label{partially-replicated-designs}}

P-rep (partially replicated) designs in which not all entries are replicated, and augmented designs in which a small number of control varieties may be highly replicated but candidate entries are not, are useful in the early stages of plant breeding when only limited seed is available. We still require some replication both to provide an estimate of error and to give some local control of fertility effects through blocking and/or spatial analysis. The \texttt{blocksdesign} package can create these.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p.rep}\FloatTok{.1}\OtherTok{\textless{}{-}} \FunctionTok{blocks}\NormalTok{(}\AttributeTok{treatments=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{8}\NormalTok{), }\AttributeTok{replicates =} \FunctionTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{,}\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

The above will produce a design with 2 varieties in 10 replicates and 8 in 1. Note that the order in treatments and replicates must agree. Although this works, you should find it has produced a completely randomised design with no blocking. That is not too bad in this case, as the number of entries is so low, but it is not usually what we usually require.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p.rep}\FloatTok{.2}\OtherTok{\textless{}{-}} \FunctionTok{blocks}\NormalTok{(}\AttributeTok{treatments=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{8}\NormalTok{), }\AttributeTok{replicates =} \FunctionTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{,}\DecValTok{1}\NormalTok{), }\AttributeTok{blocks=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

If we run the above we now have two blocks.

\begin{rmdcaution}
A cautionary note: many people (including me sometimes) use `blocks' and `replicates' interchangeably. But this only true for an randomised complete block design. In other cases it can lead to confusion when, as here, there are variable replicate numbers so that none of the block structures corresponds to `replicate blocks' or `complete blocks'.
\end{rmdcaution}

\begin{rmdquiz}
\textbf{2} What do you find about the allocation of the two control varieties over the blocks? As the design is small you can just look and count.
\end{rmdquiz}

If we wanted to count control number across block in larger trial we could run:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(p.rep}\FloatTok{.2}\SpecialCharTok{$}\NormalTok{Design}\SpecialCharTok{$}\NormalTok{Level\_1, p.rep}\FloatTok{.2}\SpecialCharTok{$}\NormalTok{Design}\SpecialCharTok{$}\NormalTok{treatments)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     
##      1 2 3 4 5 6 7 8 9 10
##   B1 5 5 0 1 0 0 0 1 1  1
##   B2 5 5 1 0 1 1 1 0 0  0
\end{verbatim}

This uses the function \texttt{table}. Perhaps we think that a block size of 10 is too great, but that 5 would be better.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p.rep}\FloatTok{.3}\OtherTok{\textless{}{-}} \FunctionTok{blocks}\NormalTok{(}\AttributeTok{treatments=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{8}\NormalTok{), }\AttributeTok{replicates =} \FunctionTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{,}\DecValTok{1}\NormalTok{),}\AttributeTok{blocks=}\DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Running the above will achieve a block size of 5.

\begin{rmdquiz}
\textbf{3} Either use \texttt{table} or inspect the \texttt{design} to examine the allocation of the control varieties over blocks. Does it look sensible?
\end{rmdquiz}

If we run:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p.rep}\FloatTok{.4}\OtherTok{\textless{}{-}} \FunctionTok{blocks}\NormalTok{(}\AttributeTok{treatments=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{8}\NormalTok{), }\AttributeTok{replicates =} \FunctionTok{c}\NormalTok{(}\DecValTok{10}\NormalTok{,}\DecValTok{1}\NormalTok{),}\AttributeTok{blocks=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\NormalTok{p.rep}\FloatTok{.4}\SpecialCharTok{$}\NormalTok{Plan}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Level_1 Level_2 Blocks.Plots: 1  2 3 4 5 6 7
## 1      B1   B1.B1               2  5 8 1 2 1 1
## 2      B1   B1.B2               6  1 2 2 1 2 3
## 3      B2   B2.B1               7  1 2 4 1 2 2
## 4      B2   B2.B2               2 10 1 9 1 2 1
\end{verbatim}

We have introduced an additional constraint. There are two levels of blocking: one with two blocks, each of size 10, and then one with two blocks of size five nested within each of the larger blocks. In this case, there is no difference in design between p.rep.3 and p.rep.4, but this is not usually the case.

The first level of blocking is defining complete replicates and the second incomplete blocks within replicates. The second level of blocking is labelled 1, 2 within each of the first levels. This is confusing as there are now two separate blocks both labeled 1 (and 2). It is also a potential cause of error when we come to analyse the data - we'll look at that tomorrow.

The \texttt{blocks} command is great for blocking in one dimension, which is very often all we want to do. The facility to nest blocks within one dimension has advantages: it removes much of the angst about selecting block size since you can have two or three nested blocks with little loss of precision compared to choosing the best size in the first case and a potential gain in precision compared to using the wrong block size.

\hypertarget{two-dimensional-designs}{%
\subsection{Two-dimensional designs}\label{two-dimensional-designs}}

\texttt{blocks} will not construct two-dimensional designs however. For this, we need to use the more flexible command \texttt{design}. We shall use this to create a classical two-dimensional model: the balanced lattice square design has a treatment number which is a square (9, 16, 25 etc.) and each treatment occurs once with every other treatment in both a row block and a column block. The block size, k, is the square root of the number of treatments. There are k+1 complete replicates and there are k blocks in each replicate. These are powerful designs, controlling error variation in two dimensions. They are also very restrictive in acceptable variety and replicate numbers.

We shall create the design for 9 varieties in 4 replicates. \texttt{design} works on the complete block structure and list of \texttt{treatments} and then allocates the \texttt{treatments} to the \texttt{blocks} optimally. This is very flexible but you need to create the list of blocks and treatments, I created the blocks structure in Excel. Read it in from the file ``lattice\_9.csv''.

\begin{rmdimportant}
For the next steps to work you will either need to change your RStudio working directory to the folder containing the downloaded data files for Chapter 4 (which you should have downloaded to your computer from the course website). Or move the datafiles to your current working directory outside of R.
\end{rmdimportant}

Read in the block structure from the existing file:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lattice}\FloatTok{.9}\OtherTok{\textless{}{-}}\FunctionTok{read.csv}\NormalTok{(}\StringTok{"lattice\_9.csv"}\NormalTok{,}\AttributeTok{header=}\NormalTok{T,}\AttributeTok{colClasses =} \StringTok{"factor"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In each replicate there are three row blocks and three column blocks.

The \texttt{colClasses\ =\ "factor"} option ensures all the columns are treated as factors rather than numbers. It is possible to create this in R with a few short commands; see examples under \texttt{help(design)} but in this case it is as easy to create the design in Excel.

We also need a factor for nine varieties replicated four times:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{var.code}\OtherTok{\textless{}{-}}\FunctionTok{factor}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{9}\NormalTok{,}\DecValTok{4}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

This is slight overkill in this case: there is no need to provide the replicate number as it is equal for all varieties and R just repeats the list, but it introduces the easy extension for unequal replicate numbers. For example:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{factor}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{7}\NormalTok{,}\DecValTok{4}\NormalTok{),}\FunctionTok{rep}\NormalTok{(}\DecValTok{8}\NormalTok{,}\DecValTok{8}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 1 2 3 4 5 6 7 1 2 3 4 5 6 7 1 2 3 4 5 6 7 1 2 3 4 5 6 7 8 8 8 8 8 8 8 8
## Levels: 1 2 3 4 5 6 7 8
\end{verbatim}

The above command defines a treatment structure for seven varieties in 4 replicates and one in 8 replicates.

Returning to the 3 x 3 lattice, to produce the design:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{latt.sq}\OtherTok{\textless{}{-}}\FunctionTok{design}\NormalTok{(}\AttributeTok{treatments=}\NormalTok{var.code,}\AttributeTok{blocks=}\NormalTok{lattice}\FloatTok{.9}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

That takes slightly longer to run than the \texttt{blocks} command. The syntax is clear. Check the output makes sense.

We want to check if each variety pair occurs once in each row-block and once in each column-block. We can use \texttt{interaction} to give each of these blocks a unique code over replicates and \texttt{table} to tabulate variety codes with blocks. We shall use a new command \texttt{crossprod} to work out the number of concurrences of each variety with every other in these blocks. {[}In matrix terminology, crossprod pre-multiplies a matrix by its transpose.{]}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{crossprod}\NormalTok{(}\FunctionTok{table}\NormalTok{(}\FunctionTok{interaction}\NormalTok{(latt.sq}\SpecialCharTok{$}\NormalTok{Design}\SpecialCharTok{$}\NormalTok{reps,latt.sq}\SpecialCharTok{$}\NormalTok{Design}\SpecialCharTok{$}\NormalTok{rows),}
\NormalTok{latt.sq}\SpecialCharTok{$}\NormalTok{Design}\SpecialCharTok{$}\NormalTok{treatments))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    
##     1 2 3 4 5 6 7 8 9
##   1 4 1 1 1 1 1 1 1 1
##   2 1 4 1 1 1 1 1 1 1
##   3 1 1 4 1 1 1 1 1 1
##   4 1 1 1 4 1 1 1 1 1
##   5 1 1 1 1 4 1 1 1 1
##   6 1 1 1 1 1 4 1 1 1
##   7 1 1 1 1 1 1 4 1 1
##   8 1 1 1 1 1 1 1 4 1
##   9 1 1 1 1 1 1 1 1 4
\end{verbatim}

Row blocks look good. What about column blocks?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{crossprod}\NormalTok{(}\FunctionTok{table}\NormalTok{(}\FunctionTok{interaction}\NormalTok{(latt.sq}\SpecialCharTok{$}\NormalTok{Design}\SpecialCharTok{$}\NormalTok{reps,latt.sq}\SpecialCharTok{$}\NormalTok{Design}\SpecialCharTok{$}\NormalTok{cols),}
\NormalTok{latt.sq}\SpecialCharTok{$}\NormalTok{Design}\SpecialCharTok{$}\NormalTok{treatments))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    
##     1 2 3 4 5 6 7 8 9
##   1 4 1 1 1 1 1 1 1 1
##   2 1 4 1 1 1 1 1 1 1
##   3 1 1 4 1 1 1 1 1 1
##   4 1 1 1 4 1 1 1 1 1
##   5 1 1 1 1 4 1 1 1 1
##   6 1 1 1 1 1 4 1 1 1
##   7 1 1 1 1 1 1 4 1 1
##   8 1 1 1 1 1 1 1 4 1
##   9 1 1 1 1 1 1 1 1 4
\end{verbatim}

blocksdesign searches for the best design. We can make it search harder by increasing the number of \texttt{searches}:

\begin{rmdquiz}
\textbf{4} Increase search number to 1000 in the design command. You can do this by adding \texttt{searches\ =\ 1000}. Does that improve the design?
\end{rmdquiz}

\begin{rmdnote}
Increasing search number will mean the function takes longer to run. It can be hard to know if R is still running something and you can think your computer has frozen.

When something is running, a red stop sign will appear in the toolbar of the console window (top righthand side). If you press this it will stop the run. It's also a very usefull indicator to know if something is running still.
\end{rmdnote}

\hypertarget{a-larger-balanced-lattice-square}{%
\subsubsection{A larger balanced lattice square}\label{a-larger-balanced-lattice-square}}

Read in the following file from your working directory:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lattice}\FloatTok{.25}\OtherTok{\textless{}{-}}\FunctionTok{read.csv}\NormalTok{(}\StringTok{"lattice\_25.csv"}\NormalTok{,}\AttributeTok{header=}\NormalTok{T,}\AttributeTok{colClasses =} \StringTok{"factor"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This file contains a design for 25 varieties in six replicates. Each replicate is arranged with 5 row blocks and 5 column blocks.

Define the \texttt{treatments}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{var.code}\FloatTok{.25}\OtherTok{\textless{}{-}}\FunctionTok{factor}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{25}\NormalTok{,}\DecValTok{6}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

The target is that each pair of varieties should occur together once in a row block and once in a column block. (With a block size of 5, a variety can occur with 4 other varieties in a block. There are 6 replicates of each variety so there are 6 x 4 = 24 concurrences: hopefully one for every variety pair). As accuracy of comparison between varieties should be greatest when varieties occur in the same block, all pairs of varieties should be compared with equal precision if we achieve the design objective.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{latt.sq}\FloatTok{.25}\OtherTok{\textless{}{-}} \FunctionTok{design}\NormalTok{(var.code}\FloatTok{.25}\NormalTok{,lattice}\FloatTok{.25}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Efficiencies (or \texttt{efficiency} factors) can be examined as:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{latt.sq}\SpecialCharTok{$}\NormalTok{Treatments\_model}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Treatment.model Model.DF D.Efficiency
## 1    ~ treatments        8            1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{latt.sq}\FloatTok{.25}\SpecialCharTok{$}\NormalTok{Blocks\_model}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        First_order effects  D.Effic  A.Effic       Second_order effects D.Effic
## 1           (reps)       5 1.000000 1.000000           (reps)^2       5      NA
## 2      (reps+rows)      29 0.830656 0.827968      (reps+rows)^2      29      NA
## 3 (reps+rows+cols)      53 0.659078 0.651444 (reps+rows+cols)^2     149      NA
##   A.Effic
## 1      NA
## 2      NA
## 3      NA
\end{verbatim}

\texttt{D-efficiency} and \texttt{A-efficiency} are measures of the average precision with which the parameters in the model (for us the variety effects) are estimated. More information can be found on this \href{http://homepage.divms.uiowa.edu/~gwoodwor/AdvancedDesign/KuhfeldTobiasGarratt.pdf}{here}.

These are hard to interpret. \texttt{A-efficiency} is related to the average of the variance of the treatment estimates and is usually what we are interested in.

\begin{rmdquiz}
\textbf{5} Easier is to examine the design as before. Edit the command for the 3 x 3 lattice to look at the balance of variety concurrences in row and column blocks. Have we reached the optimum design?
\end{rmdquiz}

If you think we should improve out attempt:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{latt.sq}\FloatTok{.25}\SpecialCharTok{$}\NormalTok{searches}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 34
\end{verbatim}

That shows that the computer algorithm had 34 attempts at improving the design.

Overriding this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{latt.sq}\FloatTok{.25}\OtherTok{\textless{}{-}} \FunctionTok{design}\NormalTok{(var.code}\FloatTok{.25}\NormalTok{,lattice}\FloatTok{.25}\NormalTok{,}\AttributeTok{searches =} \DecValTok{1000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{rmdquiz}
\textbf{6} Inspect the output. How did we do by adding more searches?
\end{rmdquiz}

We have a very good design, particularly if we are mainly concerned in controlling variation in one dimension. But if you need a balanced design for 25 varieties in 6 replicates, with two dimensional blocks, you are better off referring to a published catalogue of optimal designs; though you are in danger or reverting to ``Procrustean design'' \citep{mead_design_1994}. Or run \texttt{blocksdesign} for many more iterations; I eventually succeeded in recreating the balanced lattice.

\hypertarget{a-partially-replicated-design}{%
\subsubsection{A partially replicated design}\label{a-partially-replicated-design}}

As a final example, we shall return to p-rep and augmented designs: assume we have 36 experimental varieties, but only 12 of them have enough seed for 2 replicates. We want to test all 36. We also have a control variety which we want to test in 12 replications.

We therefore have (12 x 2) + (24 x 1) + (12 x 1) = 60 plots in total. We shall create a nested design with two main blocks of size 30, and 6 sub blocks of size 5.
We are going to use the \texttt{design} function for this. Therefore, we need to form our \texttt{treatments}.

\begin{rmdquiz}
\textbf{7} Have a go.

Hint = I've filled in some of the options below. You have to substitute the correct numbers or parameters for x, y and z:

\texttt{aug.treats\textless{}-factor(c(rep(1:12,x),rep(y:36,1),rep(37,z)))}
\end{rmdquiz}

Check your answer. (e.g.~is it the right length). If you are ever doing this on your own and struggle, just do it in Excel and then read it in. I've already created the blocks structure in excel, so you can read it straight in:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{aug.blocks}\OtherTok{\textless{}{-}}\FunctionTok{read.csv}\NormalTok{(}\StringTok{"aug\_blocks.csv"}\NormalTok{,}\AttributeTok{header=}\NormalTok{T,}\AttributeTok{colClasses =} \StringTok{"factor"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{rmdquiz}
\textbf{8} Now run the \texttt{design} command:
\texttt{aug.expt\textless{}-design(a,b)} \# You substitute for a and b
\end{rmdquiz}

Extract and examine the block structures again. How does that look?
You should find there is a control occurs in every block. What is the distribution of 2-rep varieties over the sub blocks?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rowMeans}\NormalTok{(}\FunctionTok{table}\NormalTok{(}\FunctionTok{interaction}\NormalTok{(aug.expt}\SpecialCharTok{$}\NormalTok{Design}\SpecialCharTok{$}\NormalTok{main\_blocks,aug.expt}\SpecialCharTok{$}\NormalTok{Design}\SpecialCharTok{$}\NormalTok{sub\_blocks),aug.expt}\SpecialCharTok{$}\NormalTok{Design}\SpecialCharTok{$}\NormalTok{treatments)[,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{12}\NormalTok{])}\SpecialCharTok{*}\DecValTok{12}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  1.1  2.1 1.10 2.10 1.11 2.11 1.12 2.12  1.2  2.2  1.3  2.3  1.4  2.4  1.5  2.5 
##    2    0    0    2    0    2    0    2    2    0    2    0    2    0    2    0 
##  1.6  2.6  1.7  2.7  1.8  2.8  1.9  2.9 
##    2    0    0    2    0    2    0    2
\end{verbatim}

The combinations of main-blocks and sub-blocks that have none of the 2-rep varieties don't exist in the trial. For the ones that do, the 2-rep varieties are spread out well.

It is always worth checking the distribution of entries of blocks appears sensible; there is a risk of error in entering the design information and also a risk that the software you are using has not found the allocation of varieties to blocks that you expected.

You could produce a similar design by hand; you could start with a design for 12 varieties in 2 replicates with 6 sub-blocks of size 2 in each replicate. Then add the control to each sub block. Then add two unreplicated entries to each sub block. This should produce a very similar design. Alternatively, you could produce an alpha design for 30 entries in two replicates and then allocate varieties to codes to end up with the same even distribution.

\hypertarget{an-augmented-paritially-replicated-design-complicated}{%
\subsubsection{An augmented paritially replicated design: complicated}\label{an-augmented-paritially-replicated-design-complicated}}

The vignette for \texttt{blocksdesign} suggests that if you have unreplicated entries, it may be better to create a design for the replicated entries first, then fill in the unreplicated entries by hand. For example, I tried to create a design with 4 controls in 10 replicates, 55 candidates in 2 replicates and 250 entries in 1 replicate, with two big blocks of size 400 and 20 nested blocks of size 10 in each big block. The target design would have a single control in each of the 40 small blocks -- slightly increasing the accuracy of the comparison between controls and candidates at the expense of reducing the accuracy among the four controls.

Running:

\texttt{aug.p.rep\textless{}-blocks(treatments\ =\ c(4,55,250),replicates\ =\ c(10,2,1),blocks=c(2,20),searches=1000)}

Produced a design in which the four controls concurred once or twice in the small blocks and in which two controls concurred with the two replicate entries 17 times and the other two controls concurred 18 times.

However, re-running the design as:

\texttt{aug.p.rep.2\textless{}-blocks(treatments\ =\ c(4,55),replicates\ =\ c(10,2),blocks=c(2,20),searches=1000)}

Created a design in which the controls never concurred, but concurred 28, 27, 28 and 29 times with the 2-rep candidates. 30 blocks had 4 entries and 10 had 3. Allocating the 1 replicate entries round up all blocks to size ten would give the desired design. (Entries within blocks would require re-randomising too.) This design will have slightly reduced the accuracy of comparisons among the four controls and increased the accuracy among candidates and between candidates and the controls. There is nothing wrong with the first deign, however, though the second is aesthetically more pleasing.

\begin{rmdnote}
For a multi-location trials series, you can treat each location as a block, and then have nested blocks within locations as before. The locations need not be complete blocks -- not all varieties are tested at all locations. This is becoming common in large plant breeding programmes. An advantage of this approach is that genomic prediction methods (discussed next week) can be used to predict the performance of the varieties in the missing locations.
\end{rmdnote}

\hypertarget{farm-scale-experiments}{%
\section{Farm scale experiments}\label{farm-scale-experiments}}

On farm experiments, using farm equipment, are experiencing a revival, partly promoted by better farming equipment with on-harvester yield assessment, GPS, remote sensing and so on. The principles of good experimental design, the three R's, still apply however. Generally, the experimental unit is large: a fields or half a field, but the number of available experimental units available on a single farm is low. Experimental units within a farm can be treated as a single block. The size of the blocks may vary from farm to farm, however.

As an example, consider the imaginary design in this table:

\begin{longtable}[t]{ll}
\caption{\label{tab:unnamed-chunk-175}Farm Blocks}\\
\toprule
farm & fields\\
\midrule
1 & 1\\
1 & 1\\
1 & 2\\
2 & 1\\
2 & 1\\
\addlinespace
2 & 2\\
2 & 2\\
2 & 3\\
2 & 3\\
3 & 1\\
\addlinespace
3 & 1\\
3 & 2\\
4 & 1\\
4 & 1\\
4 & 2\\
\addlinespace
4 & 2\\
5 & 1\\
5 & 1\\
5 & 2\\
5 & 2\\
\addlinespace
6 & 1\\
6 & 1\\
6 & 2\\
7 & 1\\
7 & 1\\
\addlinespace
7 & 2\\
7 & 2\\
8 & 1\\
8 & 1\\
8 & 2\\
\addlinespace
8 & 2\\
8 & 3\\
9 & 1\\
9 & 1\\
9 & 2\\
\addlinespace
9 & 2\\
9 & 3\\
10 & 1\\
10 & 1\\
10 & 2\\
\addlinespace
10 & 2\\
\bottomrule
\end{longtable}

There are 10 farms, with between two and three fields available per farm. If a field is big enough, it is split in half -- two `plots' per field. This number could be larger for bigger fields, but I've kept a maximum of two. There are 41 plots in total: 18 fields with 2 plots and 5 with only 1.

Following the same process as before to create the design:

Suppose we have six varieties to test. Five of these can be replicated seven times and one six times.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{farm.var}\OtherTok{\textless{}{-}}\FunctionTok{factor}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{,}\DecValTok{7}\NormalTok{),}\FunctionTok{rep}\NormalTok{(}\DecValTok{6}\NormalTok{,}\DecValTok{6}\NormalTok{)))}
\NormalTok{farm.expt}\OtherTok{\textless{}{-}}\FunctionTok{design}\NormalTok{(}\AttributeTok{treatments=}\NormalTok{farm.var,}\AttributeTok{blocks=}\NormalTok{farm.blocks,}\AttributeTok{searches=}\DecValTok{1000}\NormalTok{)}
\NormalTok{farm.expt}\SpecialCharTok{$}\NormalTok{Design}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    farm fields treatments
## 1     1      1          5
## 2     1      1          2
## 3     1      2          4
## 4    10      1          2
## 5    10      1          6
## 6    10      2          1
## 7    10      2          3
## 8     2      1          4
## 9     2      1          1
## 10    2      2          2
## 11    2      2          3
## 12    2      3          5
## 13    2      3          6
## 14    3      1          4
## 15    3      1          2
## 16    3      2          1
## 17    4      1          5
## 18    4      1          1
## 19    4      2          3
## 20    4      2          4
## 21    5      1          2
## 22    5      1          1
## 23    5      2          5
## 24    5      2          6
## 25    6      1          3
## 26    6      1          5
## 27    6      2          2
## 28    7      1          4
## 29    7      1          3
## 30    7      2          2
## 31    7      2          6
## 32    8      1          6
## 33    8      1          4
## 34    8      2          1
## 35    8      2          5
## 36    8      3          3
## 37    9      1          6
## 38    9      1          3
## 39    9      2          4
## 40    9      2          5
## 41    9      3          1
\end{verbatim}

\begin{rmdnote}
If you want to have a go at this yourself, there is a csv file called ``farm\_trials\_design.csv'' in your data folder. Or you can inspect the output below and take our word for it.
\end{rmdnote}

Let's take a look at the results:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(farm.expt}\SpecialCharTok{$}\NormalTok{Design}\SpecialCharTok{$}\NormalTok{treatments,farm.expt}\SpecialCharTok{$}\NormalTok{Design}\SpecialCharTok{$}\NormalTok{farm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    
##     1 10 2 3 4 5 6 7 8 9
##   1 0  1 1 1 1 1 0 0 1 1
##   2 1  1 1 1 0 1 1 1 0 0
##   3 0  1 1 0 1 0 1 1 1 1
##   4 1  0 1 1 1 0 0 1 1 1
##   5 1  0 1 0 1 1 1 0 1 1
##   6 0  1 1 0 0 1 0 1 1 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{crossprod}\NormalTok{(}\FunctionTok{table}\NormalTok{(farm.expt}\SpecialCharTok{$}\NormalTok{Design}\SpecialCharTok{$}\NormalTok{farm,farm.expt}\SpecialCharTok{$}\NormalTok{Design}\SpecialCharTok{$}\NormalTok{treatments))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    
##     1 2 3 4 5 6
##   1 7 4 5 5 5 5
##   2 4 7 4 4 4 4
##   3 5 4 7 5 5 5
##   4 5 4 5 7 5 4
##   5 5 4 5 5 7 4
##   6 5 4 5 4 4 6
\end{verbatim}

The allocation of varieties across farms looks reasonable. What about the allocation within fields?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{crossprod}\NormalTok{(}\FunctionTok{table}\NormalTok{(}\FunctionTok{interaction}\NormalTok{(farm.expt}\SpecialCharTok{$}\NormalTok{Design}\SpecialCharTok{$}\NormalTok{farm,farm.expt}\SpecialCharTok{$}\NormalTok{Design}\SpecialCharTok{$}\NormalTok{fields),farm.expt}\SpecialCharTok{$}\NormalTok{Design}\SpecialCharTok{$}\NormalTok{treatments))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    
##     1 2 3 4 5 6
##   1 7 1 1 1 2 0
##   2 1 7 1 1 1 2
##   3 1 1 7 2 1 1
##   4 1 1 2 7 1 1
##   5 2 1 1 1 7 2
##   6 0 2 1 1 2 6
\end{verbatim}

\begin{rmdquiz}
\textbf{9} What do you think? I increased searches to 1,000 because one of my test runs did not do so well. It is quite likely that plot sizes will vary from farm to farm. What effect would this have? Will it invalidate the design?
\end{rmdquiz}

\hypertarget{alpha-designs.}{%
\section{Alpha designs.}\label{alpha-designs.}}

\hypertarget{creation-of-augmented-and-p-rep-designs-from-2-replicate-alpha-designs}{%
\subsection{Creation of augmented and p-rep designs from 2-replicate alpha designs}\label{creation-of-augmented-and-p-rep-designs-from-2-replicate-alpha-designs}}

Alpha designs are forms of incomplete block designs which have proven popular since their introduction in the late 1970's. They remain the standard design for NL and RL trials in the UK. They are resolvable incomplete block designs. ``Resolvable'' means that the incomplete blocks can be grouped into complete replicates. This is not necessary from the statistical point of view - non-resolvable incomplete block designs can be very effective - but does make management and scoring of the trial easier. The incomplete blocks give better control of field effects. This is particularly true of very large experiments, as often used in the early stages of testing in breeding programmes. The computer programs to create alpha designs no longer seem to be freely available. A catalogue of designs for number of varieties up to 100 has been published, and can be accessed from within GenStat (and I think within \href{http://www.edgarweb.org.uk/}{EDGAR}). For larger number of varieties, the packages \href{http://designcomputing.net/gendex/alpha/}{Gendex} or
\href{https://www.vsni.co.uk/software/cycdesign/}{CycDesigN} will create the designs, but these are not free, though you may be able to download a test copy. The R package agricolae \citep{R-agricolae} will also produce designs. However, designs for only two replicates are easy to create. These also lend themselves to simple creation of partially replicated designs. For the very large numbers of varieties found in breeders' trials, it is unlikely that designs with greater than two replicates would be required.

\citet{patterson_new_1976} presented simple methods of generating alpha designs in two, three and four replicates, with some constraints. The method for two replicates is that followed described below. The designs are not necessarily optimal but are good and will allow large numbers of varieties. I used to use these extensively.

Alpha designs are not balanced. This is in contrast to many classical incomplete block designs with numbers of varieties which are a complete square (25, 36 and so on). Lack of balance, in this sense, means that not all possible pairs of varieties occur together within an incomplete block. Alpha designs are described as 0,1 designs if pairs of varieties concur once or never. They are described as 0,1,2 designs if, in addition, some pairs of varieties concur twice. 0,1 designs are preferred since these give more even estimation of differences between varieties.

The procedure below will create a 0,1 design for two replicates for any number of varieties \(v\), for which \(v = sk\) where \(k\) is the block size (number of entries per block) and \(s\) is the number of blocks per replicate. Thus, one could produce a design for 200 varieties in \(s\)=20 blocks per replicate, each with \(k\)=10 entries. There is an additional constraint: the number of entries per block must be less than or equal to the square root of the number of varieties. These constraints are not generally restrictive for large breeders' trials with block sizes of around 10; a reasonable starting point for large trials. The method below does not create the best possible design: rearrangements of which pairs of varieties concur may make improvements, but it will be pretty close.

The small example below is for 12 varieties in block sizes of 3.

First write down the variety codes for the first replicate as \(s\) rows of \(k\) entries:

\begin{verbatim}
  1 5 9
  2 6 10
  3 7 11
  4 8 12
\end{verbatim}

Thus, the third block in the first replicate contains entries 3, 7 and 11.

Alpha designs are cyclical designs. The meaning of the term will be apparent in the creation of the second replicate. Imagine the columns in the first replicate are numbers on a loop. We leave the first column alone but nudge the next column by one position, the third column by two positions, the fourth (if we had one) by three and so on. This gives:

\begin{verbatim}
  1 6 11
  2 7 12
  3 8 9
  4 5 10
\end{verbatim}

You can check that over the eight blocks in the two replicates varieties occur together once (e.g.~1 and 6) or never (e.g.~6 and 7).

You must randomise the order of the blocks within replicates, then the order of the entries within blocks. This is then a perfectly reasonable incomplete block design.
You must also allocate the varieties to the code numbers used in the design at random.

There are some restraints that can be put on this randomisation process however, which can be useful. Note that varieties occurring within the same column never occur in the same block. If we have both control and candidate varieties but our greatest interest is in the candidates, then if we allocate the controls (up to four varieties in the example) to code numbers 1 to 4 (at random) then they never concur. Differences between the controls are then measured with less precision, but consequently the precision of comparisons between controls and candidates is increased.

Equally, suppose we had only one control, but we wished this to have greater replication, then this could be allocated to up to four code numbers (numbers 1 to 4) to give up to eight replicates. Or we could create a design with two controls in four replicates by allocating each control to two of the codes numbers 1 to 4.

Now suppose we have some candidate varieties for which we have seed for only a single replicate. For two such candidates, we allocate both to the same code number, but one goes into replicate 1 and the second into replicate 2. This acts to reduce the variation in comparisons among all the candidates. For successive pairs of un-replicated candidates, we should continue with this allocation procedure by working down the columns of the design matrix. Thus, six un-replicated candidates could be allocated to entries 5, 6 and 7. In an extreme case, we might have a single control in eight replicates and 16 un-replicated candidates. From the design above, this would give:

\begin{verbatim}
  1 5a 9a 
  1 6a 10a
  1 7a 11a
  1 8a 12a    

  1 6b 11b
  1 7b 12b
  1 8b 9b
  1 5b 10b
\end{verbatim}

Here, code 1 is the control, 5a and 5b are completely different candidates, as are 6a and 6b etc.

This design should be randomised: first blocks within replicates then position within blocks. An example randomisation gave me:

\begin{verbatim}
  10a 1  6a
  1  11a 7a
  12a 1  8a
  1  5a  9a
    
  7b 12b 1
  5b 10b 1
  1  11b 6b
  9b  1  8b 
\end{verbatim}

For this example, we didn't need to go through the alpha-design procedure: the only alternative is to arrange the controls systematically. Note we should allocate our candidates to the codes at random: they are almost certainly grouped in some sort of order, by cross or by breeder perhaps, and we should disrupt this patterns to avoid risk of bias.

With larger numbers of candidates and larger block sizes, other arrangements are possible. For example, with block sizes of 12 (requiring a minimum of 144 entries for a 0,1 design), then there will be 12 columns in the design matrix. All entries in the first column could be allocated to one control variety and all entries in the next column to another. This would ensure that each block contained each control once. The candidates could then be allocated to the remaining codes in one or two replicates as described above. A similar end point could be obtained by designing an alpha design with a block size of 10 and then adding the two controls to each block independently of the design. Some caution is required. With no control varieties, if only a small number of candidates are present in two replicates, then the incomplete block analysis may fail: insufficient replicated entries may concur to allow estimation of all block effects. In such a case, analysis as a randomised complete block or by spatial analysis would still be possible.

Regardless of the number of candidates and controls, the candidates must be allocated at random to the candidate codes and the controls at random to the control codes. Then blocks must be randomised within replicates and plot positions within blocks.

\hypertarget{final-message}{%
\subsection{Final message}\label{final-message}}

Four important reminders:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Always save your design and back it up. Keep notes: don't assume you will remember how you created the design months later, when you have some data to analyse.
\item
  \texttt{blocksdesign} and other software, will carry out the randomisation for you. However, most software does not allocate varieties to the code numbers used in the design at random. You should do this yourself if there is any systematic pattern to the list of varieties: for example varieties are listed by parentage. This could, for example, result in the difference between varieties with the same parents being estimated with greater parentage than those with different parents, which may not be what you want. If in doubt \textbf{randomise}.
\item
  It is perfectly acceptable to use the same design for multiple experiments, but not the same randomisation. Without additional randomisation, you may end up, again, with the difference between some pairs of varieties being estimated more precisely than others. Moreover, the analysis will not take account of this: it will happen without you knowing.
\item
  Please: always randomise the first replicate. I don't get around much anymore, but many breeders and official testing authorities used to lay out the first replicate in variety order. This makes it easy of visitors and breeders to inspect varieties, but invalidates the design for the same reasons given in (2) and (3) above. Pity poor variety number one. It will always be in one corner of the first replicate; nearest the gate, and more likely to get trampled on and run-over than the others. It was probably drilled first and will be harvested first; when human and machine errors are more likely. If you want demonstration plots, plant or sow them in a separate block.
\end{enumerate}

\hypertarget{Trial-analysis}{%
\chapter{Trial analysis (sugar beet example).}\label{Trial-analysis}}

\hypertarget{the-layout-of-the-sugar-beet-trial}{%
\section{The layout of the sugar beet trial}\label{the-layout-of-the-sugar-beet-trial}}

These data are from an NL/RL sugar beet trial grown in 2007, designed as an alpha design. We shall analyse this in several different ways, using it to introduce REML as a routine algorithm for the analysis of this sort of data and also to look at the improvements over randomised complete blocks that incomplete blocks and spatial analysis can bring. I've scrambled the names of varieties to avoid anyone's embarrassment. This is a variable site, chosen to illustrate the advantages of more sophisticated trial design.

The design of the trial is as follows:

\begin{verbatim}
  DESIGN TYPE    INCOMPLETE BLOCK
  PLOTS          460
  REPLICATES     4
  BLOCKS/REP     17
  VARIETIES      115
  PLOTS/VARIETY  4
\end{verbatim}

A typical sugar beet plot is in three rows wide, 0.5m apart and is 12m long, with 10m harvested -- the two discarded metres are the alleyways between the plots used by tractors for spraying. I make this trial to be 28 plots = 42m wide and 18 plots = 216m long.

\includegraphics{images/sugar_beet_trial.png}
For a better impression of the above image: there is a file called ``sugar\_beet\_trial R.xls'' in the data folder for this chapter. Inspect the field plan book.

The incomplete blocks run along the rows of the spreadsheet. Note that to accommodate the variety number and the shape of the field, the block sizes are unequal and that there are discard plots within the trial area.

\begin{rmdquiz}
\textbf{1} Can anyone come up with a better design to fit this area? What about the shape and size of the replicates? Numbers of entries per block? Do you regard the discard plots as a waste of space, or as a saving in cost?
\end{rmdquiz}

\hypertarget{fixed-effects-random-effects-and-reml}{%
\section{Fixed effects, random effects and REML}\label{fixed-effects-random-effects-and-reml}}

Fixed effects are generally the things you are interested in such as variety performances, fertilizer treatments etc. Random effects are often things that you are not interested in that get in the way of getting good estimates of the fixed effects such as fertility effects; and therefore also include factors in the design to control for fertility effects such as replicates and incomplete. Residual errors from any analysis are always random effects. Over multiple trials, the sites and year effects can be viewed as random. There is ambiguity: sometimes varieties can be viewed as a random sample from a population of possible varieties. The most pure examples of this would be a set of RIL's from an F2 or clones from a cross between two outbred parents. All data are analysed by fitting a model; even a t-test fits a model. A dataset in which some effects are fixed and some effects are random is called a mixed model. REML, residual (or restricted) estimation by maximum likelihood, is an algorithm for analysing mixed models. Variety trials, either individually or in sets, are now routinely analysed by mixed models. This is because some aspect of them - blocks, reps, sites, years and interactions of these with varieties - are often treated as random effects. Increasingly, varieties are treated as random too. There are advantages and disadvantages to this. The decision depends on the objectives of the experiment.

\hypertarget{preparing-the-data}{%
\subsection{Preparing the data}\label{preparing-the-data}}

The data are saved in the Chapter 5 data folder sorted as columns within rows. The CSV file is called: \emph{``sugar\_beet\_data.csv''}

\begin{rmdquiz}
\textbf{2} You should now have picked up on how to read a csv file into R and save it as an object. Do this now for the ``sugar\_beet\_data.csv'' and save it as: \texttt{beet.data}.\\
\end{rmdquiz}

The discard plots have been left out of the file as they had missing data. However, if they had been measured for each trait, it would have improved our fertility correction. Note there are some missing data included as \texttt{NA}: measurements which failed.

We'll work with four of the traits which were recorded:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Number of roots measured before harvest -- can have an obvious effect on yield in a row crop like beet. Closely correlated with emergence too.
\item
  Sugar content -- the proportion of the fresh weight of a root which is sugar.
\item
  Sugar yield.
\item
  Total impurities -- a measure of the chemical crud in the beet which affects the efficiency with which the factory can extract the sugar from the root.
\end{enumerate}

I've selected these because they are the most important characters and can also show different patterns of variability within a trial and among varieties.

There is no need to analyse all the characters. This year well concentrate on \emph{sugar content}.

Let's take a look at our column headers in the data object \texttt{beet.data} that you've created above:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{colnames}\NormalTok{(beet.data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "rep"           "column"        "row"           "plot"         
##  [5] "block"         "variety"       "roots"         "sugar.content"
##  [9] "yield"         "impurities"
\end{verbatim}

If you don't get the same result on your PC, you may have left out the required: \texttt{headers=T} argument in the \texttt{read.csv} command. The first 6 columns are experimental design information and therefore need to be treated as factors. We can inspect how R is currently treating them using:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(beet.data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    460 obs. of  10 variables:
##  $ rep          : int  1 2 3 4 1 2 3 4 1 2 ...
##  $ column       : int  9 4 17 21 14 4 15 19 9 1 ...
##  $ row          : int  14 4 14 8 10 3 15 7 18 9 ...
##  $ plot         : int  54 187 279 366 115 200 264 378 2 116 ...
##  $ block        : int  9 28 42 54 17 30 40 56 1 18 ...
##  $ variety      : int  469 469 469 469 468 468 468 468 467 467 ...
##  $ roots        : int  160 153 157 156 151 160 157 159 154 145 ...
##  $ sugar.content: num  19.1 18.9 19.4 19.2 18.1 ...
##  $ yield        : num  12.6 10.6 12.3 12.4 14.6 ...
##  $ impurities   : num  2.39 2.38 2.37 2.37 2.29 ...
\end{verbatim}

That's informative. The trait data (roots, sugar.content, yield and impurities) are treated as either numeric or integers. That's how we want R to treat the phenotypes. However, the experimental design columns (first 6) are treated as integers. We need to convert these to factors. Use the command: \texttt{factor()}.

You can do this for each individual column, referring to it by name {[}eg \texttt{factor(beet.data\$rep)}{]} or by index {[}e.g.~\texttt{factor(beet.data{[},1{]})}{]}:

Or put it into a loop if you wish to speed things up:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{)\{}
\NormalTok{   beet.data[,i]}\OtherTok{\textless{}{-}}\FunctionTok{factor}\NormalTok{(beet.data[,i])}
\NormalTok{\} }
\end{Highlighting}
\end{Shaded}

If you want help understanding how the loop works you can ask a demonstrator or refer to Chapter 3.

\begin{rmdnote}
We haven't attached the data before carrying out these conversions. We could do so and carry out the conversions to factors on the attached dataset, but we want to work on a subset of the data and it is easy to be confused about what version of the data we are working on.
\end{rmdnote}

As we saw in the R introduction chapter, some analysis in R requires NA to be removed in the response variable. We could do that using:

\texttt{beet.for.anal\textless{}-subset(beet.data,!is.na(sugar.content))}

However, I don't think we need to worry about that here; the functions we are using have in built handling of NA.

There are some points to consider before we start:

\begin{itemize}
\item
  \emph{Watch out for negative variance components} -- if you get them, drop that term from the analysis. This is true for all mixed model analyses - not just for variety trials.
\item
  \emph{``Informative missingness''.} Usually we should check that missing data are not associated with a particular variety. Not so much in variety trials, but in case-control analysis of disease risk, this can result in major problems.
\end{itemize}

\hypertarget{randomised-complete-block-design}{%
\subsection{Randomised complete block design}\label{randomised-complete-block-design}}

You should get the same result as from an ANOVA, as with REML. Try both.

Note that there are some missing data, so the values in the analysis of variance table will vary slightly depending on the order in which terms are fitted. You should fit \texttt{varieties} last.

Using \texttt{lm}, the standard method for fitting linear models in R:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{RCB}\OtherTok{\textless{}{-}}\FunctionTok{lm}\NormalTok{(sugar.content}\SpecialCharTok{\textasciitilde{}}\NormalTok{rep}\SpecialCharTok{+}\NormalTok{variety , }\AttributeTok{data=}\NormalTok{beet.data)}
\FunctionTok{anova}\NormalTok{(RCB)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Analysis of Variance Table
## 
## Response: sugar.content
##            Df Sum Sq Mean Sq F value    Pr(>F)    
## rep         3  4.111  1.3702  36.837 < 2.2e-16 ***
## variety   114 62.928  0.5520  14.840 < 2.2e-16 ***
## Residuals 332 12.349  0.0372                      
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{rmdnote}
Want to stop writing \texttt{data\ =\ beet.data}? You could use \texttt{attach()}. However, I find it can lead to downstream issue when you start subsetting data.
\end{rmdnote}

We could have carried this out in one step:

\texttt{anova(lm(sugar.content\textasciitilde{}rep+variety,data=beet.data))}

It makes more sense to save the output first as there is other information we want to extract. In particular we would like to see the line means.

Run this on your computer:
\texttt{summary(RCB)}

As you will see, this gives more information that we require, but it is worth knowing the command is there.

\texttt{RCB\$coefficients}

gives the ``coefficients'' without the other information, Note, however, that we are missing an entry for rep 1. You may also observe that we are missing an entry for variety 1.

R, rather than providing simple means, as you would get if you analysed the data by hand (or e.g.~by GenStat) expresses results as a deviation from an ``Intercept''. The intercept is the mean for rep 1, variety 1. You can adjust for this by adding the ``Intercept'' to all other values. As it is usually the differences between varieties we are interested in, this doesn't matter, though is not very satisfying. However, there are some packages to do this for you. One example is \emph{emmeans} \citep{R-emmeans} which will do this job, and more. Another example is \emph{predictmeans} \citep{R-predictmeans} which runs faster in my experience, so we'll use that for this tutorial. You will need to install the package by running:

\texttt{install.packages(\textquotesingle{}predictmeans\textquotesingle{})}

Then load it via:

\texttt{library(predictmeans)}

This package only has several functions but it provides some detailed summaries and diagnostics for a large range of different models. The key function is: \texttt{predictmeans}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{RCB.vars}\OtherTok{\textless{}{-}}\FunctionTok{predictmeans}\NormalTok{(RCB,}\StringTok{"variety"}\NormalTok{,}\AttributeTok{plot=}\NormalTok{F)}
\end{Highlighting}
\end{Shaded}

That's saved a list object called \texttt{RCB.vars} which contains the functions output. There's a lot of information there, but for now we are just after the line means in a sensible format, which we can access using:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{RCB.vars}\SpecialCharTok{$}\StringTok{\textasciigrave{}}\AttributeTok{Predicted Means}\StringTok{\textasciigrave{}}\NormalTok{ [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## variety
##       1      55     105     150     155 
## 17.9850 18.6025 17.7475 18.3750 18.2625
\end{verbatim}

Note I've added \texttt{{[}1:5{]}} to save paper. Print on your screens without \texttt{{[}1:5{]}} included. If we access another part of the function output, we can print the first ten standard error of the means:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{RCB.vars}\SpecialCharTok{$}\StringTok{\textasciigrave{}}\AttributeTok{Standard Error of Means}\StringTok{\textasciigrave{}}\NormalTok{ [}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## variety
##       1      55     105     150     155     157     166     189     194     204 
## 0.09643 0.09643 0.09643 0.09643 0.09643 0.09643 0.11148 0.09643 0.13668 0.09643
\end{verbatim}

\begin{rmdquiz}
\textbf{3} Note that the standard errors are not all identical. Why is this?
\end{rmdquiz}

GenStat and ASReml (and other packages?) also provide standard errors of differences between variety means, and this is often the information used in comparing different statistical analysis methods of analysis.

\begin{rmdquiz}
\textbf{4} Why is the standard error of differences between mean preferred?
\end{rmdquiz}

The function has already calculated this for us, we just need to access it from our saved object:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{RCB.vars}\SpecialCharTok{$}\StringTok{\textasciigrave{}}\AttributeTok{Standard Error of Differences}\StringTok{\textasciigrave{}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Max.SED   Min.SED  Aveg.SED 
## 0.1765377 0.1363749 0.1384401
\end{verbatim}

\hypertarget{randomised-compelete-block-design---with-mixed-models}{%
\subsection{Randomised compelete block design - with mixed models}\label{randomised-compelete-block-design---with-mixed-models}}

Next, we are going to use the package `lme4' \citep{R-lme4}. Using the techniques you've mastered, install and load this package to your R environment. This is typically the package that \texttt{R} users would use to fit and analyze linear mixed models.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{RCB.lmer}\OtherTok{\textless{}{-}}\FunctionTok{lmer}\NormalTok{(sugar.content}\SpecialCharTok{\textasciitilde{}}\NormalTok{variety}\SpecialCharTok{+}\NormalTok{(}\DecValTok{1}\SpecialCharTok{|}\NormalTok{rep), }\AttributeTok{data=}\NormalTok{beet.data)}
\end{Highlighting}
\end{Shaded}

Print the output of \texttt{RCB.lmer} to your own computers and inspect the output. The \texttt{(1\textbar{}rep)} structure fits replicates as a random effect.

\begin{rmdquiz}
\textbf{5:}
It is impossible to do this using \texttt{lm}. Here it should make little difference. Why is this?
\end{rmdquiz}

There are a number of useful commands that we can use to inspect our fitted model:

\texttt{anova(RCB.lmer)} will give you a test for significance of the fixed effects in the model (here varieties). These will be very similar to those from \texttt{lm}. The tiny difference arises from the treatment of missing data. (With replicates treated as random, there is some information about varieties in between replicates comparisons.)

\texttt{ranef(RCB.lmer)} will give you the random effects for replicates.

\texttt{fixef(RCB.lmer)} will give you the fixed effects for varieties. Note that variety 1 is missing again. Its effect, as a deviation from the intercept is zero).

\texttt{summary(RCB.lmer)} gives a lot of output, mainly consisting of correlations among the estimates of the fixed effects. At the top are the estimates of variance components and a list of fixed effects and their standard errors. Note: the standard deviation listed alongside the variance component is just the square root of the variance component. It is not a measure of precision.

As before, the fixed effects are all expressed as deviations from variety 1. An easy way to see the fitted means is to alter the way the model parameters are fitted:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test}\OtherTok{\textless{}{-}}\FunctionTok{lmer}\NormalTok{(sugar.content}\SpecialCharTok{\textasciitilde{}}\NormalTok{variety}\SpecialCharTok{+}\NormalTok{(}\DecValTok{1}\SpecialCharTok{|}\NormalTok{rep)}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{,  }\AttributeTok{data=}\NormalTok{beet.data)}
\end{Highlighting}
\end{Shaded}

\begin{rmdcaution}
Beware -- this alters the result from the anova, since this now includes one extra df testing for the difference of the mean from zero.
\end{rmdcaution}

\texttt{summary(test)} will now give all variety means and their standard errors. Again, there are no standard errors of differences. Alternative summaries of the efficiency of the analysis are to look the F ratio or p-value for the variety effects, or use Akaike's Information Criterion.

As before, we can again use \texttt{predictmeans} to extract our fitted means and standard errors.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{RCB.lmer.vars}\OtherTok{\textless{}{-}}\FunctionTok{predictmeans}\NormalTok{(RCB,}\StringTok{"variety"}\NormalTok{,}\AttributeTok{plot=}\NormalTok{F)}
\end{Highlighting}
\end{Shaded}

You can then inspect components of the saved list \texttt{RCB.lmer.vars}, such as \texttt{Predicted\ Means} and \texttt{Standard\ Error\ of\ Means}. Results are similar to those we had before. To get the average standard error of variety differences, if desired, we can edit and re run the previous commands:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{RCB.lmer.vars}\SpecialCharTok{$}\StringTok{\textasciigrave{}}\AttributeTok{Standard Error of Differences}\StringTok{\textasciigrave{}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Max.SED   Min.SED  Aveg.SED 
## 0.1765377 0.1363749 0.1384401
\end{verbatim}

We can check on the fit of the model. Residuals and fitted values are available as:
\texttt{resid(RCB.lmer)} and \texttt{fitted(RCB.lmer)}. We should plot several elements of the model to inspect the fit and check the assumptions. Such as the distribution of the residuals, residuals against fitted values and observed against fitted values. Thankfully, \texttt{predictmeans} has a function that takes care of this for us:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{residplot}\NormalTok{(RCB.lmer)}
\end{Highlighting}
\end{Shaded}

All these plots look acceptable to me.

\hypertarget{plotting-residuals-against-field-structure.}{%
\subsection{Plotting residuals against field structure.}\label{plotting-residuals-against-field-structure.}}

It is always worth checking the pattern of the residuals in the field layout. There is a method for doing this in R which can be quite resourceful. It requires row and column to be set to numeric. If you remember we changed these to factors at the start of the tutorial, we can check the current type (or mode) of a column using \texttt{class}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(beet.data}\SpecialCharTok{$}\NormalTok{column)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "factor"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(beet.data}\SpecialCharTok{$}\NormalTok{row)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "factor"
\end{verbatim}

Let's keep these columns as factors but save new columns with row and column as numeric, this is easy in R:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beet.data}\SpecialCharTok{$}\NormalTok{c }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(beet.data}\SpecialCharTok{$}\NormalTok{column)}
\NormalTok{beet.data}\SpecialCharTok{$}\NormalTok{r }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(beet.data}\SpecialCharTok{$}\NormalTok{row)}
\end{Highlighting}
\end{Shaded}

Something that is not as simple is our NA pattern in the data. We need to extract the residuals from our model and then add a column to our dataset, so the plotting function can use our layout and residuals at the same time. This is problematic because we have NAs in our phenotype, which were ignored in the model. This means we have less residuals than we have plots:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{length}\NormalTok{(}\FunctionTok{resid}\NormalTok{(RCB.lmer))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 450
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{length}\NormalTok{(beet.data}\SpecialCharTok{$}\NormalTok{plot)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 460
\end{verbatim}

There's a 10 plot difference, which is explained by the plots with NA for sugar.content:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sum}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(beet.data}\SpecialCharTok{$}\NormalTok{sugar.content))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 10
\end{verbatim}

This is a classic R problem, as it stands R won't let us merge columns of different length (which is a good thing). We could have taken the NA plots out of the data beforehand - but this can mean multiple data sets if we have multiple traits. Here's a neater solution:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#identify which positions contain none NA phenotypes and save as vector called sel}
\NormalTok{sel}\OtherTok{\textless{}{-}}\FunctionTok{which}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(beet.data}\SpecialCharTok{$}\NormalTok{sugar.content))}
\CommentTok{\#create column with all entries marked as NA in main data}
\NormalTok{beet.data}\SpecialCharTok{$}\NormalTok{res}\OtherTok{\textless{}{-}}\ConstantTok{NA}
\CommentTok{\#extract residuals from our model and save in the positions of none NA data points}
\NormalTok{beet.data}\SpecialCharTok{$}\NormalTok{res[sel] }\OtherTok{\textless{}{-}} \FunctionTok{resid}\NormalTok{(RCB.lmer)}
\end{Highlighting}
\end{Shaded}

I've used the \texttt{\#} symbol to break down what each step does. In short - we now have residuals + those 10 NA plots in the correct place in our data and we can plot the residuals against the field layout. You will need to download and load the R package: desplot \citep{R-desplot}.

The command we then run is \texttt{desplot}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{desplot}\NormalTok{(beet.data, res }\SpecialCharTok{\textasciitilde{}}\NormalTok{ c }\SpecialCharTok{+}\NormalTok{ r, }\AttributeTok{text=}\NormalTok{plot, }\AttributeTok{cex=}\FloatTok{0.5}\NormalTok{,}\AttributeTok{gg=}\NormalTok{T,}
        \AttributeTok{main=}\StringTok{"Field layout of residuals from mixed model."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-211-1.pdf}

That's worked well. The command has several arguments (see \texttt{help(desplot)}), the ones we've used were:

\begin{itemize}
\tightlist
\item
  \texttt{beet.data} simply the name of our data
\item
  \texttt{res\ \textasciitilde{}\ c\ +\ r} a formula that tells the function to plot residuals against columns + rows
\item
  \texttt{text=plot} this labels each plot, which is very helpful (note the image is flipped compared to our first inspection of the trial layout)
\item
  \texttt{gg=T} plots via \texttt{ggplot2} which speeds up the plotting function
\end{itemize}

You can see patches of high and low residuals. The residuals can show patterns of field fertility that are not controlled by the experimental design. You can see here such patches of high and low residuals. This is not too surprising as this is a large trial and the only control of field fertility effects in the analysis is through replicates. As these are large, variation within replicates is not so well controlled. That is why we use incomplete block designs.

You can use this plotting method to plot out other elements of the data against the field structure:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  We could have plotted our actual raw trait values and inspect obvious issues in the data.
\item
  We could use it plot the NA structure of the data (NA plots appear to be marked as gray). This would help us look for meaningful NA patterns.
\item
  We could use it to visualise the field structure and trial layout, you can add blocks and reps ect to the image and create figures for publications or presentations.\\
  The package for spatial analysis we shall use later will produce similar plots within R.
\end{enumerate}

\hypertarget{alpha-design-include-block-structure}{%
\subsection{Alpha design (include block structure)}\label{alpha-design-include-block-structure}}

We shall now fit the alpha design block structure. We can compare standard errors and look at the fertility patterns again. Will the alpha design offer any improvements?

To include blocks, fit as random effects using \texttt{lmer}. Add blocks to the previous run of \texttt{lmer} as \texttt{(1\textbar{}block)}.

\begin{rmdnote}
In the experimental design, each block has a unique code. What would happen if blocks were coded 1..17 in each replicate rather than 1..68 over the four replicates? The coding I have used is safer. If the coding is 1..17 in each replicate then you must fit blocks as:\texttt{(1\textbar{}rep/block)}. The \texttt{/} nests the term following it within the term preceding it, so that in this case 4 x 17 block effects are correctly fitted.
\end{rmdnote}

Here's how I ran the analysis:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alpha }\OtherTok{\textless{}{-}}\FunctionTok{lmer}\NormalTok{(sugar.content}\SpecialCharTok{\textasciitilde{}}\NormalTok{variety}\SpecialCharTok{+}\NormalTok{(}\DecValTok{1}\SpecialCharTok{|}\NormalTok{rep)}\SpecialCharTok{+}\NormalTok{(}\DecValTok{1}\SpecialCharTok{|}\NormalTok{block), }\AttributeTok{data=}\NormalTok{ beet.data)}
\FunctionTok{anova}\NormalTok{(alpha)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Analysis of Variance Table
##         npar Sum Sq Mean Sq F value
## variety  114 56.735 0.49767  18.851
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alpha.lmer.vars}\OtherTok{\textless{}{-}}\FunctionTok{predictmeans}\NormalTok{(alpha ,}\StringTok{"variety"}\NormalTok{,}\AttributeTok{plot=}\NormalTok{F)}
\NormalTok{alpha.lmer.vars}\SpecialCharTok{$}\StringTok{\textasciigrave{}}\AttributeTok{Standard Error of Differences}\StringTok{\textasciigrave{}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Max.SED   Min.SED  Aveg.SED 
## 0.1599940 0.1206777 0.1245988
\end{verbatim}

This gives me an average standard error of a variety difference of 0.1246, a reduction of about 10\% compared to the RCB design. The average variance of a difference from the alpha design divided by that from the RCB design is termed efficiency and is 123\%.

\hypertarget{rows-and-column-analysis}{%
\subsection{Rows and column analysis}\label{rows-and-column-analysis}}

As a final refinement, model the random effects as

\texttt{(1\textbar{}rep/row)+(1\textbar{}rep:column)}

What block effects are we fitting. Do these make sense?

If you compare the variance components using the two different methods, in this case you will find they are very similar. In this case, I find the average standard error of the variety means is 0.1087824. Did you get the same?

As an alternative, to compare the different analyses, and in the interests of speed, we can tabulate the F statistic from the analysis of variance:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(RCB.lmer)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Analysis of Variance Table
##         npar Sum Sq Mean Sq F value
## variety  114 62.935 0.55206  14.842
\end{verbatim}

For the other designs I get:

\begin{itemize}
\tightlist
\item
  RCB = 14.842
\item
  Alpha design = 18.851
\item
  Row and column design 24.442
\end{itemize}

\begin{rmdquiz}
\textbf{6:}
Can you think of any additional models? What do the results say about the original alpha blocking for this trial?
\end{rmdquiz}

The process of changing the trial design after the analysis is called post-blocking. For studies of alternative designs: to suggest improvements to the blocking structures used in the future, it is acceptable. However, it should not be used routinely to estimate variety means as the standard errors, if not the effects themselves, will be biased by continually searching to find the better fitting model. You should stick to the analysis appropriate to the experimental design.

\hypertarget{spatial-analysis}{%
\section{Spatial analysis}\label{spatial-analysis}}

Field fertility effects are not typically refined to the blocks, reps, rows and columns of your trial design and you may need a more sophisticated method to capture a spatial effect in the field. We saw earlier in the plot of our residuals in the rows and columns of the trial layout that there were clear non random high and low areas in our field associated with the phenotype:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{desplot}\NormalTok{(beet.data, res }\SpecialCharTok{\textasciitilde{}}\NormalTok{ c }\SpecialCharTok{+}\NormalTok{ r, }\AttributeTok{text=}\NormalTok{plot, }\AttributeTok{cex=}\FloatTok{0.5}\NormalTok{,}\AttributeTok{gg=}\NormalTok{T, }\AttributeTok{main=}\StringTok{"Residuals from mixed model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-217-1.pdf}

In that model we just included replicate and clearly missed patterns in our data. If we were to plot the residuals after fitting block, rows and column, hopefully we would account for what was missed. However, in some cases the trial design components still don't adjust appropriately for the noise in the data and we might be able to find a better model by running a spatial analysis.

Autoregressive models of order 1 (AR1: the standard methods of spatial analysis of yield trials) are available in GenStat or ASReml and guides to their use in these packages are available. ASReml is available in an R version, so it is easy to start using it if you know R. Unfortunately, there is no free version. It may be possible to fit spatial models with lmer too, though it is beyond my understanding and I have yet to find anyone who knows how to.

We shall try a slightly different approach using the free R package from Wageningen (WUR); SpATS \citep{SpATS2017}. This models fertility effects using a two dimensional spline. A one-dimensional spline is a bendy curve, so a two-dimensional spline is a sheet, bent to fit fertility patterns running over the field. This is conceptually simple, but the statistical modeling is difficult. You will need to install the package to your R and load it:

\texttt{install.packages(“SpATS”)}

\texttt{library(SpATS)}

SpATS requires rows and columns to be present as both numeric and factors. Moreover, they must both be part of the dataframe. If you remember, we have already met these requirements:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(beet.data}\SpecialCharTok{$}\NormalTok{row)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "factor"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(beet.data}\SpecialCharTok{$}\NormalTok{r)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "numeric"
\end{verbatim}

We need to specify the number of knots in our spatial fit. I believe this controls how flexible our bendy sheet is. Here we specify half the number of rows and columns in our design.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Specify the number of segments used in fitting the spline}
\NormalTok{nrow }\OtherTok{\textless{}{-}} \FunctionTok{max}\NormalTok{(beet.data}\SpecialCharTok{$}\NormalTok{r)}
\NormalTok{ncol }\OtherTok{\textless{}{-}} \FunctionTok{max}\NormalTok{(beet.data}\SpecialCharTok{$}\NormalTok{c)}
\NormalTok{nseg.row }\OtherTok{\textless{}{-}}\NormalTok{ nrow}\SpecialCharTok{/}\DecValTok{2}
\NormalTok{nseg.col }\OtherTok{\textless{}{-}}\NormalTok{ ncol}\SpecialCharTok{/}\DecValTok{2}
\end{Highlighting}
\end{Shaded}

Now fit the model using the main function of SpATS, which is conveniently called \texttt{SpATS}. Options are spread over several lines for ease of reading:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SpATS.out }\OtherTok{\textless{}{-}} \FunctionTok{SpATS}\NormalTok{(}\AttributeTok{response =} \StringTok{"sugar.content"}\NormalTok{,}
    \AttributeTok{genotype =} \StringTok{"variety"}\NormalTok{,}
    \AttributeTok{spatial =} \SpecialCharTok{\textasciitilde{}}\FunctionTok{PSANOVA}\NormalTok{(c,r, }\AttributeTok{nseg =} \FunctionTok{c}\NormalTok{(nseg.col, nseg.row)),}
    \AttributeTok{genotype.as.random =} \ConstantTok{FALSE}\NormalTok{,}
    \AttributeTok{random =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ row }\SpecialCharTok{+}\NormalTok{ column,}
    \AttributeTok{data =}\NormalTok{ beet.data,}
    \AttributeTok{control =} \FunctionTok{list}\NormalTok{(}\AttributeTok{monitoring=}\DecValTok{0}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

That looks complicated. So let's break it down step by step:

\begin{itemize}
\tightlist
\item
  \texttt{response\ =\ "sugar.content"} is our phenotype, for some reason it has be included in speech marks.
\item
  \texttt{genotype\ =\ "variety"} the name of our genotype factor\\
\item
  \texttt{spatial\ =\ \textasciitilde{}PSANOVA(c,r,\ nseg\ =\ c(nseg.col,\ nseg.row))} this is a formula for defining our spatial P-Spline model. \texttt{PSANOVA} means you are running a P-spline ANOVA, where the smoothing consists of five components each dependent on a single smoothing parameter (we'll get on to that next).
\item
  \texttt{genotype.as.random\ =\ FALSE} simply whether we express genotype as random or fixed
\item
  \texttt{random\ =\ \textasciitilde{}\ row\ +\ column} here we specify random factors of our model in a formula. We could have added block and rep here. We can add fixed factors too in the same fashion.
\item
  \texttt{data\ =\ beet.data} our dataset.
\item
  \texttt{control\ =\ list(monitoring=0)} setting monitoring to 0 here prevents a lot of unwanted output being printed during the fitting. We could control other elements of the fitting via the same means.
\end{itemize}

We can inspect the model with the following:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(SpATS.out,}\AttributeTok{which =} \StringTok{"variance"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Spatial analysis of trials with splines 
## 
## Response:                   sugar.content
## Genotypes (as fixed):       variety   
## Spatial:                    ~PSANOVA(c, r, nseg = c(nseg.col, nseg.row))
## Random:                     ~row + column
## 
## 
## Number of observations:        450
## Number of missing data:        10
## Effective dimension:           156.91
## Deviance:                      -741.432
## 
## Variance components:
##                Variance            SD     log10(lambda)
## row           6.944e-03     8.333e-02           0.44462
## column        5.932e-03     7.702e-02           0.51301
## f(c)          3.856e-04     1.964e-02           1.70007
## f(r)          9.192e-03     9.587e-02           0.32282
## f(c):r        3.107e-04     1.763e-02           1.79390
## c:f(r)        2.775e-04     1.666e-02           1.84291
## f(c):f(r)     3.380e-03     5.814e-02           0.75725
##                                                        
## Residual      1.933e-02     1.390e-01
\end{verbatim}

Here we can examine the variance components from the model, there are 8 in total:

\begin{itemize}
\tightlist
\item
  \texttt{row} and \texttt{colunm} treated as random effects
\item
  the residual variance
\end{itemize}

Then the 5 smooth effects of the spatial trend:

\begin{itemize}
\tightlist
\item
  \texttt{f(c)} and \texttt{f(r)} are main smooth effects
\item
  \texttt{f(c):r} and \texttt{c:f(r)} are smooth varying coefficient terms
\item
  \texttt{f(c):f(r)} is the smooth-by-smooth interaction between the row and column trend
\end{itemize}

The interpretation of this part can be useful for looking at how much variation has not been accounted for in our model; shown by the residual variance. The column \texttt{log10(lambda)} shows the log10 of the proportion of residual variance to each model component. The lower the value, the greater proportion of the model term variance compared to the residual variance.

Let's take a look at the dimensions of the model which are informative of the complexity of the spatial fit.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(SpATS.out,}\AttributeTok{which =} \StringTok{"dimensions"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Spatial analysis of trials with splines 
## 
## Response:                   sugar.content
## Genotypes (as fixed):       variety   
## Spatial:                    ~PSANOVA(c, r, nseg = c(nseg.col, nseg.row))
## Random:                     ~row + column
## 
## 
## Number of observations:        450
## Number of missing data:        10
## Effective dimension:           156.91
## Deviance:                      -741.432
## 
## Dimensions:
##               Effective     Model     Nominal     Ratio     Type
## variety           114.0       114         114      1.00        F
## Intercept           1.0         1           1      1.00        F
## row                13.0        18          16      0.81        R
## column             19.6        28          26      0.75        R
## c                   1.0         1           1      1.00        S
## r                   1.0         1           1      1.00        S
## rc                  1.0         1           1      1.00        S
## f(c)                0.5        15          15      0.03        S
## f(r)                0.9        10          10      0.09        S
## f(c):r              0.6        15          15      0.04        S
## c:f(r)              0.2        10          10      0.02        S
## f(c):f(r)           4.2       150         150      0.03        S
##                                                                 
## Total             156.9       364         360      0.44         
## Residual          293.1                                         
## Nobs                450                                         
## 
## Type codes: F 'Fixed'    R 'Random'    S 'Smooth/Semiparametric'
\end{verbatim}

The table is a little complicated. We can see all the components included in the model. We can see what `Type' each component is, either `Fixed', `Random' or a `Smooth' component. The other key columns show: `Effective' which can be considered the effective degrees of freedom of that component, `Model' is the number of parameters of that component,`Nominal' is the same as `Model' except for random factors which have -1 and `ratio' is the proportion of effective and nominal dimension.

There are 8 components to the spatial trend, the 5 smooth components discussed above and the linear effects of row and column and their linear interaction.

The effective dimensions (ED) are really informative for the 5 smooth components. The higher the ED of these components (and thus the higher the value in the `ratio' column) indicates that more of the spatial trend has been captured by that component. It's an indicator that the fitted surface of that component is more complex than the others and has more spatial patterns.

In our trial you can see that out of the smooth components, the smooth-by-smooth interaction between the row and column f(c):f(r) had the highest ED. This is typically informative of a more complicated spatial effect, not limited to rows and columns. We can interpret this for ourselves with the excellent \texttt{plot()} function of \texttt{SpATS}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(SpATS.out)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-224-1.pdf}

These plots are really useful for a more straight forward interpretation of our spatial model. The `Raw data' plot is the phenotype plotted out in the row and column structure of our trial. That pattern looks familiar. We saw there was an area of the field associated with lower phenotype values in our residual plot earlier. I would call the worse part of this area between columns 0-10 and rows 4-10. Now if we look at the residual plot on the top right, we should check for any remaining patterns. It looks like our model has been much better at clearing up the residual noise. What is left appears to be mostly random.

We can then compare our residual plot to the Fitted Spatial Trend. This is the easiest way of assessing how strong our spatial trend is (or how well the model has captured it). Compare the axis values of each plot. The spatial trend is of the same magnitude as the residuals. That's typically a sign of a strong spatial effect that has been accounted for by the model. If the fitted spatial trend were a few orders of magnitude lower than the raw data and residual values, we'd know that we had an unidimensional trend.

You can see in the Fitted Spatial Trend plot that there is strong and complex trend across the trial. This explains why the effective dimensions for the more complicated smoothing component (the smooth-by-smooth interaction) were highest. We can also clearly see the region of the field that had low phenotypic values (in dark blue) and we now know that the spatial effect associated with the trial partially contributed to that issue. We've accounted for that in our model now.

Line means are accessed from our SpATS model using:

\texttt{predict.SpATS(SpATS.out,which=\ "variety")}

\texttt{predict.SpATS} works to predict fixed and random effect. Standard errors of effects are reported, and these can be averaged, but unfortunately \texttt{predictmeans} does not work with the output so we cannot currently get the standard errors of differences. For the time being, the best we can do is compare average standard errors (rather than standard errors of differences).

\texttt{mean(predict.SpATS(SpATS.out,which=\ "variety"){[},7{]})}

I get 0.07984206 compared to 0.09437533 for the row and column design, which is an impressive improvement in this case. We would expect that, as the row and column design would not of captured the complexity of spatial trend that we found through SpATs.

SpATs is a relatively new package. Therefore, the literature and guidance surrounding its use and interpretation is limited. There is an associated paper with the package \citep{rodríguezálvarez2016}, it's informative but not easy reading. The paper \citet{velazco_modelling_2017} has some really nice examples and it clear to follow. I've used definitions and explanations from both for forming this section.

\hypertarget{Cross-site-analysis}{%
\chapter{Cross sites analysis in R.}\label{Cross-site-analysis}}

We shall analyse the data from the TG association mapping panel to examine methods for analysing data across sites. For this we shall use the \texttt{lmer} package and (optionally) the \texttt{emmeans} package. Running \texttt{emmeans} here can add a lot to the run-time.

It would be better to use ASReml or GenStat, or other commercial packages to analyse these data, but \texttt{lmer} in R is free and readily available.

We shall compare variety effects from various types of analyses -- BLUE / BLUP / one stage / two stage / weighted / and see what sort of a difference it makes?

\hypertarget{blues-blups-and-one-stage-analysis}{%
\section{BLUEs, BLUPs and one stage analysis}\label{blues-blups-and-one-stage-analysis}}

To start:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Load the \texttt{lme4} package to your R.
\item
  We could use \texttt{predictmeans} to extract our predicted means. However, for just that purpose, it can take a while to run. Therefore, install and load the package \texttt{emmeans} \citep{R-emmeans} instead, which runs faster for this method.
\item
  Change your working directory to the location of today's data files or copy today's data files to your current working directory.
\item
  Read in the csv file ``TG\_all\_data\_for\_stage\_1.csv'' and save as an object called TG
\end{enumerate}

We need to convert our experimental design columns to factors, to do by running the following:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{)\{}
\NormalTok{  TG[,i]}\OtherTok{\textless{}{-}}\FunctionTok{factor}\NormalTok{(TG[,i])}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

Now inspect the data with:

\texttt{summary(TG)}

\texttt{hist(TG\$YLD\_ADJ)}

\texttt{hist(log(TG\$YLD\_ADJ))}

\texttt{boxplot(TG\$YLD\textasciitilde{}TG\$SITE)}

Any comments on the data?

The conventional way to analyse these data would be to first analyse each site (there are six here) separately, then take the line means from each site and analyse these in a second analysis fitting terms for varieties and sites with the interaction (varieties x sites) used as error. This approach, with minor modifications is still common. If there are no missing variety-site combinations, the estimates of average variety effects (the BLUEs) are identical to simply taking the average across sites. There is nothing wrong with it, but we ought to be able to do better. It is described as a two-stage analysis.

Our data is of individual plot measurements at all sites. We can mimic simple analysis by:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TG.}\FloatTok{2.}\NormalTok{stage.mimic}\OtherTok{\textless{}{-}}\FunctionTok{lm}\NormalTok{(YLD\_ADJ}\SpecialCharTok{\textasciitilde{}}\NormalTok{VAR}\SpecialCharTok{+}\NormalTok{SITE}\SpecialCharTok{+}\NormalTok{SITE}\SpecialCharTok{:}\NormalTok{VAR}\SpecialCharTok{+}\NormalTok{SITE}\SpecialCharTok{:}\NormalTok{REP,}\AttributeTok{data=}\NormalTok{TG)}
\end{Highlighting}
\end{Shaded}

\begin{rmdquiz}
\textbf{1} Why do we have the term SITE:REP rather than just REP?
Have a look at the analysis of variance table. Does it make sense? Are the F tests carried out correctly?
\end{rmdquiz}

We shall now repeat this analysis using \texttt{lmer}. Which terms should be fixed and which random?

I fitted the model below. For simplicity and time, I've ignored the row and column structure of the experimental designs. This may take a little time to run.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TG.res}\FloatTok{.1}\OtherTok{\textless{}{-}}\FunctionTok{lmer}\NormalTok{(YLD\_ADJ}\SpecialCharTok{\textasciitilde{}}\NormalTok{(}\DecValTok{1}\SpecialCharTok{|}\NormalTok{SITE)}\SpecialCharTok{+}\NormalTok{(}\DecValTok{1}\SpecialCharTok{|}\NormalTok{SITE}\SpecialCharTok{:}\NormalTok{REP)}\SpecialCharTok{+}\NormalTok{(}\DecValTok{1}\SpecialCharTok{|}\NormalTok{SITE}\SpecialCharTok{:}\NormalTok{VAR)}\SpecialCharTok{+}\NormalTok{VAR,}\AttributeTok{data=}\NormalTok{TG)}
\end{Highlighting}
\end{Shaded}

Inspect the results:

\texttt{summary(TG.res.1)}

To inspect only the variance components we can use:

\texttt{VarCorr(TG.res.1)} on its own prints the standard deviations. Whereas, \texttt{print(VarCorr(TG.res.1),comp=c("Variance")} will give you variance:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(}\FunctionTok{VarCorr}\NormalTok{(TG.res}\FloatTok{.1}\NormalTok{),}\AttributeTok{comp=}\FunctionTok{c}\NormalTok{(}\StringTok{"Variance"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  Groups   Name        Variance
##  SITE:VAR (Intercept)  28.042 
##  SITE:REP (Intercept)  13.391 
##  SITE     (Intercept) 379.378 
##  Residual              62.049
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{ (TG.res}\FloatTok{.1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Analysis of Variance Table
##     npar Sum Sq Mean Sq F value
## VAR  385 108474  281.75  4.5407
\end{verbatim}

Gives us a test for the significance of the variety term.

\begin{rmdnote}
The authors of \texttt{lme4} have purposely excluded P values from these significance tests. Most people however do like to look at P values. There are several ways to obtain a P value with this test. The easiest is to download another package called \texttt{lmerTest} and load it to your R. Then next time when you run \texttt{lmer()} followed by \texttt{anova()} you will see an estimated P value.

Or you could look up the p-value via multiplying the F statistic by the degrees of freedom and treat the result as a chi-sq test statistic with the same number of df. This is perfectly acceptable for large experiments, as here.
\end{rmdnote}

To get out the variety means we use this function from \texttt{emmeans} (make sure you have this package loaded).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TG.BLUE}\OtherTok{\textless{}{-}}\FunctionTok{emmeans}\NormalTok{(TG.res}\FloatTok{.1}\NormalTok{, }\SpecialCharTok{\textasciitilde{}}\NormalTok{ VAR)}
\end{Highlighting}
\end{Shaded}

This gives a warning: you are advised to alter the options to emmeans: \texttt{emm\_options(pbkrtest.limit\ =\ 4688)} but run time is then increased greatly, and in this case it makes no difference to the estimate of the BLUEs, so we shall ignore the warning.

We could have done this using \texttt{predictmeans} and not run into these warnings, however, it takes about 5 minutes to run. Let's check that the BLUEs we can get directly from the \texttt{lmer} output agree with those from \texttt{emmeans}. There's a slightly messy workaround for extracting the BLUEs directly from lme4, remember you have to add the first variety to the rest of the data:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#extract fixed effects and add intercept to each}
\NormalTok{TG.BLUE2}\OtherTok{\textless{}{-}}\FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{fixef}\NormalTok{(TG.res}\FloatTok{.1}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{fixef}\NormalTok{(TG.res}\FloatTok{.1}\NormalTok{)[}\DecValTok{1}\NormalTok{])}
\CommentTok{\#change name of column to BLUE }
\FunctionTok{names}\NormalTok{(TG.BLUE2)}\OtherTok{\textless{}{-}}\NormalTok{(}\StringTok{"BLUE"}\NormalTok{)}
\CommentTok{\#the intercept (first variety) is x2 what it should be, so fix }
\NormalTok{TG.BLUE2[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]}\OtherTok{\textless{}{-}}\NormalTok{TG.BLUE2[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]}\SpecialCharTok{/}\DecValTok{2}
\CommentTok{\#plot data against emmeans method}
\FunctionTok{plot}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(TG.BLUE)[,}\DecValTok{2}\NormalTok{],TG.BLUE2}\SpecialCharTok{$}\NormalTok{BLUE)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-234-1.pdf}

Looks convincing! Both methods produce the same results. In passing we can check that the means from \texttt{emmeans} agree with simply getting the fixed effects from lm.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(TG.BLUE)[,}\DecValTok{2}\NormalTok{],}\FunctionTok{fixef}\NormalTok{(TG.res}\FloatTok{.1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-235-1.pdf}

Comments?

We want to save the BLUEs for comparison with others. We can do this using the \texttt{write.table} function:

\texttt{write.table(TG.BLUE,"TG.BLUE.txt")}

This saves a text file to your working directory, open it now and copy and paste it into excel for comparison later.

In principle, we could get the standard errors of differences of comparisons between variety pairs in the same way as for the sugar beet trial we analysed, I would recommend going back to \texttt{predictmeans} for this.

This is a one-stage analysis: we have taken the raw plot data and produced an across-sites BLUEs in a single analysis. We made some simplifications:

\begin{itemize}
\item
  We have ignored row and column effects within each site.
\item
  We have assumed a constant error variance at each site. This is the most important omission.
\item
  What have we assumed about replicate effects over the whole analysis?
\item
  We have lumped countries and years together into a single term: sites.
\end{itemize}

Next, we shall repeat the analysis, but treat the varieties as random effects to estimate BLUPs. Paste the BLUPs into Excel, and record the variance components again

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TG.res}\FloatTok{.2}\OtherTok{\textless{}{-}}\FunctionTok{lmer}\NormalTok{(YLD\_ADJ}\SpecialCharTok{\textasciitilde{}}\NormalTok{(}\DecValTok{1}\SpecialCharTok{|}\NormalTok{SITE)}\SpecialCharTok{+}\NormalTok{(}\DecValTok{1}\SpecialCharTok{|}\NormalTok{SITE}\SpecialCharTok{:}\NormalTok{REP)}\SpecialCharTok{+}\NormalTok{(}\DecValTok{1}\SpecialCharTok{|}\NormalTok{SITE}\SpecialCharTok{:}\NormalTok{VAR)}\SpecialCharTok{+}\NormalTok{(}\DecValTok{1}\SpecialCharTok{|}\NormalTok{VAR),}\AttributeTok{data=}\NormalTok{TG)}
\FunctionTok{print}\NormalTok{(}\FunctionTok{VarCorr}\NormalTok{(TG.res}\FloatTok{.2}\NormalTok{),}\AttributeTok{comp=}\FunctionTok{c}\NormalTok{(}\StringTok{"Variance"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  Groups   Name        Variance
##  SITE:VAR (Intercept)  28.066 
##  VAR      (Intercept)  35.551 
##  SITE:REP (Intercept)  13.395 
##  SITE     (Intercept) 379.817 
##  Residual              62.025
\end{verbatim}

To get just the random effects for variety rather than a long list of all effects:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TG.BLUP}\OtherTok{\textless{}{-}}\FunctionTok{ranef}\NormalTok{(TG.res}\FloatTok{.2}\NormalTok{)}\SpecialCharTok{$}\NormalTok{VAR}\SpecialCharTok{+}\FunctionTok{fixef}\NormalTok{(TG.res}\FloatTok{.2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

How do the BLUPs compare to the BLUEs? Look at the correlations and plot:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(TG.BLUP[,}\DecValTok{1}\NormalTok{],}\FunctionTok{data.frame}\NormalTok{(TG.BLUE)[,}\DecValTok{2}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9953824
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(TG.BLUP[,}\DecValTok{1}\NormalTok{],}\FunctionTok{data.frame}\NormalTok{(TG.BLUE)[,}\DecValTok{2}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-238-1.pdf}

To test the significance of random effects in mixed models, we use a likelihood ratio test (LRT). This is a very commonly used method in statistics and quantitative genetics. The difference between 2 x the log-likelihood of two models is approximately distributed as a chi-squared statistic with df (for the test) equal to the difference in df between the two models. This method is not appropriate for testing the significance of fixed effects in mixed models however. To get the log-likelihood of the model you should have just fitted:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{logLik}\NormalTok{(TG.res}\FloatTok{.2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'log Lik.' -17385.84 (df=6)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{logLik}\NormalTok{(}\FunctionTok{lmer}\NormalTok{(YLD\_ADJ}\SpecialCharTok{\textasciitilde{}}\NormalTok{(}\DecValTok{1}\SpecialCharTok{|}\NormalTok{SITE)}\SpecialCharTok{+}\NormalTok{(}\DecValTok{1}\SpecialCharTok{|}\NormalTok{SITE}\SpecialCharTok{:}\NormalTok{REP)}\SpecialCharTok{+}\NormalTok{(}\DecValTok{1}\SpecialCharTok{|}\NormalTok{VAR),}\AttributeTok{data=}\NormalTok{TG))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'log Lik.' -17499.43 (df=5)
\end{verbatim}

The likelihoods can be difficult to interpret. Higher values (i.e.~less negative values) indicate a better fit to the data. The original mode therefore gave a better fit but used up 1 df to do this. This is always the case: adding parameters gives a better fit (a smaller error variance in linear modeling) but each parameter requires 1 df. In mixed modeling, each variance component has 1df -- to estimate the variance.

The LRT is therefore 2 x (-17385.84 - -17499.43) with (6 - 5) df.

\texttt{pchisq()}

will return probabilities associated with chi-squared tests. There are two slight complications, Firstly pchisq returns the cumulative probability distribution: the left hand side of the distribution. You require 1 minus this for the significance test. Secondly, variance components are always positive, never negative (though their estimates can be). For this reason, the chi-sq test gives a false impression. We would always dismiss a negative component as non-significant. To adjust, for this asymmetry we halve the probability that you conventionally look up. So a p-value of 0.24 would be treated as 0.12. This isn't as esoteric as you may think: the same correction is often required in linkage analysis. In this case, there was never any doubt that this interaction was going to be significant.

\hypertarget{one-stage-analysis-with-weights}{%
\subsection{One stage analysis with weights}\label{one-stage-analysis-with-weights}}

So far, we have treated each observation as if it was known with equal precision. This isn't true. Some of the trials were better than others. The WT column in the data is a weight for each observation. It is defined as 1/variance of the observation. This is a very common weight to use. As an estimate I have used the error term from an earlier analysis of each trial.

\begin{verbatim}
SITE          Ve
CALLOW_2011   72.63
FRANCE_2010   60.81
FRANCE_2011   23.92
LGE_2010      12.64
LGE_2011      25.41
NIAB_2010     14.72
\end{verbatim}

These were estimated from a full analysis which included row and column terms (just as in the sugar beet example) so it isn't a perfect match for the model we are using. It serves to illustrate the method. We simply tell \texttt{lmer} to include the weights:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TG.res}\FloatTok{.3}\OtherTok{\textless{}{-}}\FunctionTok{lmer}\NormalTok{(YLD\_ADJ}\SpecialCharTok{\textasciitilde{}}\NormalTok{(}\DecValTok{1}\SpecialCharTok{|}\NormalTok{SITE)}\SpecialCharTok{+}\NormalTok{(}\DecValTok{1}\SpecialCharTok{|}\NormalTok{SITE}\SpecialCharTok{:}\NormalTok{REP)}\SpecialCharTok{+}\NormalTok{(}\DecValTok{1}\SpecialCharTok{|}\NormalTok{SITE}\SpecialCharTok{:}\NormalTok{VAR)}\SpecialCharTok{+}\NormalTok{VAR,}\AttributeTok{weights =}\NormalTok{ WT,}\AttributeTok{data=}\NormalTok{TG)}
\end{Highlighting}
\end{Shaded}

\texttt{emmeans} will provide the BLUEs to compare results with the previous unweighted analysis.

\begin{rmdquiz}
\textbf{2} Compare the standard errors from the two analyses as well as the BLUEs themselves. What do you find?
\end{rmdquiz}

Finally, we shall export the BLUPs for each site. As an alternative we could make the sites x varieties effect fixed and estimate BLUEs for each site, but in practice this would give us the same result as if we analysed each site separately in a two-stage analysis. There are two possible advantages to using BLUPs. Firstly we automatically get an estimate for every variety, including those with missing data. Secondly, these estimates will be shrunk towards the overall sites BLUE depending on the magnitude of the sites x varieties interaction. This seems reasonable: if there is no interaction then whatever site we want to select for, we should just select on the overall mean yet if the interactions are very large we should select on the estimated effect from the single site analysis. Treating the interaction as random, we are in effect borrowing information from the other sites depending on how high the correlation is between site means. (This is a very similar approach to treating variety means at each site as separate traits and selecting on one trait (i.e.~site) while incorporating information based on the correlation with other traits (i.e.~sites). We shall return to correlated characters later in the course.)

We just need to export the results from the analysis we have already carried out:

\texttt{ranef(TG.res.1)\$"SITE:VAR"}

This is similar to the command we used to export the variety means, except by trial and error, I found I needed to include the quotation mark to prevent R interpreting `:' as some form of separator.

The output is quite long so this time we shall export to a file. Make sure you know what your working directory is, and change it required.

\texttt{getwd()}

\texttt{write.csv(ranef(TG.res.1)\$"SITE:VAR","site.var.csv")}

\texttt{write.csv} is a version of \texttt{write.table} and writes csv files (so they can be read directly into Excel).

After this, there is some tedious formatting of the results using vlookup since not all varieties are present in all sites. We require a column for each site, but we can't simply cut and paste because not all varieties are present at all sites. I've done this for you. My results are in `results TG R.xlsx' in the Chapter 5 data folder downloaded from the website. We want to compare the BLUPs we've estimated for each site with the means we can estimate from the site by site analyses. I have previously carried out the analysis for each site and results are also in the spreadsheet. To compare means we also need to add the average effect for each variety (from emmeans) onto the BLUP. I've also done this. Strictly, we should add on the average site (not variety) effect too, but as we are only interested in the correlation between sites this doesn't matter.

I've done much of the tedious work for you here. It is the results that are important to think about. Correlating my shrunk estimates of variety effects with the individual site analysis I get:

\begin{verbatim}
CALLOW_2011 0.93
FRANCE_2010 0.94
FRANCE_2011 0.95
LGE_2010    0.87
LGE_2011    0.94
NIAB_2010   0.83
\end{verbatim}

What do you think? What has happened to the correlation between sites:

On site specific BLUEs

\begin{verbatim}
              CALL11    FR10    FR11    LGE10   LG11    NIAB10
CALLOW_2011   1.00      0.39    0.46    0.23    0.57    0.52
FRANCE_2010             1.00    0.59    0.54    0.47    0.41
FRANCE_2011                     1.00    0.51    0.48    0.40
LGE_2010                                1.00    0.37    0.32
LGE_2011                                        1.00    0.53
NIAB_2010                                               1.00
\end{verbatim}

Average 0.45

On shrunk estimates:

\begin{verbatim}
              CALL11    FR10    FR11    LGE10   LG11    NIAB10
CALLOW_2011   1.00      0.74    0.75    0.71    0.80    0.79
FRANCE_2010             1.00    0.83    0.82    0.79    0.79
FRANCE_2011                     1.00    0.87    0.79    0.79
LGE_2010                                1.00    0.79    0.79
LGE_2011                                        1.00    0.84
NIAB_2010                                               1.00
\end{verbatim}

Average 0.79

Discuss!

Compare the results here with the variance components we recorded earlier.

\begin{rmdquiz}
\textbf{3} You are a grower and must select seed for the coming season. Assume, fortuitously, that one of the trials was located on your farm. Will you select on variety means across all sites, the variety mean on your farm, the shrunk estimate of yield for your farm?
\end{rmdquiz}

As far as I am aware, the only country in which shrunk estimates are routinely provided to growers is Australia. In the UK, BLUEs across sites and years, and at individual trial sites, are published.

There is a method for getting an estimate of the expected correlation in variety mean between sites (unshrunk) from the variance components. I shall describe it, but we shall return to this later in the course. Here, I estimate 0.38. The minimum and maximum observed values are 0.23 and 0.59 and the mean 0.45, so this estimate seems reasonable.

Most across-environment analyses, including the UK national and recommended list trials for example, do less analysis than we have done so far: they simply treat varieties as fixed, and estimate the BLUEs (though they won't call them that). This will usually be done in a two-stage analysis, and there will be no weighting. Although the overall means vary little in our example, the predictions at each site can vary a lot. In addition, in many official trials, and breeders' trials, the variance associated with varieties can be much smaller than in this genetically very variable GWAS panel. There is usually a much greater lack of balance between varieties over sites and years, so the difference between methods could be greater than we see in this example.

\hypertarget{weighted-two-stage-analysis}{%
\section{Weighted two-stage analysis}\label{weighted-two-stage-analysis}}

A comprehensive one-stage analysis, as implemented in Australia, will include the full experimental design at each site, with separate error variances for each site, analyse the data at site level with an AR1 x AR1 spatial model, incorporate information on relationships between varieties, and implement a more sophisticated G x E analysis than we have used here. Computation is in ASReml. That is beyond both the scope of this course and beyond me. Hans-Peter Piepho advocates use of the two-stage analysis, but weighting by the precision of the variety means, or better by the full variance-covariance matrix of variety effects. This has the advantage of making the analysis of individual sites much more flexible. Genetic relationships between varieties can also be incorporated. This too is beyond me. We shall carry out a simplified version:

Approximate weights are 1/(variance a variety mean). These could be obtained from the output of emmeans for each site. Emmeans reports the standard error of the variety mean, so the weight would be \(1/se^2\). We could analyse each site in turn here, fitting row and column effects in addition to replicates, or carrying out a spatial analysis. However we shall approximate the weights as r/(error variance) where we shall use the same error variance reported earlier.

Read in the csv file: `site\_means\_for\_stage\_2.csv', you should be able to say what each of the following commands does:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#to run the below line, you will need to remove: data/}
\NormalTok{TG2}\OtherTok{\textless{}{-}}\FunctionTok{read.csv}\NormalTok{(}\StringTok{"data/site\_means\_for\_stage\_2.csv"}\NormalTok{,}\AttributeTok{header=}\NormalTok{T)}
\NormalTok{TG2}\SpecialCharTok{$}\NormalTok{VAR}\OtherTok{\textless{}{-}}\FunctionTok{factor}\NormalTok{(TG2}\SpecialCharTok{$}\NormalTok{VAR)}
\NormalTok{TG2}\SpecialCharTok{$}\NormalTok{SITE}\OtherTok{\textless{}{-}}\FunctionTok{factor}\NormalTok{(TG2}\SpecialCharTok{$}\NormalTok{SITE)}
\NormalTok{TG2.res}\FloatTok{.1}\OtherTok{\textless{}{-}}\FunctionTok{lmer}\NormalTok{(YLD\_ADJ\_av}\SpecialCharTok{\textasciitilde{}}\NormalTok{(}\DecValTok{1}\SpecialCharTok{|}\NormalTok{SITE)}\SpecialCharTok{+}\NormalTok{VAR,}\AttributeTok{weights =}\NormalTok{ WT,}\AttributeTok{data =}\NormalTok{ TG2)}
\NormalTok{TG2.BLUE}\OtherTok{\textless{}{-}}\FunctionTok{emmeans}\NormalTok{(TG2.res}\FloatTok{.1}\NormalTok{,}\SpecialCharTok{\textasciitilde{}}\NormalTok{VAR)}
\end{Highlighting}
\end{Shaded}

\begin{rmdquiz}
\textbf{4} Export the data and compare with the BLUEs from the previous analyses. Or plot / correlate with previous results we have saved. You should be able to edit the commands above to repeat these analyses with / without weighting and with varieties as a fixed or random term.
\end{rmdquiz}

\hypertarget{ammi}{%
\section{AMMI}\label{ammi}}

The additive main effect, multiplicative interaction model has become the standard method of examining genotype by environment interactions. It has some limitations, however. It cannot handle missing data and it assumes that the G x E terms, varieties x sites in our example, are fixed effects. There are random effects alternatives (more complicated), but we shall stick to the standard model.

We shall illustrate the approach using basic R commands. There are dedicated packages, both using R and standalone, but the basic approach will, I hope, help with understanding what is going on, at the expense of learning a few more R commands (but you can copy from here).

The csv file `AMMI.csv' has variety means from the individual site analyses for the TG data. I've deleted varieties with missing data. Most of these were only present at a single site so would not contribute to the pattern of G x E. For varieties with only a single missing observation, I could have inserted the predicted effect for that site: mean + variety effect + site effect (i.e.~assuming no interaction term at that site.)

I've also added countries of origin -- deleting those for which country was unknown. The final data set has 371 varieties with three countries of origin, tested at six sites: two in each country of origin (in different years). We are interested in examining whether, on average, varieties tend to perform best at home, rather than playing away.

Read the data in and save as an object called: \texttt{TG.AMMI}

To simplify the analysis and inspection of results, we shall work on the performance of varieties averaged over their origin, rather than on the individual variety performance. As we are working on GxE, we also want deviations from the trial means and from the country of origin means. We are therefore aiming for a matrix of 3 rows and 6 columns. We could do these calculations in Excel, and in truth, like most of you I would have found that faster than working out the code below in R, but I've done it, and can copy it if I need to do a similar exercise again.

Before we produce the required deviations, let's see if any differences between countries of origin appear genuine.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  How can we test for this? (Answer is on the next page if you want to cheat, but have a think first).
\item
  Should we treat differences between varieties within countries as fixed or random effects?
\item
  Carry out the test for each trial site in turn. What do you make of the results?
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(}\FunctionTok{lm}\NormalTok{(TG.AMMI[,}\DecValTok{3}\NormalTok{]}\SpecialCharTok{\textasciitilde{}}\NormalTok{COUNTRY,}\AttributeTok{data=}\NormalTok{TG.AMMI))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Analysis of Variance Table
## 
## Response: TG.AMMI[, 3]
##            Df Sum Sq Mean Sq F value        Pr(>F)    
## COUNTRY     2   4478 2239.01  18.387 0.00000002449 ***
## Residuals 368  44813  121.77                          
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

It is worth looking at variance components too -- use \texttt{lmer} to split variance into between countries and within. The countries are a fixed effect rather than a random sample from a population -- and they are definitely not exchangeable. However, the between country variance can be used as a simple means to quantify the relative magnitude of differences between and within countries, but we mustn't get carried away in our interpretation: to infer how what we might find if we tested things in Hungary for example.

What is your interpretation of the variance components?

We shall now get a matrix of G x E terms: deviations from row and column averages. There are various ways of getting at this. This is one:

We shall use the command aggregate to group and average the data by country of origin.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{country.means}\OtherTok{\textless{}{-}}\FunctionTok{aggregate}\NormalTok{(TG.AMMI[,}\SpecialCharTok{{-}}\DecValTok{1}\SpecialCharTok{:{-}}\DecValTok{2}\NormalTok{],}\FunctionTok{list}\NormalTok{(TG.AMMI}\SpecialCharTok{$}\NormalTok{COUNTRY),mean)}
\NormalTok{country.means}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Group.1 CALLOW_2011 FRANCE_2010 FRANCE_2011 LGE_2010 LGE_2011 NIAB_2010
## 1     DEU    98.27816    124.4816    90.46138 86.33333 82.68701  68.45862
## 2     FRA    96.65825    125.2517    93.64142 86.40730 78.70014  67.00697
## 3     GBR   105.71096    125.7438    96.34849 83.18342 84.21452  70.53014
\end{verbatim}

I can talk you through this command. Do you understand the \texttt{{[},-1:-2{]}} reference? \texttt{list} is required to tell R that the term in the brackets is to be treated as a list -- one of the R internal structures. Without it, you get an error. This is an annoying quirk of R: it is very fussy about the type of structure it is working on, but often the fix is very simple, as here, yet still takes some trial and error to identify.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cm.rm}\OtherTok{\textless{}{-}} \FunctionTok{rowMeans}\NormalTok{(country.means[,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{])}
\NormalTok{cm.cm}\OtherTok{\textless{}{-}} \FunctionTok{colMeans}\NormalTok{(country.means[,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{])}
\NormalTok{cm.mean}\OtherTok{\textless{}{-}}\FunctionTok{mean}\NormalTok{(}\FunctionTok{as.matrix}\NormalTok{(country.means[,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

It is reasonably obvious what these commands are doing, together with another example of a requirement to change the data type to get a command to run.

Now that we have the marginal means, we can work out the GxE terms: in effect the error term from a simple linear model. Can you see what this is? Writing the model down may help if it isn't obvious.

The following bit of code - employing two loops, one nested inside the other, works out the deviations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cm.GE}\OtherTok{\textless{}{-}}\NormalTok{country.means}
 \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{2}\SpecialCharTok{:}\DecValTok{7}\NormalTok{) \{}
   \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{)\{ }
\NormalTok{     cm.GE[j,i]}\OtherTok{\textless{}{-}}
\NormalTok{     cm.GE[j,i]}\SpecialCharTok{+}\NormalTok{ cm.mean }\SpecialCharTok{{-}}\NormalTok{ cm.rm[j]}\SpecialCharTok{{-}}\NormalTok{ cm.cm[(i}\DecValTok{{-}1}\NormalTok{)]}
\NormalTok{     \}}
\NormalTok{ \}}
\end{Highlighting}
\end{Shaded}

Let's inspect cm.GE:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cm.GE}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Group.1 CALLOW_2011 FRANCE_2010 FRANCE_2011  LGE_2010   LGE_2011   NIAB_2010
## 1     DEU   -1.271135  -0.0109323   -2.355892  1.691808  1.4862798  0.45987222
## 2     FRA   -2.385319   1.2648473    1.329880  2.271503 -1.9948595 -0.48605159
## 3     GBR    3.656454  -1.2539150    1.026012 -3.963310  0.5085797  0.02617936
\end{verbatim}

This has been fiddly and we want to check that the GxE terms are correct. There is a simple way of doing this. Do you know what it is? Answer is coming up.

\texttt{colMeans(cm.GE{[},-1{]})}

\texttt{rowMeans(cm.GE{[},-1{]})}

We now have the GxE terms for country in the data.table \texttt{cm.GE}. This is a matrix of 3 rows (countries) x 6 columns (sites).

We have just fitted a simple linear model to a table. We had row effects, column effects, and an overall mean and from these we make an estimate of the true value of each cell in the table. These are often referred to as the fitted values. The deviations from the fitted values (which are our GxE terms in our data) sum to zero over rows and to zero over columns. If we squared these deviations and added them up, that would be the error sum of squares, or residual sum of squares in an analysis of variance.

Rather than fitting a linear model, we can fit a multiplicative model. In this, the row effects and column effects are estimated in a different way, and are often called the row and column eigenvalues, or the principle components. The estimate of the true value of each cell in the table is made by multiplying the row effect and the column effect together. The deviations from the fitted values can be squared and added up and treated as a residual sum of squares as before. However, the row and column means are no longer zero. If we wanted, though it would be daft, we could use the row and column means of these residuals to estimate the residuals themselves. Rather than do this though, we can fit another multiplicative model, with a second set of row vectors and column vectors. The error sum of squares will be smaller than before, and the deviations would still not sum to zero. We could iterate in this manner as many times as we wished, but eventually we would get a perfect fit to the data matrix. In practice, we often find that two or three iterations are all that is required to get a very good fit (a very small error sum of squares). This process of iteration and getting closer and closer to a perfect fit is called Singular Value Decomposition (SVD). It is very very closely related to what we do in Principle Component Analysis or Principle Co-ordinate Analysis.

So what? Well suppose you had a matrix of 20 sites and 1000 varieties -- 20 000 entries. It is very difficult to identify or interpret patterns in the data. But two multiplicative iterations of the type described about would reduce the data to 20 x 2 eigenvectors for sites and 1000 x 2 eigenvectors for varieties. We are now trying to interpret 2040 rather than 20,000 data points, which is still quite a task but potentially easier. The interpretation is helped, however, because we can plot the data in something called a biplot. This is just a scattergram of the eigenvectors from the first and second iterations. It is called a biplot not because it is a plot in two dimensions, but because it plots both row (variety) and column (site) effects together, and these often lead to easier interpretation. We'll see this once we produce our own.

The second complication is that the solution to the multiplicative model is not unique. We could multiply every row eigenvalue, by any constant and divide the corresponding column eigenvalue by the same constant and we would get exactly the same prediction. The standard approach is to scale the sum of squares of row eigenvalues to 1 and independently scale the SS of the column eigenvalues to 1. When these two eigenvectors are multiplied together to estimate the values in the table, they then need rescaling. This rescaling factor is called the eigenvalue. That seems complicated, but the eigenvalue has the nice property that the bigger it is, the more variation is accounted for, so it is a measure of importance in its own right.

Back to the data. To carry out the SVD:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{svd.cm.GE}\OtherTok{\textless{}{-}}\FunctionTok{svd}\NormalTok{(cm.GE[,}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

Inspect. We can discuss it.

We could produce the biplot, as it's just a scattergram, using the standard plot command, but there is a command biplot which helps interpretation:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{biplot}\NormalTok{(svd.cm.GE}\SpecialCharTok{$}\NormalTok{u,svd.cm.GE}\SpecialCharTok{$}\NormalTok{v)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-253-1.pdf}

That is a bit ugly though, so we'll add some labels:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{biplot}\NormalTok{(svd.cm.GE}\SpecialCharTok{$}\NormalTok{u,svd.cm.GE}\SpecialCharTok{$}\NormalTok{v,}\AttributeTok{xlabs =} \FunctionTok{c}\NormalTok{(}\StringTok{"DEU"}\NormalTok{,}\StringTok{"FRA"}\NormalTok{,}\StringTok{"GRB"}\NormalTok{),}\AttributeTok{ylabs=}\FunctionTok{colnames}\NormalTok{(cm.GE[}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]),}\AttributeTok{cex=}\FloatTok{0.5}\NormalTok{,}\AttributeTok{xlab=}\StringTok{"eigenvector 1"}\NormalTok{,}\AttributeTok{ylab=}\StringTok{"eigenvector 2"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-254-1.pdf}

Zoom the plot and we'll discuss it: is there any visible evidence for country specific adaptation? To my mind, this has worked very well. I wish I had done this earlier as I would have included it in the publication.

Finally, with the TG data, we could have produced a biplot on the variety x sites matrix, without first summarising over countries. Following the same commands as above run:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TG.AMMI.rowMeans}\OtherTok{\textless{}{-}}\FunctionTok{rowMeans}\NormalTok{(TG.AMMI[,}\DecValTok{3}\SpecialCharTok{:}\DecValTok{8}\NormalTok{])}
\NormalTok{TG.AMMI.colMeans}\OtherTok{\textless{}{-}}\FunctionTok{colMeans}\NormalTok{(TG.AMMI[,}\DecValTok{3}\SpecialCharTok{:}\DecValTok{8}\NormalTok{])}
\NormalTok{TG.AMMI.mean}\OtherTok{\textless{}{-}}\FunctionTok{mean}\NormalTok{(}\FunctionTok{as.matrix}\NormalTok{(TG.AMMI[,}\DecValTok{3}\SpecialCharTok{:}\DecValTok{8}\NormalTok{]))        }

\NormalTok{TG.AMMI.GE}\OtherTok{\textless{}{-}}\NormalTok{TG.AMMI}
 \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{3}\SpecialCharTok{:}\DecValTok{8}\NormalTok{) \{}
   \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{371}\NormalTok{)\{ }
\NormalTok{     TG.AMMI.GE[j,i]}\OtherTok{\textless{}{-}}
\NormalTok{     TG.AMMI.GE[j,i]}\SpecialCharTok{+}\NormalTok{ TG.AMMI.mean }\SpecialCharTok{{-}}\NormalTok{ TG.AMMI.rowMeans[j]}\SpecialCharTok{{-}}\NormalTok{ TG.AMMI.colMeans[(i}\DecValTok{{-}2}\NormalTok{)]}
\NormalTok{     \}}
\NormalTok{ \}}

\FunctionTok{colMeans}\NormalTok{(TG.AMMI.GE[,}\DecValTok{3}\SpecialCharTok{:}\DecValTok{8}\NormalTok{])}
\FunctionTok{rowMeans}\NormalTok{(TG.AMMI.GE[,}\DecValTok{3}\SpecialCharTok{:}\DecValTok{8}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

Then we can run svd and plot the results:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{svd.TG.AMMI.GE}\OtherTok{\textless{}{-}}\FunctionTok{svd}\NormalTok{(TG.AMMI.GE[,}\DecValTok{3}\SpecialCharTok{:}\DecValTok{8}\NormalTok{])}
\FunctionTok{biplot}\NormalTok{(svd.TG.AMMI.GE}\SpecialCharTok{$}\NormalTok{u,svd.TG.AMMI.GE}\SpecialCharTok{$}\NormalTok{v,}\AttributeTok{cex=}\FloatTok{0.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-256-1.pdf}

Can you make sense of that? It is a little easier if we produce plots for each country of origin in turn. For example for the UK:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{biplot}\NormalTok{(svd.TG.AMMI.GE}\SpecialCharTok{$}\NormalTok{u[TG.AMMI}\SpecialCharTok{$}\NormalTok{COUNTRY}\SpecialCharTok{==}\StringTok{"GBR"}\NormalTok{,],svd.TG.AMMI.GE}\SpecialCharTok{$}\NormalTok{v,}\AttributeTok{cex=}\FloatTok{0.5}\NormalTok{,}\AttributeTok{ylabs=}\FunctionTok{colnames}\NormalTok{(TG.AMMI[,}\SpecialCharTok{{-}}\DecValTok{1}\SpecialCharTok{:{-}}\DecValTok{2}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-257-1.pdf}

Any better?

\hypertarget{Population-genetics}{%
\chapter{Population genetics.}\label{Population-genetics}}

\hypertarget{hardy-weinberg-equilibrium-and-population-structure}{%
\section{Hardy-Weinberg equilibrium and population structure}\label{hardy-weinberg-equilibrium-and-population-structure}}

\emph{Hardy-Weinberg equilibrium frequencies and population subdivision (Fst)}

In the absence of the action of evolutionary forces (selection, mutation, migration, non-random mating, genetic drift), genotypic frequencies in a population follow directly from the individual allele frequencies in the population. This is called ``Hardy-Weinberg equilibrium''. However, in most populations and in particular in breeding populations, the conditions under which HW hold are not very realistic. Nevertheless, when averaged over multiple markers, the HW test provides a simple means of assessing inbreeding, selfing, or population mixing in genetic samples. In particular, it is useful for detecting population sub-structure within a population.

To illustrate this, we will use a subset of a Teosinte (\emph{Zea mays} ssp. \emph{parviglumis}) data set published by \citet{weber_major_2007}. Teosinte is the wild, outcrossing ancestral species from which maize was domesticated. Details of the entire data set can be found in the paper. Here we will use data of 592 SNP markers from 544 genotypes. The genotypes can be classified in four groups based on the geographical origin (States in Mexico).

\begin{figure}
\centering
\includegraphics{images/pop_gen1.png}
\caption{Teosinte distribution over the states of Mexico}
\end{figure}

In the tables below are the SNP genotype frequencies of two markers are given (for all individuals in the data set).

\begin{table}

\caption{\label{tab:unnamed-chunk-258}SNP genotype frequencies for two markers.}
\centering
\begin{tabular}[t]{lrlr}
\toprule
PZA00219\_6 & N.individuals & PZA03001\_18 & N\_individuals\\
\midrule
A/A & 163 & A/A & 190\\
A/G & 158 & A/C & 207\\
G/G & 213 & C/C & 145\\
\bottomrule
\end{tabular}
\end{table}

Use the information in the table to answer the following questions:

\begin{rmdquiz}
\textbf{1}

a: Based on the information in the table, give the allele frequencies, the observed (Ho) and expected (He) heterozygocity, and the fixation index (F) for each marker.

b: What do the results in (a) suggest in terms of population structure? Explain.

c: Test whether the population is in HW equilibrium based on these two markers. (to do this you will need to do a Chi-square test by hand).
\end{rmdquiz}

The table below gives the same information for SNP PZA00219\_6 but separated by group (defined by the Mexican state in which they were collected).

\begin{table}

\caption{\label{tab:unnamed-chunk-260}SNP genotype frequencies by Mexican state for a single marker}
\centering
\begin{tabular}[t]{lrrrr}
\toprule
PZA00219\_6 & Guerrero & Jalisco & Mexico & Michoacan\\
\midrule
A/A & 30 & 79 & 18 & 36\\
A/G & 61 & 5 & 29 & 63\\
G/G & 134 & 9 & 31 & 39\\
\bottomrule
\end{tabular}
\end{table}

Use this table to address the following questions:

\begin{rmdquiz}
\textbf{2}

a: Give the Ho, He, and F for each population for marker PZA00219\_6.

b: Test whether the groups are in HW equilibrium for this marker.

c: Compare the results with those in the quiz block above. How do you explain the differences?
\end{rmdquiz}

Your conclusions so far above are based on only one/two loci. We will now look across all loci using a set of three related R packages: \texttt{adegenet} \citep{adegenet2008} which is a general purpose genetic data analysis package, and the population genetic packages \texttt{pegas} \citep{R-pegas} and \texttt{hierfstat} \citep{R-hierfstat} which use the \texttt{adegenet} data format. For a non-R alternative you could also use \href{http://cmpg.unibe.ch/software/arlequin3/}{Arlequin}. However, use of Arlequin takes some practice, to get the hang of the input formats.

You will need to install the packages \texttt{adegenet}, \texttt{pegas} and \texttt{hierfstat} to you own R environment.

\begin{rmdnote}
Throughout this tutorial we start to use longer and more complicated scripts. You can copy and paste these scripts directly from the tutorial book (as we've been doing prevoisly). Alternativly, if you are becoming tired of copying and pasting, we've added the full scripts the Chapter 7 data folder on the course website. See: ``teosinte\_HardyWeinberg\_script.R'', ``teosinte\_pca\_script.R'', ``AG\_script.R'' and ``LD\_Script.R'', if requried.
\end{rmdnote}

Set your home directory to a folder containing today's data files. For the ``genind'' data format used in \texttt{adegenet}, it is easier to read in the genotype data and population classification of individuals from separate input files. Take a look at the datafiles -- ``teosinte\_genos\_R.txt'' and ``teosinte\_pops\_R.txt'' -- to see the format required. Then read in the data files ``teosinte\_genos\_R.txt'' and ``teosinte\_pops\_R.txt'', these data files are found on the course website.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load required libraries. adegenet and its associated packages are good for population genetic analysis in R}
\FunctionTok{library}\NormalTok{(adegenet)}
\FunctionTok{library}\NormalTok{(pegas)}
\FunctionTok{library}\NormalTok{(hierfstat)}
\CommentTok{\# read in the data for genotypes and population codes separately.}
\CommentTok{\#you will need to delete "data/" if you copy this script}
\NormalTok{genos}\OtherTok{\textless{}{-}}\FunctionTok{read.table}\NormalTok{(}\StringTok{"data/teosinte\_genos\_R.txt"}\NormalTok{, }\AttributeTok{header=}\NormalTok{T)}
\NormalTok{pops}\OtherTok{\textless{}{-}}\FunctionTok{read.table}\NormalTok{(}\StringTok{"data/teosinte\_pops\_R.txt"}\NormalTok{, }\AttributeTok{header=}\NormalTok{T)}
\end{Highlighting}
\end{Shaded}

When loaded, convert to the ``genind'' format, add in the population information, and check that you have inputted the data correctly:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# convert to genind data format}
\NormalTok{teosinte}\OtherTok{\textless{}{-}} \FunctionTok{df2genind}\NormalTok{(genos,}\AttributeTok{ploidy=}\DecValTok{2}\NormalTok{,}\AttributeTok{sep=}\StringTok{"/"}\NormalTok{)}
\CommentTok{\# add in the population information}
\NormalTok{teosinte}\SpecialCharTok{@}\NormalTok{pop}\OtherTok{\textless{}{-}}\FunctionTok{as.factor}\NormalTok{(pops}\SpecialCharTok{$}\NormalTok{pop)}
\CommentTok{\#check that the data is as expected}
\NormalTok{teosinte}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## /// GENIND OBJECT /////////
## 
##  // 544 individuals; 592 loci; 1,184 alleles; size: 2.8 Mb
## 
##  // Basic content
##    @tab:  544 x 1184 matrix of allele counts
##    @loc.n.all: number of alleles per locus (range: 2-2)
##    @loc.fac: locus factor for the 1184 columns of @tab
##    @all.names: list of allele names for each locus
##    @ploidy: ploidy of each individual  (range: 2-2)
##    @type:  codom
##    @call: df2genind(X = genos, sep = "/", ploidy = 2)
## 
##  // Optional content
##    @pop: population of each individual (group size range: 80-227)
\end{verbatim}

First we make a summary of the data, which includes the observed and expected heterozygosities for every marker, assuming that all individuals belong to a single population.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#make a population genetic summary of the data set considering it as a single population}
\NormalTok{Teo\_Summary}\OtherTok{\textless{}{-}}\FunctionTok{summary}\NormalTok{(teosinte)}
\end{Highlighting}
\end{Shaded}

Inspect \texttt{Teo\_Summary} on your own computer.

We can calculate the ``fixation index'' as you did earlier in the exercise, and also plot Ho-He for a subset of markers to see if there are any patterns. What do you observe?

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# plot Ho{-}He for the first 40 markers}
\FunctionTok{barplot}\NormalTok{(Teo\_Summary}\SpecialCharTok{$}\NormalTok{Hexp[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{40}\NormalTok{]}\SpecialCharTok{{-}}\NormalTok{Teo\_Summary}\SpecialCharTok{$}\NormalTok{Hobs[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{40}\NormalTok{],}\AttributeTok{xaxt=}\StringTok{\textquotesingle{}n\textquotesingle{}}\NormalTok{,}\AttributeTok{main=}\StringTok{"He{-}Ho across all individuals for first 40 loci"}\NormalTok{, }\AttributeTok{ylab=} \StringTok{"He{-}Ho"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Markers"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-266-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# calculate F values}
\NormalTok{Fvalues}\OtherTok{\textless{}{-}}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{(Teo\_Summary}\SpecialCharTok{$}\NormalTok{Hobs}\SpecialCharTok{/}\NormalTok{Teo\_Summary}\SpecialCharTok{$}\NormalTok{Hexp)}
\end{Highlighting}
\end{Shaded}

Now we can test agreement with HW expectations at each locus.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#test for Hardy{-}Weinberg equilibrium at each marker separately}
\NormalTok{hwt.all}\OtherTok{\textless{}{-}}\FunctionTok{hw.test}\NormalTok{(teosinte, }\AttributeTok{B=}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Inspect the first 40 markers on your own computer by running: \texttt{hwt.all{[}1:40,1:3{]}}

We can count how many of the 592 HW tests we have done are significant at the 5\% level. (In R, the \texttt{sum} function will count the number of times a condition is \texttt{TRUE} when used as in the script here).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# count the number of markers that significantly deviate from HW equilibrium with P\textless{}0.05}
\FunctionTok{sum}\NormalTok{(hwt.all[,}\DecValTok{3}\NormalTok{]}\SpecialCharTok{\textless{}}\FloatTok{0.05}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 559
\end{verbatim}

\begin{rmdquiz}
\textbf{3} Are more results significant than we might expect by chance?
\end{rmdquiz}

Next, calculate the average Ho, He and F across all markers:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate average (se) of Ho, He, F across all markers}

\NormalTok{se }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{var}\NormalTok{(x)}\SpecialCharTok{/}\FunctionTok{length}\NormalTok{(x))}
\NormalTok{Ho\_ave}\OtherTok{\textless{}{-}}\FunctionTok{round}\NormalTok{(}\FunctionTok{mean}\NormalTok{(Teo\_Summary}\SpecialCharTok{$}\NormalTok{Hobs),}\DecValTok{4}\NormalTok{)}
\NormalTok{Ho\_se}\OtherTok{\textless{}{-}}\FunctionTok{round}\NormalTok{(}\FunctionTok{se}\NormalTok{(Teo\_Summary}\SpecialCharTok{$}\NormalTok{Hobs),}\DecValTok{4}\NormalTok{)}
\NormalTok{He\_ave}\OtherTok{\textless{}{-}}\FunctionTok{round}\NormalTok{(}\FunctionTok{mean}\NormalTok{(Teo\_Summary}\SpecialCharTok{$}\NormalTok{Hexp),}\DecValTok{4}\NormalTok{)}
\NormalTok{He\_se}\OtherTok{\textless{}{-}}\FunctionTok{round}\NormalTok{(}\FunctionTok{se}\NormalTok{(Teo\_Summary}\SpecialCharTok{$}\NormalTok{Hexp),}\DecValTok{4}\NormalTok{)}
\NormalTok{F\_ave}\OtherTok{\textless{}{-}}\FunctionTok{round}\NormalTok{(}\FunctionTok{mean}\NormalTok{(Fvalues),}\DecValTok{4}\NormalTok{)}
\NormalTok{F\_se}\OtherTok{\textless{}{-}}\FunctionTok{round}\NormalTok{(}\FunctionTok{se}\NormalTok{(Fvalues),}\DecValTok{4}\NormalTok{)}

\NormalTok{Average\_table}\OtherTok{\textless{}{-}}\FunctionTok{data.frame}\NormalTok{(}\ConstantTok{NA}\NormalTok{,}\AttributeTok{nrow=}\DecValTok{5}\NormalTok{,}\AttributeTok{ncol=}\DecValTok{3}\NormalTok{)}
\NormalTok{Average\_table[}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{]}\OtherTok{\textless{}{-}}\FunctionTok{paste}\NormalTok{(Ho\_ave, }\StringTok{"("}\NormalTok{, Ho\_se, }\StringTok{")"}\NormalTok{) }
\NormalTok{Average\_table[}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{]}\OtherTok{\textless{}{-}}\FunctionTok{paste}\NormalTok{(He\_ave, }\StringTok{"("}\NormalTok{, He\_se, }\StringTok{")"}\NormalTok{)}
\NormalTok{Average\_table[}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{]}\OtherTok{\textless{}{-}}\FunctionTok{paste}\NormalTok{(F\_ave, }\StringTok{"("}\NormalTok{, F\_se, }\StringTok{")"}\NormalTok{)}
\FunctionTok{colnames}\NormalTok{(Average\_table)}\OtherTok{\textless{}{-}}\FunctionTok{c}\NormalTok{(}\StringTok{"Ho"}\NormalTok{, }\StringTok{"He"}\NormalTok{, }\StringTok{"F"}\NormalTok{)}
\FunctionTok{rownames}\NormalTok{(Average\_table)[}\DecValTok{1}\NormalTok{]}\OtherTok{\textless{}{-}}\FunctionTok{c}\NormalTok{(}\StringTok{"All"}\NormalTok{)}
\NormalTok{Average\_table}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                   Ho                He                 F
## All 0.2871 ( 0.004 ) 0.3747 ( 0.0043 ) 0.2327 ( 0.0059 )
\end{verbatim}

Then we can test the hypothesis that Ho-He is significant across all markers using a paired t-test, firstly checking if the Ho and He variances are equal. Is the result as expected?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{bartlett.test}\NormalTok{(}\FunctionTok{list}\NormalTok{(Teo\_Summary}\SpecialCharTok{$}\NormalTok{Hexp, Teo\_Summary}\SpecialCharTok{$}\NormalTok{Hobs))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Bartlett test of homogeneity of variances
## 
## data:  list(Teo_Summary$Hexp, Teo_Summary$Hobs)
## Bartlett's K-squared = 3.1346, df = 1, p-value = 0.07665
\end{verbatim}

Test the hypothesis that Hobs\textgreater Hexp across all markers:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(Teo\_Summary}\SpecialCharTok{$}\NormalTok{Hexp, Teo\_Summary}\SpecialCharTok{$}\NormalTok{Hobs, }\AttributeTok{pair=}\NormalTok{T, }\AttributeTok{var.equal=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{alter=}\StringTok{"greater"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Paired t-test
## 
## data:  Teo_Summary$Hexp and Teo_Summary$Hobs
## t = 34.023, df = 591, p-value < 2.2e-16
## alternative hypothesis: true difference in means is greater than 0
## 95 percent confidence interval:
##  0.08336362        Inf
## sample estimates:
## mean of the differences 
##              0.08760561
\end{verbatim}

We will now rerun these analyses for each state separately. The script shows how to do this for Mexico state (MEX).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#separate into "populations" by state:}

\NormalTok{teosinte\_bypop}\OtherTok{\textless{}{-}}\FunctionTok{seppop}\NormalTok{(teosinte)}
\NormalTok{teo.GUE}\OtherTok{\textless{}{-}}\FunctionTok{summary}\NormalTok{(teosinte\_bypop}\SpecialCharTok{$}\NormalTok{GUE)}
\NormalTok{teo.JAL}\OtherTok{\textless{}{-}}\FunctionTok{summary}\NormalTok{(teosinte\_bypop}\SpecialCharTok{$}\NormalTok{JAL)}
\NormalTok{teo.MEX}\OtherTok{\textless{}{-}}\FunctionTok{summary}\NormalTok{(teosinte\_bypop}\SpecialCharTok{$}\NormalTok{MEX)}
\NormalTok{teo.MIC}\OtherTok{\textless{}{-}}\FunctionTok{summary}\NormalTok{(teosinte\_bypop}\SpecialCharTok{$}\NormalTok{MIC)}

\CommentTok{\# repeat above analysis for MEX only}
\FunctionTok{barplot}\NormalTok{(teo.MEX}\SpecialCharTok{$}\NormalTok{Hexp[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{40}\NormalTok{]}\SpecialCharTok{{-}}\NormalTok{teo.MEX}\SpecialCharTok{$}\NormalTok{Hobs[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{40}\NormalTok{], }\AttributeTok{xaxt=}\StringTok{\textquotesingle{}n\textquotesingle{}}\NormalTok{, }\AttributeTok{main=}\StringTok{"He{-}Ho in MEX for first 40 loci "}\NormalTok{, }\AttributeTok{ylab=} \StringTok{"He{-}Ho"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Markers"}\NormalTok{)}

\NormalTok{Fvalues}\OtherTok{\textless{}{-}}\DecValTok{1}\SpecialCharTok{{-}}\NormalTok{(teo.MEX}\SpecialCharTok{$}\NormalTok{Hobs}\SpecialCharTok{/}\NormalTok{teo.MEX}\SpecialCharTok{$}\NormalTok{Hexp)}

\NormalTok{hwt.MEX}\OtherTok{\textless{}{-}}\FunctionTok{hw.test}\NormalTok{(teosinte\_bypop}\SpecialCharTok{$}\NormalTok{MEX, }\AttributeTok{B=}\DecValTok{0}\NormalTok{)}
\NormalTok{hwt.MEX[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{40}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{3}\NormalTok{]}
\FunctionTok{sum}\NormalTok{(hwt.MEX[,}\DecValTok{3}\NormalTok{]}\SpecialCharTok{\textless{}}\FloatTok{0.05}\NormalTok{)}

\NormalTok{Ho\_ave.MEX}\OtherTok{\textless{}{-}}\FunctionTok{round}\NormalTok{(}\FunctionTok{mean}\NormalTok{(teo.MEX}\SpecialCharTok{$}\NormalTok{Hobs),}\DecValTok{4}\NormalTok{)}
\NormalTok{Ho\_se.MEX}\OtherTok{\textless{}{-}}\FunctionTok{round}\NormalTok{(}\FunctionTok{se}\NormalTok{(teo.MEX}\SpecialCharTok{$}\NormalTok{Hobs),}\DecValTok{4}\NormalTok{)}
\NormalTok{He\_ave.MEX}\OtherTok{\textless{}{-}}\FunctionTok{round}\NormalTok{(}\FunctionTok{mean}\NormalTok{(teo.MEX}\SpecialCharTok{$}\NormalTok{Hexp),}\DecValTok{4}\NormalTok{)}
\NormalTok{He\_se.MEX}\OtherTok{\textless{}{-}}\FunctionTok{round}\NormalTok{(}\FunctionTok{se}\NormalTok{(teo.MEX}\SpecialCharTok{$}\NormalTok{Hexp),}\DecValTok{4}\NormalTok{)}
\NormalTok{F\_ave.MEX}\OtherTok{\textless{}{-}}\FunctionTok{round}\NormalTok{(}\FunctionTok{mean}\NormalTok{(Fvalues),}\DecValTok{4}\NormalTok{)}
\NormalTok{F\_se.MEX}\OtherTok{\textless{}{-}}\FunctionTok{round}\NormalTok{(}\FunctionTok{se}\NormalTok{(Fvalues),}\DecValTok{4}\NormalTok{)}

\NormalTok{Average\_table[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{]}\OtherTok{\textless{}{-}}\FunctionTok{paste}\NormalTok{(Ho\_ave.MEX, }\StringTok{"("}\NormalTok{, Ho\_se.MEX, }\StringTok{")"}\NormalTok{) }
\NormalTok{Average\_table[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{]}\OtherTok{\textless{}{-}}\FunctionTok{paste}\NormalTok{(He\_ave.MEX, }\StringTok{"("}\NormalTok{, He\_se.MEX, }\StringTok{")"}\NormalTok{)}
\NormalTok{Average\_table[}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{]}\OtherTok{\textless{}{-}}\FunctionTok{paste}\NormalTok{(F\_ave.MEX, }\StringTok{"("}\NormalTok{, F\_se.MEX, }\StringTok{")"}\NormalTok{)}
\FunctionTok{rownames}\NormalTok{(Average\_table)[}\DecValTok{2}\NormalTok{]}\OtherTok{\textless{}{-}}\FunctionTok{c}\NormalTok{(}\StringTok{"MEX"}\NormalTok{)}
\NormalTok{Average\_table}

\FunctionTok{bartlett.test}\NormalTok{(}\FunctionTok{list}\NormalTok{(teo.MEX}\SpecialCharTok{$}\NormalTok{Hexp, teo.MEX}\SpecialCharTok{$}\NormalTok{Hobs))}
\FunctionTok{t.test}\NormalTok{(teo.MEX}\SpecialCharTok{$}\NormalTok{Hexp, teo.MEX}\SpecialCharTok{$}\NormalTok{Hobs, }\AttributeTok{pair=}\NormalTok{T, }\AttributeTok{var.equal=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{alter=}\StringTok{"greater"}\NormalTok{)}

\CommentTok{\# finally calculate F statistics across all markers (this may take a while)}
\FunctionTok{fstat}\NormalTok{(teosinte)}
\end{Highlighting}
\end{Shaded}

For other populations, I have compiled the data in the exercise below.

So for doing the same for all states, we get the following values for Ho, He and F.

\begin{table}

\caption{\label{tab:unnamed-chunk-274}Ho, He and F for Mexican population and states.}
\centering
\begin{tabular}[t]{llll}
\toprule
  & Ho & He & F\\
\midrule
Entire population & 0.287 (0.004) & 0.375 (0.004) & 0.233 (0.006)\\
Guerrero & 0.298 (0.004) & 0.375 (0.004) & 0.203 (0.007)\\
Jalisco & 0.221 (0.005) & 0.326 (0.006) & 0.295 (0.009)\\
Mexico & 0.312 (0.005) & 0.362 (0.005) & 0.137 (0.008)\\
Michoacan & 0.300 (0.005) & 0.357 (0.005) & 0.157 (0.007)\\
\bottomrule
\end{tabular}
\end{table}

\begin{rmdquiz}
\textbf{4} Inspect the above table and give your conclusions: how do individual states compare to the entire population?\\
\end{rmdquiz}

Finally, we can calculate F statistics, considering each state as a population. This takes a few minutes so we provide the values here. Averaged over all loci , these are shown below this paragraph. \(F_{IS}\) is a measure of within population inbreeding, \(F_{IT}\) is a measure of total inbreeding and \(F_{ST}\) is a measure of population divergence. Note that \(1-F_{IT} = (1-F_{IS})(1-F_{ST})\), as it should be.

\(F_{IS} = 0.205\)

\(F_{IT} = 0.246\)

\(F_{ST}= 0.052\)

\begin{rmdquiz}
\textbf{5} What can you conclude from these statistics?
\end{rmdquiz}

By now you may have some opinion on the importance of the population structure in this data set. You may also have noticed some differences among the sub-populations. We will further investigate this using principal component analysis (PCA) of the entire molecular marker matrix using R. PCA is a statistical technique which summarizes the variation observed across all markers into a smaller number of underlying component variables. Each principal component accounts for a decreasing amount of variation in the dataset, e.g.~PC1 accounts for the highest amount of variability in the original data as possible, and each subsequent PC explains the highest amount of variance possible in the remainder of the data. As a result, PCA finds the number of effective dimensions in the data defined by the significant PCA axes. If two major groups are present one dimension (PC1) might be enough to describe it. If three, then an extra axis might be necessary, and so on and so forth.

The data for this analysis are in a file called ``teosinte\_imputed\_R.txt''. I have \emph{imputed} missing data in this data set -- you will learn more about this later in the course (for the previous analysis, missing data were left as ``NA''). We will use some simple R code to conduct a PCA on the data.

Let us load the data and required package \texttt{ggfortify}, you will need to install this if you don't have it already.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggfortify)}
\CommentTok{\#load data (delete \textquotesingle{}data/\textquotesingle{} if you copy this!)}
\NormalTok{teosintepca}\OtherTok{\textless{}{-}}\FunctionTok{read.table}\NormalTok{(}\StringTok{"data/teosinte\_imputed\_R.txt"}\NormalTok{, }\AttributeTok{header=}\NormalTok{T)}
\end{Highlighting}
\end{Shaded}

First we fit the PCA and observe (and plot) the percentage of variance explained by each of the first 10 PCs.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#define pca input (markers only, not state name)}
\NormalTok{pca.input}\OtherTok{\textless{}{-}}\NormalTok{teosintepca[}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{592}\NormalTok{)]}
\CommentTok{\#run PCA}
\NormalTok{pca.output}\OtherTok{\textless{}{-}}\FunctionTok{prcomp}\NormalTok{(pca.input)}
\end{Highlighting}
\end{Shaded}

List the first 10 PCAs on your computer by running:

\texttt{summary(pca.output)\$importance{[},1:10{]}}

We can also plot the proportion of variance explained by first 10 PCAs by running:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(}\FunctionTok{summary}\NormalTok{(pca.output)}\SpecialCharTok{$}\NormalTok{importance[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{], }\AttributeTok{main=}\StringTok{"PCA analysis {-} Teosinte"}\NormalTok{, }\AttributeTok{ylab=}\StringTok{"percentage variance explained"}\NormalTok{, }\AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\FloatTok{0.06}\NormalTok{),}\AttributeTok{xlab=}\StringTok{"PCA"}\NormalTok{, }\AttributeTok{lty=}\FunctionTok{par}\NormalTok{(}\StringTok{"lty"}\NormalTok{), }\AttributeTok{lwd=}\FunctionTok{par}\NormalTok{(}\StringTok{"lwd"}\NormalTok{))}
\FunctionTok{lines}\NormalTok{(}\FunctionTok{summary}\NormalTok{(pca.output)}\SpecialCharTok{$}\NormalTok{importance[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-279-1.pdf}

Inspect these outputs to comment on the following question:

\begin{rmdquiz}
\textbf{6} How much of the variation in this data set is explained by the first 10 PCs? What does this suggest in terms of the underlying population structure?
\end{rmdquiz}

Now, we will plot the first two PCs against each other, colouring the points by geography (state).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#plot first 2 PCAs}
\FunctionTok{autoplot}\NormalTok{(pca.output, }\AttributeTok{data=}\NormalTok{teosintepca, }\AttributeTok{colour=}\StringTok{\textquotesingle{}State\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-281-1.pdf}

Take a look at this plot and address the questions below:

\begin{rmdquiz}
\textbf{7}

a: Describe what you see in the plot. Is there (some) agreement with the classification purely on the geographical origin?

b: Based on the scatter plot you have just made, what seems to be odd about the group ``Jalisco''? How does your observation here correspond with your findings in the fixation index for this sate?
\end{rmdquiz}

We could have also plotted the alternative PCs (1 and 3 here). Try running this on your laptop.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pca\_extra}\OtherTok{=}\NormalTok{pca.output}
\NormalTok{pca\_extra}\SpecialCharTok{$}\NormalTok{x}\OtherTok{=}\NormalTok{pca\_extra}\SpecialCharTok{$}\NormalTok{x[,}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{)]}
\FunctionTok{colnames}\NormalTok{(pca\_extra}\SpecialCharTok{$}\NormalTok{x)}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\StringTok{"PC1"}\NormalTok{,}\StringTok{"PC2"}\NormalTok{)}
\NormalTok{pca\_extra}\SpecialCharTok{$}\NormalTok{rotation}\OtherTok{=}\NormalTok{pca\_extra}\SpecialCharTok{$}\NormalTok{rotation[,}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{)]}
\FunctionTok{colnames}\NormalTok{(pca\_extra}\SpecialCharTok{$}\NormalTok{rotation)}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\StringTok{"PC1"}\NormalTok{,}\StringTok{"PC2"}\NormalTok{)}
\FunctionTok{autoplot}\NormalTok{(pca\_extra, }\AttributeTok{data=}\NormalTok{teosintepca,}\AttributeTok{colour=}\StringTok{\textquotesingle{}State\textquotesingle{}}\NormalTok{)}\SpecialCharTok{+}\FunctionTok{labs}\NormalTok{(}\AttributeTok{y=}\StringTok{"PC3"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Given what we have observed with the PCA investigation, it seems that state boundaries may not be the best way of grouping populations. Looking at the output from the PCA, it seems 3 states (Jalisco, Mexico and Michoacan) may have at least 2 groups of distinct individuals in them. A closer look at the data reveals that in all 3 states, these individuals represent distinct natural populations. If we sub-divide the samples of each of these 3 states into 2 geographic groups based on the PCA data, we now have 7 geographic groupings. We can see what difference this makes to our population statistics by recalculating the Ho, He and F values, and the F statistics using the methods from part c above. Feel free to do this (replace the original pop file with the file ``teosinte\_pops\_subdivided\_R.txt'') and explore the data. You will obtain the following summary.

\begin{table}

\caption{\label{tab:unnamed-chunk-284}Ho, He and F for Mexican population and sub-divided states.}
\centering
\begin{tabular}[t]{llll}
\toprule
  & Ho & He & F\\
\midrule
Entire population & 0.287 (0.004) & 0.375 (0.004) & 0.233 (0.006)\\
Guerrero & 0.298 (0.004) & 0.375 (0.004) & 0.203 (0.007)\\
Jalisco 1 & 0.199 (0.007) & 0.245 (0.007) & 0.161 (0.011)\\
Jalisco 2 & 0.242 (0.006) & 0.329 (0.006) & 0.244 (0.010)\\
Mexico 1 & 0.318 (0.005) & 0.358 (0.005) & 0.110 (0.008)\\
\addlinespace
Mexico 2 & 0.249 (0.009) & 0.245 (0.008) & -0.025 (0.015)\\
Michoacan 1 & 0.301 (0.005) & 0.355 (0.005) & 0.151 (0.007)\\
Michoacan 2 & 0.293 (0.009) & 0.306 (0.007) & 0.020 (0.018)\\
\bottomrule
\end{tabular}
\end{table}

\(F_{IS}\), \(F_{IT}\), \(F_{ST}\) averaged over all loci:

\(F_{IS}\) = 0.185

\(F_{IT}\) = 0.251

\(F_{ST}\) = 0.081

\begin{rmdquiz}
\textbf{8} What conclusions can you draw from this exercise?
\end{rmdquiz}

The reassignment of populations based on the PCA here seems a bit ad hoc. Data exploration of this kind is very important in population genetics but there are more powerful methods for exploring how many ``true'' populations there are in a geographic data set, as well as how to assign individuals of unknown provenance to these populations. We will explore these methods more this afternoon.

\hypertarget{other-ways-of-analysing-population-structure}{%
\section{Other ways of analysing population structure}\label{other-ways-of-analysing-population-structure}}

\hypertarget{distance-trees}{%
\subsection{Distance Trees}\label{distance-trees}}

We have examined population structure in natural populations of Teosinte, an outbreeding species. For contrast, and to see what plant breeders are more likely to encounter, we now investigate population structure in a breeding population of a self-pollinated species (barley). The data to be used is from a UK association mapping study of barley (\href{http://agoueb.org}{AGOUEB}, and includes 1500 mapped SNPs for 621 genotypes (varieties). We'll run our analysis on the barley AGOUEB data set, using SNPs from across the whole genome but thinned so that markers are \textgreater2cM away from each other. This prevents results being dominated by patterns of diversity from particular chromosome regions (especially the centromere -- lots of markers but very little recombination. Thinning in this manner has become standard practice. It is easily forgotten but is important. In this case, 307 markers remain to estimate relationships between the genotypes. Note that markers were scored as 0, 1 or ``NA'', because we are dealing with inbred lines, so there are no heterozygotes. However, for one of the approaches below (bootstrapping) no missing data is allowed. Therefore, I have imputed the missing data. The imputed values should be clear as they have values intermediate between 0 and 1. Data are in ``AG\_for\_R.txt,'' already formatted.

We are going to scale the data so that each marker has mean 0 and standard deviation 1. That way they all have equal weight in the analysis.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#load barley data, take out /data if you copy this}
\NormalTok{AG\_data}\OtherTok{\textless{}{-}}\FunctionTok{read.table}\NormalTok{(}\StringTok{"data/AG\_for\_R.txt"}\NormalTok{, }\AttributeTok{header=}\NormalTok{T)}

\CommentTok{\#scale data so all markers have mean=0, sd=1}
\NormalTok{AG\_scaled}\OtherTok{\textless{}{-}}\FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{scale}\NormalTok{(AG\_data))}
\end{Highlighting}
\end{Shaded}

Check that this has worked by running the following on your computer:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#check scaling worked}
\FunctionTok{mean}\NormalTok{(AG\_scaled}\SpecialCharTok{$}\NormalTok{GM1,}\AttributeTok{na.rm=}\NormalTok{T)}
\FunctionTok{sd}\NormalTok{(AG\_scaled}\SpecialCharTok{$}\NormalTok{GM1,}\AttributeTok{na.rm=}\NormalTok{T)}
\end{Highlighting}
\end{Shaded}

Then calculate a distance matrix (distance between all lines). Here we are using a simple ``Euclidean'' measure, based on geometric distance; other options are available.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#calculate a distance matrix}
\NormalTok{dist\_mat}\OtherTok{\textless{}{-}}\FunctionTok{dist}\NormalTok{(AG\_scaled, }\AttributeTok{method=}\StringTok{\textquotesingle{}euclidean\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can then use this distance matrix in two different clustering approaches. Firstly, \emph{Neighbor joining}. A basic method of agglomerative clustering method where individuals are grouped into progressively higher order clusters. Bootstrapping is a method of obtaining confidence in each node:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#use neighbor joining with bootstrapping}
\FunctionTok{par}\NormalTok{(}\AttributeTok{cex=}\FloatTok{0.7}\NormalTok{) }\CommentTok{\#changes font size so you can see more labels}
\NormalTok{tree}\OtherTok{\textless{}{-}}\FunctionTok{nj}\NormalTok{(dist\_mat)}
\NormalTok{boot.tree}\OtherTok{\textless{}{-}}\FunctionTok{boot.phylo}\NormalTok{(tree,AG\_scaled,}\AttributeTok{FUN =}\ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{nj}\NormalTok{(}\FunctionTok{dist}\NormalTok{(x)),}\AttributeTok{B=}\DecValTok{100}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Running bootstraps:       100 / 100
## Calculating bootstrap values... done.
\end{verbatim}

Now we can plot the tree:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(tree)}
\FunctionTok{nodelabels}\NormalTok{(boot.tree,}\AttributeTok{frame=}\StringTok{"none"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-290-1.pdf}

\begin{rmdquiz}
\textbf{9} Use the zoom function to look more closely at the plot. How many higher-level groups can reliably be distinguished in this network?
\end{rmdquiz}

Our second option is Principal Coordinate Analysis. For this dataset, PCoA is probably more informative than a tree. PCoA is related to, but different from Principal Component Analysis; PCoA analysis is based on distances between pairs of individuals.

Install and load the package called \texttt{labdsv} for principal co-ordinate analysis (there are many alternatives available).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(labdsv)}
\end{Highlighting}
\end{Shaded}

Carry out principal co-ordinate analysis:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AG\_pco}\OtherTok{\textless{}{-}}\FunctionTok{pco}\NormalTok{(dist\_mat,}\AttributeTok{k=}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Plot the eigenvalues:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(AG\_pco}\SpecialCharTok{$}\NormalTok{eig)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-294-1.pdf}

Tabulate percentage variation explained by top 10 eigenvalues:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pcntvar}\OtherTok{\textless{}{-}}\NormalTok{AG\_pco}\SpecialCharTok{$}\NormalTok{eig}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(AG\_pco}\SpecialCharTok{$}\NormalTok{eig)}
\NormalTok{pcntvar[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] 0.18035911 0.05144023 0.02956470 0.02314040 0.02253805 0.01925869
##  [7] 0.01824109 0.01648530 0.01535072 0.01433016
\end{verbatim}

Plot the first two pco axes:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(AG\_pco)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-296-1.pdf}

You can also plot different pairs of pco axes using something like \texttt{plot(AG\_pco\$points{[},1{]},AG\_pco\$points{[},3{]})}.

Use your inspection of the PCoA results to answer the following questions:

\begin{rmdquiz}
\textbf{10}

a: Examine the Eigenvalues for the first 10 axes in the text output. What does this tell you about the structure in this data set?

b: Examine the graph for Axes 1 vs 2 but also axes 1 vs 3 and 2 vs 3. Compare your findings for the 2 approaches (NJ tree vs PCoA).

c: How many ``natural'' groupings do you think there are in this data set?
\end{rmdquiz}

\hypertarget{structure}{%
\section{Structure}\label{structure}}

This exercise is to show you how to run STRUCTURE. STRUCTURE is easy to run but has its pitfalls. This guide should not be used as a substitute for the STRUCTURE manual, which gives more detailed guidance.

\hypertarget{what-structure-does}{%
\subsection{What STRUCTURE does}\label{what-structure-does}}

Imagine a population of a randomly mating diploid. Suppose we have a genome-wide set of markers, not necessarily mapped, but not closely linked to one another. Studying a sample of individuals from a single population, we would expect each marker to be in Hardy-Weinberg equilibrium. We would also expect every pair of markers to be in linkage equilibrium. Now imagine a second population of the same species. This too should be in equilibrium between and within markers. Suppose the populations have been separated for long enough that their allele frequencies have diverged and that mating within each population is at random. Now if we have a single sample drawn from both populations, but we don't know the origin of the individuals in the sample, then we'll find that the population is no longer in HW equilibrium nor in linkage equilibrium. However, by trial and error, we could allocate individuals to populations until we found the division which give HW proportions and no LD within each of the populations. In practice, allocating individuals by trial and error is impossible. STRUCTURE does this allocation for us by use of a model based Bayesian approach. It has proven tremendously successful. The source publication has been cited \textgreater2000 times according to Google Scholar. STRUCTURE will cope with more complex scenarios than that described above such as: more than two populations; ploidy levels other than diploid; ancestry of individuals mixed between populations; modest linkage between markers. Latest versions handle dominant markers. Other related programs focus on specific scenarios such as:

\begin{itemize}
\item
  \emph{STRUCTURAMA} (available from same site as STRUCTURE). An alternative to STRUCTURE which includes calculation of the number of populations. Only works for outcrossed diploids.
\item
  \emph{INSTRUCT}. An alternative to STRUCTURE which works for partial selfers -- estimates selfing rate simultaneously. It is available as a command line driven stand-alone or as a web based application run remotely from here: \url{http://cbsuapps.tc.cornell.edu/InStruct.aspx}.
\item
  More recently, \emph{\href{http://software.genetics.ucla.edu/admixture/}{ADMIXTURE}} uses the same statistical model as STRUCTURE but a faster optimization algorithm.
\end{itemize}

\hypertarget{limitations-of-structure}{%
\subsection{Limitations of STRUCTURE}\label{limitations-of-structure}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  It takes time to run
\item
  Individuals are assumed to be random mating within populations -- problems for mixed mating systems.
\item
  It is often not easy to decide on the number of sub-populations.
\item
  It is best at de-convoluting distant splits in population structure rather than more recent co-ancestry
\item
  Just as for PCoA, tree drawing, and most other methods of displaying genetic relationships, it is important to remember that the answer you get will depend on the distribution of markers. Once again, unlinked, or only loosely linked markers are advised for STRUCTURE. This is important. Remember to thin your markers if their distribution is patchy with clumps of markers in high LD. In our analysis, the markers have already been thinned we shall treat them as unlinked. In practice, this is what most people seem to do.
\end{enumerate}

\emph{A note on selfing and ploidy}. If species are obligate or near obligate selfers, then they should be analysed as haploids in STRUCTURE. Otherwise STRUCTURE will attempt to assign individuals to groups which are in HW equilibrium, and with a population of near-inbred lines, spurious clusters will result. For the few heterozygous markers that there may be, in something like barley for example, the simplest procedure is to switch these markers off. This is throwing out data, so as the degree of outcrossing increases it becomes more and more costly. The programme INSTRUCT should overcome these limitations; if you have a dataset for a crop with a more mixed mating system, then it is worth a go.

\hypertarget{using-structure}{%
\subsection{Using Structure}\label{using-structure}}

We will now again use the barley data from the AGOUEB project, thinned to 307 markers. The format of the data can be seen in the file ``AG for Struct.txt.'' First we will learn how to use structure. Then, when we have some data, we will run through an exercise of how to interpret the data.

First we need to enter the data and learn how to set up a run. Structure guides you through data input process using a Windows style import wizard. To start the data import window: File, New Project\ldots{}

\textbf{Step 1}

\emph{Enter the project name and select a directory and the input file}. This process can be a fiddle -- sometimes you have to click once to select, and sometimes twice. Trial and error should see you through, however.

\textbf{Step 2}

\emph{Enter information about the data.} If you've forgotten or don't know this, clicking on ``Show data file format'' may help.

Don't forget to enter the ``Missing Data Value.'' If you don't do this, the missing data will be treated as an extra allele. In this data file, the missing is -9 which is also the default.

\emph{Ploidy:} For a collection of inbred lines enter 1 here to treat the data as haploid. Number of loci -- look in the dataset

\textbf{Step 3}

This formats data input for any additional columns or rows in the file.

\emph{Select the row of marker names} -- our file includes this.

\emph{We are not using map data} -- Linkage information can be included in the analysis for loosely linked markers. Map distances between markers can be included at the top of the input file. See the STRUCTURE manual for details. Generally, good results are obtained treating markers as unlinked. In fact for markers which are linked and in high LD (\(r^2>0.9\) or even less) it is worth deleting one of the pair (our marker thinning here has effectively done this).

\emph{Phase information}: irrelevant here because we are dealing with a haploid organism! Generally, with outbred polyploid individuals, this box would need ticking if the phase was known. See the manual for further details.

\emph{Data file stores data for individuals in a single line}: This also does not apply to haploids. With diploids, data may be entered on a single line as:

1 1 2 2 3 3 4 4 for four loci

or as:

\begin{verbatim}
1 2 3 4 
1 2 3 4 
\end{verbatim}

The second format is the default for STRUCTURE.

\textbf{Step 4}

Individual ID for each individual: we have a unique variety code as the first column of our data, so select this.

\emph{Putative population of origin for each individual}: With a priori knowledge of the population of origin of each individual, this can be entered and included in the analysis. This can work extremely well if, for example, you have a set of lines of known origin, and a set of unknown or admixed individuals. The data from the known lines is effectively used as a training set to classify the unknown lines. Here, leave it blank. If you had a mix of say, spring and winter wheats, you could use this as prior population information.

\emph{USEPOPINFO selection flag}: Used in conjunction with the previous column to identify which individuals to use in the training set and which are to be classified without prior information. See the Structure manual for more details. Leave blank here.

\emph{Phenotype information}: Do not select this.

\emph{Other extra columns}: We don't have any.

Click on Finish, Check the details are correct, then click Proceed. STRUCTURE will test the format of the data and you will be prompted to correct any errors. If the data input is successful, a spreadsheet-like display of the input data should be returned.

\begin{figure}
\centering
\includegraphics{images/pop_gen_structure1.png}
\caption{Spreadsheet-like display of the input data}
\end{figure}

\textbf{Step 5. Parameter Sets}

Before running STRUCTURE, we need to supply some input parameters. This is deceptively easy. However, the default parameters are not necessarily suitable for all datasets. See the manual for further details. One approach to selecting sensible parameter sets is to find a good publication working on a similar dataset to your own and copy from there.

Again there is an input wizard to guide you through the process. Select: Parameter Set, New\ldots{}

\emph{Run Length}: STRUCTURE uses Monte Carlo Markov Chain (MCMC) methods. To run successfully, the program iterates many times. There is generally an initial ``burn-in'' period during which the program settles down, and then a further period in which the program runs and results are generated. The longer these periods are, the more reliable the results. In practice, the numbers selected are determined by the power of your computer and your patience. Generally, select a burn in and a run length of 100,000 at least. In this demonstration, for reasons of speed, select 10,000 for each box. In our experience, long runs are best -- up to 106. One can use lower values for initial analysis and then crank up the numbers for the definitive runs.

\emph{Ancestry Model}: Use the Admixture model. For most crop applications, we expect there will have been some crossing between sub-populations. Otherwise stick with the defaults. See manual for more details

\emph{Allele frequency model}: Stick with the defaults -- correlated allele frequencies. Again, although our subpopulations differ, for crops, we expect they haven't been isolated for so long that there is no correlation in allele frequencies between them. (If you were working on indica and japonica rice, you may have another view.) See manual for more details.

\emph{Advanced}: ``Compute probability of the data'' should be ticked -- we need this to work out how many subpopulations we may have. In addition, select ``Print Q-hat''. This writes the membership of each individual in each of the inferred populations to a separate file: useful for subsequent analyses.

Finally, click OK and give your parameter set a name.

\textbf{Step 6 Running the program}

We are now finally ready to run the program. To establish how many cryptic populations we have, STRUCTURE is run multiple times, varying the population number. ``K'' is the number of populations. To check for stability / repeatability of the STRUCTURE run, it is advisable to replicate each run several times. We shall have only two replicates -- for reasons of time - and look at the range K=1-3.

\emph{Select}: Project, Start a Job. Select the name of your new project. In a session running STRUCTURE you may create several different parameter sets (e.g.~with different run times) and you would be prompted with a list here.

Remember, when running STRUCTURE on your own data, you would typically use a longer burn-in, a longer run-time, test more population numbers, and carry out more repeat runs.

Click Start. Structure will take a few minutes to run.

\textbf{When the runs have finished}:

\emph{Select: View, Simulation Summary} This presents a table of summary information from each run. The most important column here is the fourth: Ln P(D).

Ln P(D) gives the posterior probability of the population number. Hopefully, you will see that this increases (gets less negative) as population number increases, but that values are reasonably close within replicate runs. Ideally, with more time, one would continue to increase K to find the value at which Ln P(D) was maximised. In practice, this is not always possible: runs are unstable (see below), they take too long, or K can continue to increase to improbable values. Some compromise is required. The manual describes the problems of deciding on an appropriate value of K in more detail. We will discuss this below.

In the left hand side of the screen, select one of your runs. The right hand side screen should now change to display the results for that run. One can select various graphical displays of the results. It is worth exploring and experimenting with these. The ``Data plot'' options are very useful to check on the stability of the runs. In particular the plot of Log Likelihood against the number of iterations should be seen to stabilise during the burn-in and then fluctuate around a constant value during the run. There should be no trend upwards or downwards during this period (which would indicate a longer burn-in was required). The data plot of Ln P(D) may fluctuate initially, but should settle to a constant value by the end of the run, with no increasing or decreasing trend.

The ``Bar plot'' shows population membership for each individual in the dataset. This can be sorted by maximum population membership to give a cleaner display (Sort by Q). This plot is often seen in publications using STRUCTURE.

The ``Triangle plot'' shows group membership for any pair of populations, plus the residual pooled membership for the remaining populations, all in a single graph. Generally, if the dataset has population structure, and STRUCTURE has detected it, these plots will show clusters of individuals in the corners of the triangle (they come from that particular population), with some individuals scattered along the sides and in the body of the triangle (they are admixed between two or more populations). If this pattern is not seen, then you should suspect that there is no population structure, or none has been detected. We know from the values for Ln P(D) that at least three subpopulations are present. However, 10,000 iterations is too few for this dataset, especially once K is increased much beyond 3.

The ``Tree plot'' shows the relationships between the K populations in a run.

We can also plot the output files in R. View the output files in explorer -- xxx\_q and xxx\_f.~They should open in notepad or wordpad. The xxx\_q file will also open in excel and read into R. The code below will produce a histogram of the maximum population membership for each variety. (The run below is one of the K=3 runs). Remember to set your directory in R before running the code. These plots are often bimodal and often show that many individuals have strong membership of one or other of the populations, but other individuals have mixed ancestry.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{struct}\OtherTok{\textless{}{-}}\FunctionTok{read.table}\NormalTok{(“100000\_run\_5\_q”)}
\FunctionTok{attach}\NormalTok{(struct)}
\FunctionTok{hist}\NormalTok{(}\FunctionTok{pmax}\NormalTok{(V2,V3,V4))}
\end{Highlighting}
\end{Shaded}

\hypertarget{the-exercise}{%
\subsection{The exercise}\label{the-exercise}}

I have run both the AGOUEB barely data set and the teosinte data set with 100000 run length for K=1-10 with 2 runs for every K. We are going to compare the two data sets to understand the strengths and limitations of analyses using STRUCTURE for analysing inbred crop species vs outbreeding wild species. The likelihoods and alpha values are plotted against K and the bar plots for K=3, K=5 and K=10 are shown below, together with the histograms of the maximum assignment value to a population for all individuals, for K=3, K=5 and K=10. In addition, using the \href{http://taylor0.biology.ucla.edu/structureHarvester}{Structure Harvester website}, \citet{STRUCTURE2012}, I have plotted the Delta K distribution of \citet{Evanno2005}. This is a method for determining the most informative value of K.

\hypertarget{teosinte-data-set}{%
\subsubsection{Teosinte data set}\label{teosinte-data-set}}

\includegraphics{images/pop_gen_structure2.PNG}

\includegraphics{images/pop_gen_structure3.PNG}

\hypertarget{teosinte-data-set-alternative-view-with-individuals-ordered-by-natural-populations-within-states}{%
\subsubsection{Teosinte data set alternative view with individuals ordered by natural populations within states}\label{teosinte-data-set-alternative-view-with-individuals-ordered-by-natural-populations-within-states}}

States:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Guerrero
\item
  Jalisco
\item
  Mexico
\item
  Michoacan
\end{enumerate}

\includegraphics{images/pop_gen_structure4.PNG}

\hypertarget{agoueb-barely-data-set}{%
\subsubsection{AGOUEB Barely data set}\label{agoueb-barely-data-set}}

\includegraphics{images/pop_gen_structure5.PNG}

\includegraphics{images/pop_gen_structure6.PNG}

\hypertarget{exercise-questions-teosinte-data}{%
\subsubsection{Exercise questions (teosinte data)}\label{exercise-questions-teosinte-data}}

Looking firstly at the teosinte data:

\begin{rmdquiz}
\textbf{11}

Looking at the scatter plots, what happens to the likelihood, the rate of change of the likelihood and the value of alpha as K increases?

What appears to be happening as we move from K=3 to K=5 and K=10 in the bar plots? Use the histograms to confirm your observations. Note that the bar plots for the teosinte data set are arranged by collection population in 4 groups for the separate states (GUE, JAL, MEX, MIC).

What value of K may be the most appropriate for this data set, taking into account all the information in the figures above? (hint: there is no clear answer -- think of how you might try to reach a conclusion)

You have now used several techniques to investigate the population structure of teosinte from field collections. As plant breeders, you may have conducted this exercise in order to select lines for a crossing programme to improve your maize breeding germplasm (say for drought or disease resistance). Say you are selecting 15 lines, which would you select? Explain the reasons for your choice and which data analyses were most useful to reaching your decision. What other (non-genetic) information might you take into account?
\end{rmdquiz}

\hypertarget{exercise-questions-barley-data}{%
\subsubsection{Exercise questions (barley data)}\label{exercise-questions-barley-data}}

Now looking at the barley data:

\begin{rmdquiz}
\textbf{12}

Looking at the scatter plots, what happens to the likelihood, the rate of change of the likelihood and the value of alpha as K increases -- how does this differ from the teosinte data set

Looking at your own tree plot for K=3, what would you conclude about the relationship between the 3 inferred populations?

What appears to be happening as we move from K=3 to K=5 and K=10 in the bar plots? Use the histograms to confirm your observations. Is this different than the case for teosinte?

What value of K do you think is most appropriate for this data set, taking into account all the information in the figures above? How does this compare to your results from the PCoA and tree drawing exercise?

Do you think ``Structure'' is a useful tool for analysing inbred crop datasets? If not, what technique should we use to account for structure in association mapping?
\end{rmdquiz}

\hypertarget{linkage-disequilibrium-analysis}{%
\section{Linkage disequilibrium analysis}\label{linkage-disequilibrium-analysis}}

Linkage disequilibrium mapping underlies all marker-trait association studies in crop genetics. Studying the pattern of LD in your population is one of the first things you should do before carrying out association mapping. It will help in deciding the marker density and the population size that you require for a well-designed association mapping study. This will be discussed again later in the course. There are several R packages that will estimate and plot LD but they seem to have quite fast turnover. Here, we will use a package called \texttt{LDcorSV}, which can calculate \(R^2\) values accounting for population structure or kinship. We will estimate LD for both the teosinte and barley data sets, with and without accounting for population structure.

\hypertarget{linkage-disequilibrium-in-the-teosinte-and-barley-data-sets}{%
\subsection{Linkage disequilibrium in the teosinte and barley data sets}\label{linkage-disequilibrium-in-the-teosinte-and-barley-data-sets}}

Start RStudio and load packages; you may need to first install some of these packages. \texttt{ggplot2} is a very useful graphics package to get to know in R, and part of the \href{https://www.tidyverse.org/}{Tidyverse}: publication-ready figures can be created very straightforwardly. \texttt{RcolorBrewer} is useful for designing your own colour schemes in heat maps. You will need to install the package \texttt{LDcorSV}.

We will investigate LD between loci by plotting LD against genetic distance along Teosinte chromosome 1 and barley chromosome 1 from the data sets we used earlier. For convenience I have used the imputed teosinte dataset but \texttt{LDcorSV} does accept missing data. The map positions are in ``Teosintemap.txt'' and ``AGmap.txt''. In this exercise we will compare LD decay with and without correction for population structure so we will also load two population structure data sets for teosinte: ``Teosinte\_State.txt'' for the Mexican states where the sample were collected, and ``Teosinte\_structure\_k5.txt'' which has the population assignments from the Structure analysis with k=5. The columns in ``Teosinte\_State.txt'' and ``Teosinte\_structure\_k5.txt'' are the probability of belonging to each population (always 0 or 1 for states). For barley, we will account for population structure using use the results of the PCoA, using the first two principal co-ordinates (3 populations). The number of columns in the population structure data files is 1-the number of populations; the last population can be omitted as the proportion of ancestry across all populations must sum to 1.

The command \texttt{LD.measures} from \texttt{LDcorSV} calculates all pairwise \(R^2\) values, the option \texttt{S=} allows you to incorporate population structure. The output is a table with the format: marker 1 name / marker 2 name / \(R^2\) value. To plot a LD heatmap, we need to rearrange the results into a matrix, and to plot LD decay against genetic distance, we need to calculate the distance between the marker pairs. The scripts for this data manipulation are provided -- we will work through them and explain the details in the class.

\hypertarget{preparing-the-analysis}{%
\subsubsection{Preparing the analysis}\label{preparing-the-analysis}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#load packages (you will need to install them first if you don\textquotesingle{}t have them)}
\FunctionTok{library}\NormalTok{(}\StringTok{"LDcorSV"}\NormalTok{)}
\FunctionTok{library}\NormalTok{(}\StringTok{"ggplot2"}\NormalTok{)}
\FunctionTok{library}\NormalTok{(}\StringTok{"RColorBrewer"}\NormalTok{)}
\NormalTok{colnew}\OtherTok{\textless{}{-}}\FunctionTok{colorRampPalette}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"grey"}\NormalTok{,}\StringTok{"red"}\NormalTok{))(}\AttributeTok{n=}\DecValTok{99}\NormalTok{)}

\CommentTok{\#Read in the data (after setting working directory), delete "/data" if you copy this script}

\NormalTok{Teosintemap}\OtherTok{\textless{}{-}}\FunctionTok{read.table}\NormalTok{(}\StringTok{"data/Teosintemap.txt"}\NormalTok{,}\AttributeTok{header=}\NormalTok{T)}
\NormalTok{Teosinte}\OtherTok{\textless{}{-}}\FunctionTok{read.table}\NormalTok{(}\StringTok{"data/Teosinte\_Chr1.txt"}\NormalTok{,}\AttributeTok{header=}\NormalTok{T)}
\NormalTok{Teosinte\_state}\OtherTok{\textless{}{-}}\FunctionTok{read.table}\NormalTok{(}\StringTok{"data/Teosinte\_state.txt"}\NormalTok{,}\AttributeTok{header=}\NormalTok{T)}
\NormalTok{Teosinte\_structurek5}\OtherTok{\textless{}{-}}\FunctionTok{read.table}\NormalTok{(}\StringTok{"data/Teosinte\_structure\_k5.txt"}\NormalTok{,}\AttributeTok{header=}\NormalTok{T)}
\NormalTok{AGmap}\OtherTok{\textless{}{-}}\FunctionTok{read.table}\NormalTok{(}\StringTok{"data/AGmap.txt"}\NormalTok{,}\AttributeTok{header=}\NormalTok{T)}
\NormalTok{AG}\OtherTok{\textless{}{-}}\FunctionTok{read.table}\NormalTok{(}\StringTok{"data/AG\_Chr1.txt"}\NormalTok{,}\AttributeTok{header=}\NormalTok{T)}
\NormalTok{AGpcoa}\OtherTok{\textless{}{-}}\FunctionTok{read.table}\NormalTok{(}\StringTok{"data/AG\_Chr1\_pcoa.txt"}\NormalTok{,}\AttributeTok{header=}\NormalTok{T)}
\end{Highlighting}
\end{Shaded}

\hypertarget{teosinte-analysis-with-no-population-structure}{%
\subsubsection{Teosinte analysis with no population structure}\label{teosinte-analysis-with-no-population-structure}}

Run the script for the teosinte analysis with no population structure. You should obtain a heatmap of all pairwise r2 values and a scatter plot of the LD (values) versus the genetic distance:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Teosinte: calculate r\^{}2 without taking account of population structure}

\NormalTok{Teosinte\_No\_structure}\OtherTok{\textless{}{-}}\FunctionTok{LD.Measures}\NormalTok{(Teosinte,}\AttributeTok{na.presence=}\ConstantTok{TRUE}\NormalTok{)}

\CommentTok{\#Extract r\^{}2 and the intermarker distances.}
\NormalTok{Teosinte\_No\_structure}\SpecialCharTok{$}\NormalTok{dist}\OtherTok{=}\StringTok{"NA"}
\NormalTok{nmarkers}\OtherTok{=}\DecValTok{82}
\NormalTok{k}\OtherTok{\textless{}{-}}\DecValTok{0}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{ (nmarkers}\DecValTok{{-}1}\NormalTok{))}
\NormalTok{\{}
  \ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in}\NormalTok{ (i}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{nmarkers)}
\NormalTok{  \{}
\NormalTok{    k}\OtherTok{\textless{}{-}}\NormalTok{k}\SpecialCharTok{+}\DecValTok{1}
\NormalTok{    Teosinte\_No\_structure[k,}\DecValTok{4}\NormalTok{]}\OtherTok{\textless{}{-}}\FunctionTok{abs}\NormalTok{(Teosintemap[i,}\DecValTok{3}\NormalTok{]}\SpecialCharTok{{-}}\NormalTok{Teosintemap[j,}\DecValTok{3}\NormalTok{])}
\NormalTok{  \}}
\NormalTok{\}}

\NormalTok{Teosinte\_No\_struct\_heatmap}\OtherTok{\textless{}{-}}\FunctionTok{matrix}\NormalTok{(}\ConstantTok{NA}\NormalTok{,}\AttributeTok{nrow=}\NormalTok{nmarkers}\DecValTok{{-}1}\NormalTok{,}\AttributeTok{ncol=}\NormalTok{nmarkers}\DecValTok{{-}1}\NormalTok{)}
\NormalTok{k}\OtherTok{\textless{}{-}}\DecValTok{0}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{ (nmarkers}\DecValTok{{-}1}\NormalTok{))}
\NormalTok{\{}
  \ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{(nmarkers}\DecValTok{{-}1}\NormalTok{))}
\NormalTok{  \{}
\NormalTok{    k}\OtherTok{\textless{}{-}}\NormalTok{k}\SpecialCharTok{+}\DecValTok{1}
    \FunctionTok{ifelse}\NormalTok{(i}\SpecialCharTok{\textless{}}\NormalTok{(j}\SpecialCharTok{+}\DecValTok{1}\NormalTok{),Teosinte\_No\_struct\_heatmap[i,j]}\OtherTok{\textless{}{-}}\NormalTok{Teosinte\_No\_structure[k,}\DecValTok{3}\NormalTok{], k}\OtherTok{\textless{}{-}}\NormalTok{k}\DecValTok{{-}1}\NormalTok{)}
\NormalTok{  \}}
\NormalTok{\}}
\NormalTok{Teosinte\_No\_structure}\SpecialCharTok{$}\NormalTok{dist}\OtherTok{\textless{}{-}}\FunctionTok{as.numeric}\NormalTok{(Teosinte\_No\_structure}\SpecialCharTok{$}\NormalTok{dist)}
\FunctionTok{image}\NormalTok{(}\FunctionTok{t}\NormalTok{(Teosinte\_No\_struct\_heatmap),}\AttributeTok{col=}\NormalTok{colnew)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-302-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Teosinte\_No\_structure\_ordered}\OtherTok{\textless{}{-}}\NormalTok{Teosinte\_No\_structure[}\FunctionTok{order}\NormalTok{(Teosinte\_No\_structure}\SpecialCharTok{$}\NormalTok{dist),]}
\FunctionTok{ggplot}\NormalTok{(Teosinte\_No\_structure\_ordered, }\FunctionTok{aes}\NormalTok{(dist,r2))}\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{()}\SpecialCharTok{+}
    \FunctionTok{ylim}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{)}\SpecialCharTok{+}
    \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{formula=}\NormalTok{Teosinte\_No\_structure\_ordered}\SpecialCharTok{$}\NormalTok{r2}\SpecialCharTok{\textasciitilde{}}\NormalTok{ Teosinte\_No\_structure\_ordered}\SpecialCharTok{$}\NormalTok{dist,}\AttributeTok{se=}\NormalTok{F)}\SpecialCharTok{+}
    \FunctionTok{geom\_hline}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{yintercept=}\FloatTok{0.1}\NormalTok{),}\AttributeTok{col=}\StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-302-2.pdf}

Inspect the results and answer the following questions:

\begin{rmdquiz}
\textbf{13}

a: What is your preliminary conclusion based on the scatter plot. Does LD decay rapidly or slowly as a function of genetic distance between markers in this species?

b: Now take a look at the LD matrix heatmap. What extra information is provided by this plot? Can you identify pairs of markers that show relatively high LD despite being quite far apart?
\end{rmdquiz}

\hypertarget{teosinte-analysis-with-population-structure-states}{%
\subsubsection{Teosinte analysis with population structure (states)}\label{teosinte-analysis-with-population-structure-states}}

Rerun the teosinte analysis selecting state as a subpopulation grouping (the script is provided below):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Teosinte: calculate r\^{}2 taking account of population structure (states)}

\NormalTok{Teosinte\_structure}\OtherTok{\textless{}{-}}\FunctionTok{LD.Measures}\NormalTok{(Teosinte,}\AttributeTok{S=}\NormalTok{Teosinte\_state,}\AttributeTok{na.presence=}\ConstantTok{TRUE}\NormalTok{)}


\CommentTok{\#Extract r\^{}2 and the intermarker distances.}
\NormalTok{Teosinte\_structure}\SpecialCharTok{$}\NormalTok{dist}\OtherTok{=}\StringTok{"NA"}
\NormalTok{nmarkers}\OtherTok{=}\DecValTok{82}
\NormalTok{k}\OtherTok{\textless{}{-}}\DecValTok{0}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{ (nmarkers}\DecValTok{{-}1}\NormalTok{))}
\NormalTok{\{}
  \ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in}\NormalTok{ (i}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{nmarkers)}
\NormalTok{  \{}
\NormalTok{    k}\OtherTok{\textless{}{-}}\NormalTok{k}\SpecialCharTok{+}\DecValTok{1}
\NormalTok{    Teosinte\_structure[k,}\DecValTok{5}\NormalTok{]}\OtherTok{\textless{}{-}}\FunctionTok{abs}\NormalTok{(Teosintemap[i,}\DecValTok{3}\NormalTok{]}\SpecialCharTok{{-}}\NormalTok{Teosintemap[j,}\DecValTok{3}\NormalTok{])}
\NormalTok{  \}}
\NormalTok{\}}

\NormalTok{Teosinte\_struct\_heatmap}\OtherTok{\textless{}{-}}\FunctionTok{matrix}\NormalTok{(}\ConstantTok{NA}\NormalTok{,}\AttributeTok{nrow=}\NormalTok{nmarkers}\DecValTok{{-}1}\NormalTok{,}\AttributeTok{ncol=}\NormalTok{nmarkers}\DecValTok{{-}1}\NormalTok{)}
\NormalTok{k}\OtherTok{\textless{}{-}}\DecValTok{0}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{ (nmarkers}\DecValTok{{-}1}\NormalTok{))}
\NormalTok{\{}
  \ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{(nmarkers}\DecValTok{{-}1}\NormalTok{))}
\NormalTok{  \{}
\NormalTok{    k}\OtherTok{\textless{}{-}}\NormalTok{k}\SpecialCharTok{+}\DecValTok{1}
    \FunctionTok{ifelse}\NormalTok{(i}\SpecialCharTok{\textless{}}\NormalTok{(j}\SpecialCharTok{+}\DecValTok{1}\NormalTok{),Teosinte\_struct\_heatmap[i,j]}\OtherTok{\textless{}{-}}\NormalTok{Teosinte\_structure[k,}\DecValTok{4}\NormalTok{], k}\OtherTok{\textless{}{-}}\NormalTok{k}\DecValTok{{-}1}\NormalTok{)}
\NormalTok{  \}}
\NormalTok{\}}
\NormalTok{Teosinte\_structure}\SpecialCharTok{$}\NormalTok{dist}\OtherTok{\textless{}{-}}\FunctionTok{as.numeric}\NormalTok{(Teosinte\_structure}\SpecialCharTok{$}\NormalTok{dist)}
\FunctionTok{image}\NormalTok{(}\FunctionTok{t}\NormalTok{(Teosinte\_struct\_heatmap),}\AttributeTok{col=}\NormalTok{colnew )}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-304-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Teosinte\_structure\_ordered}\OtherTok{\textless{}{-}}\NormalTok{Teosinte\_structure[}\FunctionTok{order}\NormalTok{(Teosinte\_structure}\SpecialCharTok{$}\NormalTok{dist),]}
\FunctionTok{ggplot}\NormalTok{(Teosinte\_structure\_ordered, }\FunctionTok{aes}\NormalTok{(dist,r2s))}\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{()}\SpecialCharTok{+}
  \FunctionTok{ylim}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{formula=}\NormalTok{Teosinte\_structure\_ordered}\SpecialCharTok{$}\NormalTok{r2s}\SpecialCharTok{\textasciitilde{}}\NormalTok{ Teosinte\_structure\_ordered}\SpecialCharTok{$}\NormalTok{dist,}\AttributeTok{se=}\NormalTok{F)}\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{yintercept=}\FloatTok{0.1}\NormalTok{),}\AttributeTok{col=}\StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-304-2.pdf}

Now consider the following questions:

\begin{rmdquiz}
\textbf{14}

a: Inspect the scatter plot. What is similar and what is different from the one obtained from the model that does not account for the population structure? Hint: Pay attention to the scale of the Y-axis.

b: Compare the LD matrix plot with that obtained from the model that does not account for population structure.
\end{rmdquiz}

\hypertarget{teosinte-analysis-with-population-structure-structure-k5}{%
\subsubsection{Teosinte analysis with population structure (Structure k=5)}\label{teosinte-analysis-with-population-structure-structure-k5}}

You might have observed some effect from the inclusion of the groups. However, we already know from the Hardy-Weinberg analysis that ``state'' does not seem to very accurately represent the true population structure in this data set. Rerun the analysis again using the Structure output as the population structure.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Teosinte: calculate r\^{}2 taking account of population structure (Structure k=5)}

\NormalTok{Teosinte\_structurek5}\OtherTok{\textless{}{-}}\FunctionTok{LD.Measures}\NormalTok{(Teosinte,}\AttributeTok{S=}\NormalTok{Teosinte\_structurek5,}\AttributeTok{na.presence=}\ConstantTok{TRUE}\NormalTok{)}

\CommentTok{\#Extract r\^{}2 and the intermarker distances.}
\NormalTok{Teosinte\_structurek5}\SpecialCharTok{$}\NormalTok{dist}\OtherTok{=}\StringTok{"NA"}
\NormalTok{nmarkers}\OtherTok{=}\DecValTok{82}
\NormalTok{k}\OtherTok{\textless{}{-}}\DecValTok{0}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{ (nmarkers}\DecValTok{{-}1}\NormalTok{))}
\NormalTok{\{}
  \ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in}\NormalTok{ (i}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{nmarkers)}
\NormalTok{  \{}
\NormalTok{    k}\OtherTok{\textless{}{-}}\NormalTok{k}\SpecialCharTok{+}\DecValTok{1}
\NormalTok{    Teosinte\_structurek5[k,}\DecValTok{5}\NormalTok{]}\OtherTok{\textless{}{-}}\FunctionTok{abs}\NormalTok{(Teosintemap[i,}\DecValTok{3}\NormalTok{]}\SpecialCharTok{{-}}\NormalTok{Teosintemap[j,}\DecValTok{3}\NormalTok{])}
\NormalTok{  \}}
\NormalTok{\}}

\NormalTok{Teosinte\_structk5\_heatmap}\OtherTok{\textless{}{-}}\FunctionTok{matrix}\NormalTok{(}\ConstantTok{NA}\NormalTok{,}\AttributeTok{nrow=}\NormalTok{nmarkers}\DecValTok{{-}1}\NormalTok{,}\AttributeTok{ncol=}\NormalTok{nmarkers}\DecValTok{{-}1}\NormalTok{)}
\NormalTok{k}\OtherTok{\textless{}{-}}\DecValTok{0}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{ (nmarkers}\DecValTok{{-}1}\NormalTok{))}
\NormalTok{\{}
  \ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{(nmarkers}\DecValTok{{-}1}\NormalTok{))}
\NormalTok{  \{}
\NormalTok{    k}\OtherTok{\textless{}{-}}\NormalTok{k}\SpecialCharTok{+}\DecValTok{1}
    \FunctionTok{ifelse}\NormalTok{(i}\SpecialCharTok{\textless{}}\NormalTok{(j}\SpecialCharTok{+}\DecValTok{1}\NormalTok{),Teosinte\_structk5\_heatmap[i,j]}\OtherTok{\textless{}{-}}\NormalTok{Teosinte\_structurek5[k,}\DecValTok{4}\NormalTok{], k}\OtherTok{\textless{}{-}}\NormalTok{k}\DecValTok{{-}1}\NormalTok{)}
\NormalTok{  \}}
\NormalTok{\}}
\NormalTok{Teosinte\_structurek5}\SpecialCharTok{$}\NormalTok{dist}\OtherTok{\textless{}{-}}\FunctionTok{as.numeric}\NormalTok{(Teosinte\_structurek5}\SpecialCharTok{$}\NormalTok{dist)}
\FunctionTok{image}\NormalTok{(}\FunctionTok{t}\NormalTok{(Teosinte\_structk5\_heatmap),}\AttributeTok{col=}\NormalTok{colnew )}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-306-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Teosinte\_structurek5\_ordered}\OtherTok{\textless{}{-}}\NormalTok{Teosinte\_structurek5[}\FunctionTok{order}\NormalTok{(Teosinte\_structurek5}\SpecialCharTok{$}\NormalTok{dist),]}
\FunctionTok{ggplot}\NormalTok{(Teosinte\_structurek5\_ordered, }\FunctionTok{aes}\NormalTok{(dist,r2s))}\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{()}\SpecialCharTok{+}
  \FunctionTok{ylim}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{formula=}\NormalTok{Teosinte\_structurek5\_ordered}\SpecialCharTok{$}\NormalTok{r2s}\SpecialCharTok{\textasciitilde{}}\NormalTok{ Teosinte\_structurek5\_ordered}\SpecialCharTok{$}\NormalTok{dist,}\AttributeTok{se=}\NormalTok{F)}\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{yintercept=}\FloatTok{0.1}\NormalTok{),}\AttributeTok{col=}\StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-306-2.pdf}

\begin{rmdquiz}
\textbf{15}
Compare the results with those in the earlier parts. What do you conclude?
\end{rmdquiz}

\hypertarget{barley-association-mapping-panel}{%
\subsubsection{Barley association mapping panel}\label{barley-association-mapping-panel}}

Understanding the breakdown of LD and whether the pattern of LD is affected by population structure are critical components of all association mapping studies such as the barely AGOUEB study.

\begin{rmdquiz}
\textbf{16}
Before looking at any results, what is your expectation of LD decay as function of genetic distance in the barley association mapping panel, compared to teosinte? Think of all the factors that differ between the two data sets and how they may affect LD.
\end{rmdquiz}

\hypertarget{barley-association-mapping-panel-no-population-structure}{%
\subsubsection{Barley association mapping panel (no population structure)}\label{barley-association-mapping-panel-no-population-structure}}

Run the LD analysis for barley chromosome 1 without population structure.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Barley: calculate r\^{}2 without taking account of population structure}

\NormalTok{AG\_No\_structure}\OtherTok{\textless{}{-}}\FunctionTok{LD.Measures}\NormalTok{(AG,}\AttributeTok{na.presence=}\ConstantTok{TRUE}\NormalTok{)}

\CommentTok{\#Extract r\^{}2 and the intermarker distances.}
\NormalTok{AG\_No\_structure}\SpecialCharTok{$}\NormalTok{dist}\OtherTok{=}\StringTok{"NA"}
\NormalTok{nmarkers}\OtherTok{=}\DecValTok{159}
\NormalTok{k}\OtherTok{\textless{}{-}}\DecValTok{0}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{ (nmarkers}\DecValTok{{-}1}\NormalTok{))}
\NormalTok{\{}
  \ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in}\NormalTok{ (i}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{nmarkers)}
\NormalTok{  \{}
\NormalTok{    k}\OtherTok{\textless{}{-}}\NormalTok{k}\SpecialCharTok{+}\DecValTok{1}
\NormalTok{    AG\_No\_structure[k,}\DecValTok{4}\NormalTok{]}\OtherTok{\textless{}{-}}\FunctionTok{abs}\NormalTok{(AGmap[i,}\DecValTok{3}\NormalTok{]}\SpecialCharTok{{-}}\NormalTok{AGmap[j,}\DecValTok{3}\NormalTok{])}
\NormalTok{  \}}
\NormalTok{\}}

\NormalTok{AG\_No\_struct\_heatmap}\OtherTok{\textless{}{-}}\FunctionTok{matrix}\NormalTok{(}\ConstantTok{NA}\NormalTok{,}\AttributeTok{nrow=}\NormalTok{nmarkers}\DecValTok{{-}1}\NormalTok{,}\AttributeTok{ncol=}\NormalTok{nmarkers}\DecValTok{{-}1}\NormalTok{)}
\NormalTok{k}\OtherTok{\textless{}{-}}\DecValTok{0}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{ (nmarkers}\DecValTok{{-}1}\NormalTok{))}
\NormalTok{\{}
  \ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{(nmarkers}\DecValTok{{-}1}\NormalTok{))}
\NormalTok{  \{}
\NormalTok{    k}\OtherTok{\textless{}{-}}\NormalTok{k}\SpecialCharTok{+}\DecValTok{1}
    \FunctionTok{ifelse}\NormalTok{(i}\SpecialCharTok{\textless{}}\NormalTok{(j}\SpecialCharTok{+}\DecValTok{1}\NormalTok{),AG\_No\_struct\_heatmap[i,j]}\OtherTok{\textless{}{-}}\NormalTok{AG\_No\_structure[k,}\DecValTok{3}\NormalTok{], k}\OtherTok{\textless{}{-}}\NormalTok{k}\DecValTok{{-}1}\NormalTok{)}
\NormalTok{  \}}
\NormalTok{\}}
\NormalTok{AG\_No\_structure}\SpecialCharTok{$}\NormalTok{dist}\OtherTok{\textless{}{-}}\FunctionTok{as.numeric}\NormalTok{(AG\_No\_structure}\SpecialCharTok{$}\NormalTok{dist)}
\FunctionTok{image}\NormalTok{(}\FunctionTok{t}\NormalTok{(AG\_No\_struct\_heatmap), }\AttributeTok{col=}\NormalTok{colnew)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-309-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AG\_No\_structure\_ordered}\OtherTok{\textless{}{-}}\NormalTok{AG\_No\_structure[}\FunctionTok{order}\NormalTok{(AG\_No\_structure}\SpecialCharTok{$}\NormalTok{dist),]}
\FunctionTok{ggplot}\NormalTok{(AG\_No\_structure\_ordered, }\FunctionTok{aes}\NormalTok{(dist,r2))}\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{()}\SpecialCharTok{+}
  \FunctionTok{ylim}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{formula=}\NormalTok{AG\_No\_structure\_ordered}\SpecialCharTok{$}\NormalTok{r2}\SpecialCharTok{\textasciitilde{}}\NormalTok{ AG\_No\_structure\_ordered}\SpecialCharTok{$}\NormalTok{dist,}\AttributeTok{se=}\NormalTok{F)}\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{yintercept=}\FloatTok{0.1}\NormalTok{),}\AttributeTok{col=}\StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-309-2.pdf}

\begin{rmdquiz}
\textbf{17}

a: Inspect the scatter plot of r2 values versus genetic distance. How rapidly does LD decay in this population?

b: What is your initial interpretation of both plots?
\end{rmdquiz}

\hypertarget{re-estimate-ld-in-the-barley-data-set-controlling-for-population-structure.}{%
\subsubsection{Re-estimate LD in the barley data set controlling for population structure.}\label{re-estimate-ld-in-the-barley-data-set-controlling-for-population-structure.}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Barley: calculate r\^{}2 taking account of population structure (pcoas 1 and 2)}

\NormalTok{AG\_structure}\OtherTok{\textless{}{-}}\FunctionTok{LD.Measures}\NormalTok{(AG,}\AttributeTok{S=}\NormalTok{AGpcoa,}\AttributeTok{na.presence=}\ConstantTok{TRUE}\NormalTok{)}


\CommentTok{\#Extract r\^{}2 and the intermarker distances.}
\NormalTok{AG\_structure}\SpecialCharTok{$}\NormalTok{dist}\OtherTok{=}\StringTok{"NA"}
\NormalTok{nmarkers}\OtherTok{=}\DecValTok{159}
\NormalTok{k}\OtherTok{\textless{}{-}}\DecValTok{0}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{ (nmarkers}\DecValTok{{-}1}\NormalTok{))}
\NormalTok{\{}
  \ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in}\NormalTok{ (i}\SpecialCharTok{+}\DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{nmarkers)}
\NormalTok{  \{}
\NormalTok{    k}\OtherTok{\textless{}{-}}\NormalTok{k}\SpecialCharTok{+}\DecValTok{1}
\NormalTok{    AG\_structure[k,}\DecValTok{5}\NormalTok{]}\OtherTok{\textless{}{-}}\FunctionTok{abs}\NormalTok{(AGmap[i,}\DecValTok{3}\NormalTok{]}\SpecialCharTok{{-}}\NormalTok{AGmap[j,}\DecValTok{3}\NormalTok{])}
\NormalTok{  \}}
\NormalTok{\}}

\NormalTok{AG\_struct\_heatmap}\OtherTok{\textless{}{-}}\FunctionTok{matrix}\NormalTok{(}\ConstantTok{NA}\NormalTok{,}\AttributeTok{nrow=}\NormalTok{nmarkers}\DecValTok{{-}1}\NormalTok{,}\AttributeTok{ncol=}\NormalTok{nmarkers}\DecValTok{{-}1}\NormalTok{)}
\NormalTok{k}\OtherTok{\textless{}{-}}\DecValTok{0}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{ (nmarkers}\DecValTok{{-}1}\NormalTok{))}
\NormalTok{\{}
  \ControlFlowTok{for}\NormalTok{(j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{(nmarkers}\DecValTok{{-}1}\NormalTok{))}
\NormalTok{  \{}
\NormalTok{    k}\OtherTok{\textless{}{-}}\NormalTok{k}\SpecialCharTok{+}\DecValTok{1}
    \FunctionTok{ifelse}\NormalTok{(i}\SpecialCharTok{\textless{}}\NormalTok{(j}\SpecialCharTok{+}\DecValTok{1}\NormalTok{),AG\_struct\_heatmap[i,j]}\OtherTok{\textless{}{-}}\NormalTok{AG\_structure[k,}\DecValTok{4}\NormalTok{], k}\OtherTok{\textless{}{-}}\NormalTok{k}\DecValTok{{-}1}\NormalTok{)}
\NormalTok{  \}}
\NormalTok{\}}
\NormalTok{AG\_structure}\SpecialCharTok{$}\NormalTok{dist}\OtherTok{\textless{}{-}}\FunctionTok{as.numeric}\NormalTok{(AG\_structure}\SpecialCharTok{$}\NormalTok{dist)}
\FunctionTok{image}\NormalTok{(}\FunctionTok{t}\NormalTok{(AG\_struct\_heatmap),}\AttributeTok{col=}\NormalTok{colnew)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-311-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AG\_structure\_ordered}\OtherTok{\textless{}{-}}\NormalTok{AG\_structure[}\FunctionTok{order}\NormalTok{(AG\_structure}\SpecialCharTok{$}\NormalTok{dist),]}
\FunctionTok{ggplot}\NormalTok{(AG\_structure\_ordered, }\FunctionTok{aes}\NormalTok{(dist,r2s))}\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{()}\SpecialCharTok{+}
  \FunctionTok{ylim}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{formula=}\NormalTok{AG\_structure\_ordered}\SpecialCharTok{$}\NormalTok{r2s}\SpecialCharTok{\textasciitilde{}}\NormalTok{ AG\_structure\_ordered}\SpecialCharTok{$}\NormalTok{dist,}\AttributeTok{se=}\NormalTok{F)}\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{yintercept=}\FloatTok{0.1}\NormalTok{),}\AttributeTok{col=}\StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-311-2.pdf}

\begin{rmdquiz}
\textbf{18}

a: Compare the scatter plot with that obtained from the null model. Does the new plot give a clearer picture of the LD decay than before? If so, why?

b: What is your conclusion regarding LD between pairs of markers in general when comparing the new LD matrix plot with that from the model that does not account for population structure? ?
\end{rmdquiz}

\hypertarget{comparisons-accross-datasets.}{%
\subsubsection{Comparisons accross datasets.}\label{comparisons-accross-datasets.}}

Compare the results observed for barley with those for Teosinte.

\begin{rmdquiz}
\textbf{19}
What might be the explanation for the differences? How might this impact on association mapping studies?
\end{rmdquiz}

\hypertarget{recombination-and-linkage-disequilibrium-applications}{%
\section{Recombination and linkage disequilibrium: applications}\label{recombination-and-linkage-disequilibrium-applications}}

The following figure is from a paper \citet{Gardner2016} which created a genetic map for wheat using an experimental MAGIC (Multiparent Advanced Generation InterCross) population. The population was created by crossing 8 distinct UK wheat varieties in all combinations over 3 generations, so that each individual has on average 1/8 of its genome from each parent. The lines are then selfed for several generations. This population has an overall high recombination rate compared to standard biparental mapping populations used for QTL detection. The top figure plots the genetic map (based on recombination frequencies) vs the physical map (in base pairs) for chromosome 3B. The genetic map is based on recombination frequencies estimated in the population. The bottom figure shows the recombination rate along the chromosome based on a comparison of the 2 maps. For mapping QTL, we would use the genetic map.

\begin{figure}
\centering
\includegraphics{images/pop_gen_magic.png}
\caption{MAGIC genetic vs.~physical map positions (a). Recombination rate in MAGIC population (b).}
\end{figure}

\begin{rmdquiz}
\textbf{20}

a: It is clear that the relationship between the 2 maps is non-linear. Explain why this is so with reference to the recombination rate graph

b: Why does there appear to be a large region of no recombination in the centre of the chromosome?

c: Recombination frequencies were estimated from the linkage disequilibrium generated in this population. What would a graph of LD along the chromosome look like? Which parts of the chromosome have high and low linkage disequilibrium? What implications does this have for QTL mapping in wheat?

d: As well as the centre of the chromosome, there are other shorter regions (e.g.~around 610 Mb and 690 Mb) of the chromosome which have low recombination. The markers in these regions are also distinctive in being out of Hardy-Weinberg equilibrium. What could be going on here? (hint: think back to the causes of underdominance)
\end{rmdquiz}

\hypertarget{pedigree-analysis}{%
\section{Pedigree analysis}\label{pedigree-analysis}}

Inspect the complex pedigree below.

\begin{rmdquiz}
\emph{21} Calculate the inbreeding coefficient of individual N in the following complex pedigree. This is difficult so we give two hints:

a: there are 7 possible paths
b: two of the 7 paths have an inbred individual at the top -- you must account for the fact that this individual is inbred.
\end{rmdquiz}

\begin{figure}
\centering
\includegraphics{images/pop_gen_pedigree.png}
\caption{Example of a complex pedigree}
\end{figure}

\hypertarget{Imputation}{%
\chapter{Imputation.}\label{Imputation}}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Marker data almost always come with missing values. When working with a small number of values, say regressing a phenotype on a single marker, this is not a problem: the records with the missing data are simply dropped from the analysis. However, with very many markers, and in analyses which work on multiple markers simultaneously, this does cause problems. For example, computation of kinship from a marker set with missing data can result in a matrix which does not invert. Many methods using genomic selection also require complete data.

For this purpose, methods have been adapted to fill in the missing values with a best guess, and then analyses proceed as if the data were complete. The validity of using these imputed data as if they were real is questionable: if the imputation is accurate, then there is no problem, but with increasing inaccuracy come enhanced Type I and Type II error rates; significance tests assume you have more data than is true. Also, biased imputation (of heterozygotes for one of the homozygous classes say) could act to increase or reduce power -- it is hard to predict which.

There are other uses of imputation. If we knew one marker could be perfectly predicted from a set of others, there would be no requirement to genotype it. Over the whole genome, this can save a lot of genotyping. This approach is described as ``haplotype tagging'' or SNP tagging. Imputation can also be used to search for genotype errors: where a predicted SNP call disagrees with the observed call this may indicate bad data rather than bad prediction. This is akin to the use of observed double recombinants in linkage analysis as indicators of genotyping error. Finally, in genotyping by sequencing, individuals are sequenced with very low coverage to keep costs down. Very many SNPs are detected but with a massive missing data problem. However, because the SNP density is so high there are always SNPs in high LD with each other that can be used to patch-in the missing data. That's the theory anyway.

There are specific methods for imputation of SNP data. These rely on having a genetic or physical map and explicitly or implicitly model linkage disequilibrium. These are much used in human genetics with very high densities of markers. Marchini and Howie (2010) give a review of methods and software.

For many of the datasets we have available we don't have an adequate genetic map so these methods cannot be applied. In addition, I am uncertain how well they perform with errors in map order or in allocation of markers to linkage groups, or whether the marker density used in many crops is adequate for their successful application. This is an area of active research.

Fortunately, imputation is not restricted to genetic applications so generic methods can also be used. It is some of these which we shall look at today. We'll first describe three possible approaches to imputation.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Mean}: replace the missing marker genotype with the mean allele frequency (or twice the allele frequency for diploids). This isn't strictly imputation. It has the merit of being quick and easy. It is also probably adequate for many purposes where significance testing and/or precise estimation of effects are not necessary. Examples would be in computation of kinship matrices and in genomic prediction.
\item
  \textbf{Singular value decomposition (SVD)}: approximates the missing values from the first n row and column eigenvectors and values. This is not straight forward since SVD requires a complete matrix in the first place, so an initial guess at the missing values, usually the column means, is first made, followed by prediction from SVD, often with iteration until the prediction of the missing values stabilises.
\item
  \textbf{Random Forest}: This method is based on decision trees. A decision tree is a classification or regression system whereby data for an outcome variable are repeatedly split into smaller subsets on the basis of an optimally selected threshold of an optimally selected predictor. At each split different predictors/thresholds are selected. Subsequently, given data on the predictors, by following the tree through these splits, an estimate of the outcome variable is made. A random forest is a resampling method whereby multiple decision trees are built from bootstrapped samples of the data. For each bootstrapped dataset, at each split, a different subset of predictors is sampled at random from which the optimal predictor/threshold for that subset is then selected. The final set of random forest predictions is just the average of the predictions from each tree. The resampling ensures that the random forest predictions are more robust than those from single trees.
\end{enumerate}

For more on these and other contemporary approaches to handling large data sets see \citet{james_2013} for more details. A pdf is also available free online but it's worth buying.

\hypertarget{the-exercise-1}{%
\section{The exercise}\label{the-exercise-1}}

The aims of today's exercise are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Compare the different imputation approaches.
\item
  Examine how accuracy of imputation decreases with decreasing LD among the markers.
\end{enumerate}

We are going to work on a subset of data from a NIAB MAGIC population. We have extracted 4434 markers on 710 lines for which there are no missing data. This means we can artificially delete data, impute these missing values and compare the imputed values with the originals. Many of these markers are in very high LD with each other. We shall therefore delete one marker of each pair with correlation above a given threshold and repeat the delete--impute--compare process to assess how accuracy of imputation decreases with LD.

Below is a description of the work flow:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Skim the markers on correlation coefficients of \textless1, \textless0.9, \textless0.8, \textless0.7, \textless0.6, \textless0.5 to give seven datasets with reduced levels of LD among the markers. To speed things up we have already carried out this stage of the exercise (using the ``deduplicate'' script provided in the Chapter 8 data folder). The files are labeled ``magic.full.txt'', ``magic.lt.1.0.txt'', ``magic.lt.0.9.txt'',etc. These are in the data folder on the course website.
\item
  For each dataset, select 500 markers at random -- so that accuracy of imputation isn't confounded with numbers of markers. We would expect better results with higher marker density.
\item
  For each dataset, delete 10\% of the genotypes at random.
\item
  Impute the missing data and compare observed with imputed.
\item
  Tabulate the results for each LD threshold and method of imputation and discuss.
\end{enumerate}

To speed things up, we are going to share the workload. Everyone will run the exercise using only one of the different MAGIC data sets (1.0, 0.9, etc), with 3-4 people assigned to each data set.

The order in which we run the analyses will depend on the time of day: the random forest approach is slow so best run over a break. We shall start with the svd package.

\begin{rmdnote}
There is a R script the data folder for today: ``Imputation\_Script\_2021.R''. You can use this to avoid copying the sections below. Or just copy from this tutorial if you rather.
\end{rmdnote}

\hypertarget{data-preparation-for-all-methods}{%
\section{Data preparation (for all methods)}\label{data-preparation-for-all-methods}}

Note that the data are coded `0, 1, 2' and that there is a low frequency of heterozygosity among these lines -- they are F4:5 families. Read the data in, here shown for the data after skimming on a correlation coefficient of 0.5

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#delete the "data/" if you copy}
\NormalTok{magic.data}\OtherTok{\textless{}{-}}\FunctionTok{read.table}\NormalTok{(}\StringTok{"data/magic.lt.0.5.txt"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Sample 500 markers using the function sample.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample}\FloatTok{.500}\OtherTok{\textless{}{-}}\NormalTok{magic.data[,}\FunctionTok{sort}\NormalTok{(}\FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{dim}\NormalTok{(magic.data)[}\DecValTok{2}\NormalTok{],}\DecValTok{500}\NormalTok{,}\AttributeTok{replace=}\ConstantTok{FALSE}\NormalTok{))]}
\end{Highlighting}
\end{Shaded}

Delete 10\% of markers at random (may take a while):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{missing}\OtherTok{\textless{}{-}}\NormalTok{sample}\FloatTok{.500}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{710}\NormalTok{) \{}
  \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{500}\NormalTok{)  \{}
    \ControlFlowTok{if}\NormalTok{(}\FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{) }\SpecialCharTok{\textless{}}\FloatTok{0.1}\NormalTok{) missing[i,j]}\OtherTok{\textless{}{-}}\ConstantTok{NA}
\NormalTok{  \}}
\NormalTok{\}}
\FunctionTok{dim}\NormalTok{(missing) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 710 500
\end{verbatim}

We can use the below script to calculate the mean NA \% per marker:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#count NA per marker}
\NormalTok{na\_count }\OtherTok{\textless{}{-}}\FunctionTok{sapply}\NormalTok{(missing, }\ControlFlowTok{function}\NormalTok{(y) }\FunctionTok{sum}\NormalTok{(}\FunctionTok{length}\NormalTok{(}\FunctionTok{which}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(y)))))}
\CommentTok{\#convert to a percentage}
\NormalTok{na\_count }\OtherTok{\textless{}{-}}\NormalTok{(na\_count}\SpecialCharTok{/}\DecValTok{710}\NormalTok{)}\SpecialCharTok{*}\DecValTok{100}
\CommentTok{\#pull mean}
\FunctionTok{mean}\NormalTok{(na\_count)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 9.927887
\end{verbatim}

On average we have 10\% missing data per marker, as we intended. You now have a data set ready to be imputed.

\hypertarget{svd-imputation}{%
\section{SVD imputation}\label{svd-imputation}}

In this exercise, we shall use the package \texttt{softImpute} which implements a sophisticated version of SVD which seems to work, though I don't wholly understand the method.

The basic command in softImpute is: \texttt{softImpute(data.matrix,\ rank.max\ =\ 100)}.

\texttt{data} is the name of the matrix holding your data.

\texttt{rank.max} is the maximum rank of the solution. It must be less than or equal to the smallest of either the number of rows or columns. The larger the value, the longer the programme will take to run but potentially the more accurate the solution. The more structured the data, the lower \texttt{rank.max} could be. If \texttt{rank.max} is too large we risk overfitting (adding noise) and accuracy of imputation could decrease. We are going to run \texttt{softImpute} with 500 markers at a time, so \texttt{500} is the maximum possible value. There are other parameters that one can adjust, but this is the minimum we need and seems to work well on this dataset.

There are two other useful commands within the package that we shall use:

\texttt{biScale(data,row.center=F,row.scale=F,col.center=T,col.scale=T)}

\texttt{biScale} standardises rows and columns to means of zero and variances of one. It is similar to the standard R command scale, except it can scale over rows and columns simultaneously. Marker data are generally standardised for use in PCO and kinship matrices so we shall adopt that as the default position here. In the package, the default is to standardise data over both rows and columns. We shall be explicit in defining that we only want columns (ie markers) standardised.

\texttt{complete(data,softImpute.output.file,unscale=T)}

\texttt{complete} takes the original data, and substitutes the imputed data for the missing values. A convenient option is \texttt{unscale=T}. This transforms the imputed values from the standardised scale (if you used \texttt{biScale}) back to the original. This back transformation may not always be required, but for comparison of original and imputed data it makes things easier.

All the other commands we shall use are standard R for manipulating and displaying the data and results.

Now we can start\ldots.

Scale the selected data:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(softImpute) }\CommentTok{\#you will need to install if you have not already}
\NormalTok{missing.scale}\OtherTok{\textless{}{-}}\FunctionTok{biScale}\NormalTok{(missing,}\AttributeTok{row.center=}\NormalTok{F,}\AttributeTok{row.scale=}\NormalTok{F,}\AttributeTok{col.center=}\NormalTok{T,}\AttributeTok{col.scale=}\NormalTok{T)}
\end{Highlighting}
\end{Shaded}

Impute the missing data -- may take a while. Note we have set the maximum rank of the solution to 100.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{missing.scale.imp}\OtherTok{\textless{}{-}}\FunctionTok{softImpute}\NormalTok{(missing.scale,}\AttributeTok{rank.max=}\DecValTok{100}\NormalTok{,}\AttributeTok{lambda=}\DecValTok{0}\NormalTok{)  }
\end{Highlighting}
\end{Shaded}

Create a new matrix, including the imputed data, transformed back to the original scale.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{missing.imp}\OtherTok{\textless{}{-}}\FunctionTok{complete}\NormalTok{(missing,missing.scale.imp,}\AttributeTok{unscale=}\NormalTok{T)}
\end{Highlighting}
\end{Shaded}

Plot the imputed data for each original class:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{boxplot}\NormalTok{(missing.imp[}\FunctionTok{is.na}\NormalTok{(missing)]}\SpecialCharTok{\textasciitilde{}}\NormalTok{sample}\FloatTok{.500}\NormalTok{[}\FunctionTok{is.na}\NormalTok{(missing)],}\AttributeTok{main=}\StringTok{"distribution of imputed data"}\NormalTok{,}\AttributeTok{ylab=}\StringTok{"imputed"}\NormalTok{,}\AttributeTok{xlab=}\StringTok{"original"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-324-1.pdf}

Plot histograms of the imputed data for each class in turn:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(missing.imp[}\FunctionTok{is.na}\NormalTok{(missing)][sample}\FloatTok{.500}\NormalTok{[}\FunctionTok{is.na}\NormalTok{(missing)]}\SpecialCharTok{==}\DecValTok{1}\NormalTok{],}\AttributeTok{breaks=}\DecValTok{1000}\NormalTok{,}\AttributeTok{main=}\StringTok{"imputed hets"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-325-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(missing.imp[}\FunctionTok{is.na}\NormalTok{(missing)][sample}\FloatTok{.500}\NormalTok{[}\FunctionTok{is.na}\NormalTok{(missing)]}\SpecialCharTok{==}\DecValTok{2}\NormalTok{],}\AttributeTok{breaks=}\DecValTok{1000}\NormalTok{,}\AttributeTok{main=}\StringTok{"imputed homs (2)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-325-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(missing.imp[}\FunctionTok{is.na}\NormalTok{(missing)][sample}\FloatTok{.500}\NormalTok{[}\FunctionTok{is.na}\NormalTok{(missing)]}\SpecialCharTok{==}\DecValTok{0}\NormalTok{],}\AttributeTok{breaks=}\DecValTok{1000}\NormalTok{,}\AttributeTok{main=}\StringTok{"imputed homs (0)"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-325-3.pdf}

What do you think?

The imputed data are on the original scale, but they are non-integer values. Round these to the nearest whole number. Values of 0, 1 and 2 represent the three genotype classes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{missing.imp.round}\OtherTok{\textless{}{-}}\FunctionTok{round}\NormalTok{(missing.imp)}
\end{Highlighting}
\end{Shaded}

On your own computer, look at the plots of rounded, imputed genotype data:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{boxplot}\NormalTok{(missing.imp.round[}\FunctionTok{is.na}\NormalTok{(missing)]}\SpecialCharTok{\textasciitilde{}}\NormalTok{sample}\FloatTok{.500}\NormalTok{[}\FunctionTok{is.na}\NormalTok{(missing)],}\AttributeTok{main=}\StringTok{"distribution of imputed data"}\NormalTok{,}\AttributeTok{ylab=}\StringTok{"imputed"}\NormalTok{,}\AttributeTok{xlab=}\StringTok{"original"}\NormalTok{)}

\FunctionTok{hist}\NormalTok{(missing.imp.round[}\FunctionTok{is.na}\NormalTok{(missing)][sample}\FloatTok{.500}\NormalTok{[}\FunctionTok{is.na}\NormalTok{(missing)]}\SpecialCharTok{==}\DecValTok{1}\NormalTok{],}\AttributeTok{breaks=}\DecValTok{1000}\NormalTok{,}\AttributeTok{main=}\StringTok{"imputed hets"}\NormalTok{)}

\FunctionTok{hist}\NormalTok{(missing.imp.round[}\FunctionTok{is.na}\NormalTok{(missing)][sample}\FloatTok{.500}\NormalTok{[}\FunctionTok{is.na}\NormalTok{(missing)]}\SpecialCharTok{==}\DecValTok{2}\NormalTok{],}\AttributeTok{breaks=}\DecValTok{1000}\NormalTok{,}\AttributeTok{main=}\StringTok{"imputed homs (2)"}\NormalTok{)}

\FunctionTok{hist}\NormalTok{(missing.imp.round[}\FunctionTok{is.na}\NormalTok{(missing)][sample}\FloatTok{.500}\NormalTok{[}\FunctionTok{is.na}\NormalTok{(missing)]}\SpecialCharTok{==}\DecValTok{0}\NormalTok{],}\AttributeTok{breaks=}\DecValTok{1000}\NormalTok{,}\AttributeTok{main=}\StringTok{"imputed homs (0)"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

Some integers are not \textless0 or \textgreater2. Round these up or down to the nearest genotype class.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{missing.imp.round[missing.imp.round}\SpecialCharTok{\textgreater{}}\DecValTok{2}\NormalTok{]}\OtherTok{\textless{}{-}}\DecValTok{2}
\NormalTok{missing.imp.round[missing.imp.round}\SpecialCharTok{\textless{}}\DecValTok{0}\NormalTok{]}\OtherTok{\textless{}{-}}\DecValTok{0}
\end{Highlighting}
\end{Shaded}

Now, compare the imputed and original class using `table':

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{imp.test}\OtherTok{\textless{}{-}}\FunctionTok{table}\NormalTok{(missing.imp.round[}\FunctionTok{is.na}\NormalTok{(missing)],sample}\FloatTok{.500}\NormalTok{[}\FunctionTok{is.na}\NormalTok{(missing)],}\AttributeTok{dnn=}\FunctionTok{c}\NormalTok{(}\StringTok{"imputed"}\NormalTok{,}\StringTok{"original"}\NormalTok{))}
\NormalTok{imp.test}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        original
## imputed     0     1     2
##       0  8321   228  1774
##       1  5649   392  6364
##       2  1774   251 10491
\end{verbatim}

A single number we can use summarise the accuracy of the imputation is the contingency chi-squared test statistic -- high values are good in this context: we want the rows (imputed) and columns (observed) to be highly dependent on each other.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{chisq.test}\NormalTok{(imp.test) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Pearson's Chi-squared test
## 
## data:  imp.test
## X-squared = 10391, df = 4, p-value < 2.2e-16
\end{verbatim}

Has the imputation done a good job? Record the chisq value on the board in the classroom for subsequent comparison with other runs.

Aside from our methods comparison, there are a couple more things that can be investigated with softImpute:

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  Round all predictions to homozygotes only.
\item
  Change rank.max
\end{enumerate}

We have observed above that most misclassifications are of true homozygotes as heterozygotes. e.g.~for the full data set:

\begin{verbatim}
            original 
imputed   0     1     2 
0       11508   135   824 
1       3303    623   3365 
2       760     141   14691 
\end{verbatim}

Suppose we refused to impute missing genotypes as heterozygotes but insisted on classifying them has homozygotes:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{missing.imp.round}\FloatTok{.2} \OtherTok{\textless{}{-}} \DecValTok{2}\SpecialCharTok{*}\FunctionTok{round}\NormalTok{(missing.imp}\SpecialCharTok{/}\DecValTok{2}\NormalTok{)}

\NormalTok{missing.imp.round}\FloatTok{.2}\NormalTok{[missing.imp.round}\FloatTok{.2}\SpecialCharTok{\textgreater{}}\DecValTok{2}\NormalTok{]}\OtherTok{\textless{}{-}}\DecValTok{2}
\NormalTok{missing.imp.round}\FloatTok{.2}\NormalTok{[missing.imp.round}\FloatTok{.2}\SpecialCharTok{\textless{}}\DecValTok{0}\NormalTok{]}\OtherTok{\textless{}{-}}\DecValTok{0}
\NormalTok{imp.test}\FloatTok{.2}\OtherTok{\textless{}{-}}\FunctionTok{table}\NormalTok{(missing.imp.round}\FloatTok{.2}\NormalTok{[}\FunctionTok{is.na}\NormalTok{(missing)],sample}\FloatTok{.500}\NormalTok{[}\FunctionTok{is.na}\NormalTok{(missing)],}\AttributeTok{dnn=}\FunctionTok{c}\NormalTok{(}\StringTok{"imputed"}\NormalTok{,}\StringTok{"original"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

The first line above forces genotypes which would be otherwise be imputed as heterozygotes into the nearest homozygous class. All the other lines of code are the same as before. The results seem better:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{imp.test}\FloatTok{.2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##        original
## imputed     0     1     2
##       0 11677   414  4119
##       2  4067   457 14510
\end{verbatim}

In practice, the wisdom of this reclassification is unclear. Carrying out the same exercise using mean imputation amounts to substituting all missing values by the most common allele, which doesn't seem very sensible.

Why did we choose a value for \texttt{max.rank} of 100? This was initially by trial and error on my part. Too low a rank and the prediction is too poor. Too high a value and there is a risk of overfitting: additional eigenvalues give a better fit to the observed data but are adding noise to the predictions.

One approach to determining an appropriate value which works on these data, at least, is as follows:

First impute the missing data using the mean. To do this we need to work out the means:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{av}\OtherTok{\textless{}{-}}\FunctionTok{colMeans}\NormalTok{(missing,}\AttributeTok{na.rm=}\NormalTok{T)}

\CommentTok{\#Then, as before: }
 
\NormalTok{missing.imp.av}\OtherTok{\textless{}{-}}\NormalTok{missing     }
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{500}\NormalTok{) \{}
\NormalTok{  missing.imp.av[}\FunctionTok{is.na}\NormalTok{(missing[,i]),i]}\OtherTok{\textless{}{-}}\NormalTok{av[i]}
\NormalTok{\}}

\CommentTok{\#Then compute the eigenvalues and plot them, individually and cumulatively: }
 
\NormalTok{svd.imp}\OtherTok{\textless{}{-}}\FunctionTok{svd}\NormalTok{(missing.imp.av)        }
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfcol=}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{),}\AttributeTok{cex=}\FloatTok{0.5}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(svd.imp}\SpecialCharTok{$}\NormalTok{d[}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{])         }
\FunctionTok{plot}\NormalTok{(}\FunctionTok{ecdf}\NormalTok{(svd.imp}\SpecialCharTok{$}\NormalTok{d[}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]))   }
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-333-1.pdf}

\texttt{svd} is the standard R function to carry out SVD. \texttt{ecdf} is a function for plotting empirical cumulative distributions. We drop the first eigenvalue from the plot ( the {[}-1{]} ) since this is very large and its vector merely predicts the mean allele frequency of each marker. We could avoid this problem by rescaling the data first, but as we are aware of the problem it is hardly necessary.

The cumulative distribution would suggest a rank of about 50 (though my results were better using 100). If you have time, try altering \texttt{rank.max}, repeat the analyses and compare results.

\hypertarget{mean-imputation}{%
\section{Mean imputation}\label{mean-imputation}}

First work out the means:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{av}\OtherTok{\textless{}{-}}\FunctionTok{colMeans}\NormalTok{(missing,}\AttributeTok{na.rm=}\NormalTok{T)   }
\end{Highlighting}
\end{Shaded}

For each missing value, insert the mean allele frequency:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{missing.imp.av}\OtherTok{\textless{}{-}}\NormalTok{missing }
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{500}\NormalTok{) \{}
\NormalTok{  missing.imp.av[}\FunctionTok{is.na}\NormalTok{(missing[,i]),i]}\OtherTok{\textless{}{-}}\NormalTok{av[i]   }
\NormalTok{\} }
\end{Highlighting}
\end{Shaded}

Round up or down as before and tabulate

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{missing.imp.av}\OtherTok{\textless{}{-}}\FunctionTok{round}\NormalTok{(missing.imp.av)}
\NormalTok{missing.imp.av[missing.imp.av}\SpecialCharTok{\textgreater{}}\DecValTok{2}\NormalTok{]}\OtherTok{\textless{}{-}}\DecValTok{2}
\NormalTok{missing.imp.av[missing.imp.av}\SpecialCharTok{\textless{}}\DecValTok{0}\NormalTok{]}\OtherTok{\textless{}{-}}\DecValTok{0}

\NormalTok{imp.test.av}\OtherTok{\textless{}{-}}\FunctionTok{table}\NormalTok{(missing.imp.av[}\FunctionTok{is.na}\NormalTok{(missing)],sample}\FloatTok{.500}\NormalTok{[}\FunctionTok{is.na}\NormalTok{(missing)],}
                   \AttributeTok{dnn=}\FunctionTok{c}\NormalTok{(}\StringTok{"imputed"}\NormalTok{,}\StringTok{"       original"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Inspect the results:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{imp.test.av}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##               original
## imputed     0     1     2
##       0  6588   137  1005
##       1  7425   500  7595
##       2  1731   234 10029
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{chisq.test}\NormalTok{(imp.test.av)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Pearson's Chi-squared test
## 
## data:  imp.test.av
## X-squared = 9915, df = 4, p-value < 2.2e-16
\end{verbatim}

What do you think? We'll collate the results over repeat runs and different values of LD threshold and discuss.

\hypertarget{random-forest-imputation}{%
\section{Random Forest Imputation}\label{random-forest-imputation}}

We shall also look at the random forest, as implemented in the R package \texttt{missForest}. This gave the most accurate results in a recent review \citet{Rutkoski_2013}. However, for accurate results it is slow to run.

Work on the same dataset as you used for svd and mean imputation.

\texttt{missForest} can impute missing factors as well as missing numerical values, which ought to be better for marker data, since we will no longer have numerical values which need to be rounded up or down to be \{0, 1, 2\}. When I tested this, this was indeed the case, though the analysis took two hours compared to twenty minutes. Nevertheless, we'll work on factors, but shall reduce the running time for demonstration purposes.

First we therefore need to change our numeric data to factor:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{missing.factor}\OtherTok{\textless{}{-}}\NormalTok{missing}
\NormalTok{missing.factor[]}\OtherTok{\textless{}{-}}\FunctionTok{lapply}\NormalTok{(missing.factor,factor)}
\end{Highlighting}
\end{Shaded}

Apply applies a function, here \texttt{factor}, across all the elements of the data frame. Using the \texttt{{[}{]}} at the end of \texttt{missing.factor} ensures the data frame structure is maintained. It runs quicker than a loop doing the same thing.

The Random Forest method takes bootstrap samples of the original data. In \texttt{missForest}, the default number of bootstrap samples is 100. In the interests of speed we'll just take 10 (\texttt{ntree=10}). If you have the computing power, missForest can be run in parallel. In addition to bootstrapping the data, the random forest algorithm selects a random sample of variables for each split of the tree. This prevents the bootstrapped trees from being too similar to each other. The recommended default number to be selected is √(no. of variables). Reducing the number will make the algorithm run faster. Increasing it may result in overfitting. The option mtry can be used to increase or reduce the number of selected variables. It is more important for the number of bootstrap samples to be large enough. Ten is too small: many lines will never be included in the analysis. There is no harm in having a very large number of bootstraps, but once a limit is reached there is no improvement either.

Install and then load the package in your R:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(missForest) }\CommentTok{\#you will also need to install}
\end{Highlighting}
\end{Shaded}

Running an analysis with 10 bootstraps only to save time:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#this took 2 minutes on my laptop}
\NormalTok{missing.imp.forest}\OtherTok{\textless{}{-}}\FunctionTok{missForest}\NormalTok{(missing.factor,}\AttributeTok{ntree=}\DecValTok{10}\NormalTok{,}\AttributeTok{mtry=}\DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   missForest iteration 1 in progress...done!
##   missForest iteration 2 in progress...done!
\end{verbatim}

We can also estimate imputation error using the function \texttt{mixError}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mixError}\NormalTok{(missing.imp.forest}\SpecialCharTok{$}\NormalTok{ximp,missing,sample}\FloatTok{.500}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       PFC 
## 0.2504824
\end{verbatim}

for mixError, values close to 0 are good, close to 1 are bad.

We compare across methods using our earlier code, summarising the results:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{chi.forest}\OtherTok{\textless{}{-}}\FunctionTok{table}\NormalTok{(missing.imp.forest}\SpecialCharTok{$}\NormalTok{ximp[}\FunctionTok{is.na}\NormalTok{(missing)],sample}\FloatTok{.500}\NormalTok{[}\FunctionTok{is.na}\NormalTok{(missing)],}\AttributeTok{dnn=}\FunctionTok{c}\NormalTok{(}\StringTok{"imputed"}\NormalTok{,}\StringTok{"       original"}\NormalTok{))}
\NormalTok{chi.forest}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##               original
## imputed     0     1     2
##       0 11725   406  3933
##       1    24    10    15
##       2  3995   455 14681
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{chisq.test}\NormalTok{(chi.forest) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in chisq.test(chi.forest): Chi-squared approximation may be incorrect
\end{verbatim}

\begin{verbatim}
## 
##  Pearson's Chi-squared test
## 
## data:  chi.forest
## X-squared = 9887.8, df = 4, p-value < 2.2e-16
\end{verbatim}

How does this compare with what we had before? Are there any differences in the pattern of assignment to different classes?

\hypertarget{summary-1}{%
\section{Summary}\label{summary-1}}

\begin{rmdquiz}
Now we have collected all the data, we can compare chi-squared values across methods and across different LD values. Is one method generally better, or do different methods work better with different amounts of LD in the data set? Do some methods perform more strongly as LD decreases?
\end{rmdquiz}

\hypertarget{conclusions-and-comments}{%
\section{Conclusions and comments}\label{conclusions-and-comments}}

Before adopting any method of imputation, test it against simply substituting the mean (it could do worse).

For many analyses, substitution of the mean allele frequency is probably adequate. This applies particularly to kinship matrices.

With a new dataset, it is worth experimenting by deleting data to see how well an imputation method works.

You need a lot of markers for imputation. Therefore impute before you delete highly correlated markers, not after. i.e.~thin markers after imputation.

Just because you've run an imputation programme, it doesn't mean the imputed values are accurate.

Problems surrounding imputation are going to become more important in the future as datasets continue to get bigger and methods such as genotyping by sequencing, become more common.

Random forest looks good but is slow: run overnight or in parallel. Random forest approaches are now becoming widely used in genetics in general, e.g.~to predict trait values. Try the R package \texttt{randomForest}.

\hypertarget{Association-mapping}{%
\chapter{Association Mapping.}\label{Association-mapping}}

\hypertarget{introduction-1}{%
\section{Introduction}\label{introduction-1}}

\emph{Association Mapping with GWASpoly}

We are going to use the R package \texttt{GWASpoly} to carry out association mapping using the mixed model. We shall analyse the \href{http://www.triticeaegenome.eu}{TriticeaeGenome data set} which we have used previously in this course. To recap: it was a European collaborative project which developed a panel of 384 UK, French and German winter wheat varieties. Here we are going to use the variety means across trials for a range of phenotypes together with a genome-wide set of DArT markers for association mapping.

\texttt{GWASpoly} is an R package from the Endelman group in the University of Wisconsin. The group is more noted for the well-established package \texttt{rrBLUP} for genomic prediction. You can see the group's software on their \href{http://potatobreeding.cals.wisc.edu/software/}{website}. The installation process for GWASpoly has recently changed. You use to be able to download from their website. However, the package is now hosted on \href{https://github.com/jendelman/GWASpoly}{GitHub}. If you carry on working with \texttt{R} you will find that a number of packages are hosted on GitHub. The only real difference between a package hosted on Github and through CRAN that we need to worry about for now is the installation process. \texttt{install.packages()} will not work in this case. Instead we will use the \texttt{devtools} package to install from Github:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#install devtools}
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"devtools"}\NormalTok{) }
\CommentTok{\#use devtools function to install from Github}
\CommentTok{\# you may be asked to update some packages, just follow the promts in the console}
\NormalTok{devtools}\SpecialCharTok{::}\FunctionTok{install\_github}\NormalTok{(}\StringTok{"jendelman/GWASpoly"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

That should of installed \texttt{GWASpoly} and all the required associated packages. We should load the package and test it's installed ok (there should be no error messages):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(GWASpoly)}
\end{Highlighting}
\end{Shaded}

GWASpoly is relatively straightforward to use. Its unique selling point is that it works on autopolyploid crops too. For these, the form of the kinship matrix changes, and this is accounted for. The main weakness of \texttt{GWASpoly} is that is can only handle bi-allelic markers, thus making it less useful for haplotype analysis. Several other GWAS packages are available for use in plants, notably \href{https://www.maizegenetics.net/tassel,\%20which\%20can\%20deal\%20with\%20multiple\%20alleles}{TASSEL} and GAPIT (\url{http://www.zzlab.net/GAPIT/}), both originally developed in the Buckler lab at Cornell. More recently, the authors of GAPIT have released an improvement over GAPIT called \href{http://www.zzlab.net/FarmCPU}{FarmCPU}, which incorporates recent methodological advances to reduce the number of false negatives which occur as a result of the adjustment for kinship and population structure. \texttt{GAPIT} and \texttt{FarmCPU} are both R packages.

For \texttt{GWASpoly}, there is a vignette available \href{www.jendelman.github.io/GWASpoly/GWASpoly.html}{here}, which is worth reading at some stage.

\hypertarget{preparing-the-data-set}{%
\section{Preparing the data set}\label{preparing-the-data-set}}

The complete TG data set is found in the file ``TG data for GWAS.poly.xlsx''. For reading into \texttt{GWASpoly}, two data files are required: a phenotype file and a genotype file which includes map co-ordinates. Here, phenotype data are in the file ``TG\_phens.csv''. The first column must be the accession names. The next set of columns contains data on the traits to be analysed. There is then an optional final set of columns which gives covariates to be included in the analysis. In the TriticeaeGenome data there are six covariates:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  PCR marker for the Rht2 dwarfing allele
\item
  PCR marker for the Ppd flowering time insensitivity allele
\item
  Year of registration of the variety
\item
  Country of variety origin (DEU, FRA, GBR)
\item
  2 columns for country 0/1 ``DEU'', ``FRA''
\end{enumerate}

Genotype data for the GWAS are in the file ``TGgeno.csv''. Data here are DArT markers, coded 0/1(this is a data set of inbred lines). The data are transposed compared to the phenotypic data: variety names are in the first row rather than the first column. The first column here is the marker name, the second the chromosome number and the third the position of the marker in cM. As this is wheat, the chromosomes are numbered consecutively 1\ldots21 for chromosomes 1A, 1B, 1D,2A, 2B\ldots, 7B, 7D , Chromosome 22 is of SNPs provided by BioGemma. Chromosome 23 is of unmapped DArT markers, and chromosome 24 is a small number of diagnostic markers for known QTL. Subsequent columns contain the marker data.

In preparing the data for analysis, markers with a minor allele frequency (maf) \textless4\% were deleted. Most association mapping studies delete loci with low maf as they are unlikely, on their own, to detect any association, even if they are functional. The threshold is somewhat subjective. In the current case, \textless4\% means less than 15 copies of the rare allele, which seems reasonable: think of doing a t-test where one group has fewer than 15 samples. The package itself, however, sets the minimum maf for inclusion in the analysis to 0.05, so we shall work with that. The maf threshold can be changed, however. Markers with \textgreater10\% missing data were also deleted, as they are likely to be of poor quality.

The sheet ``TGkin'' contains a subset of the markers to use for kinship calculation. Markers are frequently unevenly distributed across plant genomes. If kinship matrices are dominated by dense clusters of markers, then any operations upon these matrices, such as PCO and tree construction, will be biased towards these markers. So a tree, for example, will not necessarily reflect average relationships over the genome, but the relationships among varieties in the genomic regions which have the highest number of markers. This is particularly a problem if, as is often the case, the dense clusters of markers are also in high LD. Such clusters are frequently found in crop species where non-recombining genomic regions from related species have been introgressed by plant breeders. For this reason, it is good practice to thin the markers to give a more uniform distribution over the whole genome. One way of doing this is to calculate the correlation coefficient among all markers and delete one marker of each pair in high LD (say r\textgreater0.9). Another is to delete markers if they are too closely linked. There is a fine balancing act in thinning markers. If markers are not uniformly distributed over the genome, then marker-trait associations will be over-corrected in some parts of the genome and under-corrected in others. If too few markers are left, then kinship will not be accurately estimated and adjusted for. For our data, we shall use only the mapped markers for kinship estimation, deleting all but one of any markers which map to the same position. This left 1,044 markers (from 2686 before thinning) selected for kinship calculation.

For some reason, no packages we are aware of provide this option of thinning markers selectively by correlation coefficient or by map position, despite the necessity of doing so. There are exceptions: \citet{speed2012} developed an approach to calculating a kinship matrix which takes into account the clustering of markers without the need for thinning markers.

\hypertarget{running-gwaspoly}{%
\section{Running GWASpoly}\label{running-gwaspoly}}

\begin{rmdnote}
A script has been provided with this tutorial. You can access it from the data folder for this Chapter. Open the script, load the GWASpoly package and set the working directory to the directory containing the tutorial data. You can also copy from the tutorial as we've done in the other tutorials.
\end{rmdnote}

The command to read in the data is \texttt{read.GWASpoly()}. Various data formats are available. Markers can be coded in AB format (common in linkage analysis), as ACGT nucleotides, or numeric; 0, 1, 2, 3, 4 for a tetraploid for instance. As this is an R package, missing data are coded as \texttt{NA}. GWASpoly only works with SNPs or binary markers. To analyse SSRs or haplotype data, each allele could be tested for association independently, possibly with some amalgamation of the results at the end, but this is an unnecessary complication for demonstration purposes. If you have a haplotype data set, using TASSEL is a better option.

Read in the data as follows. In my current working directory I have a subfolder called data, where all the data files are kept. Therefore, I start each file name with ``data/'', you will need to take this out if you copy the below:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TGall}\OtherTok{\textless{}{-}}\FunctionTok{read.GWASpoly}\NormalTok{(}\AttributeTok{ploidy=}\DecValTok{1}\NormalTok{,}\StringTok{"data/TGphens.csv"}\NormalTok{,}\StringTok{"data/TGgeno.csv"}\NormalTok{,}\StringTok{"numeric"}\NormalTok{,}\AttributeTok{n.traits=}\DecValTok{18}\NormalTok{,}\AttributeTok{delim=}\StringTok{","}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Number of polymorphic markers: 2686 
## Missing marker data imputed with population mode 
## N = 376 individuals with phenotypic and genotypic information 
## Detected following fixed effects:
## Rht2_400
## PpdD1_297
## YEAR
## COUNTRY
## FRA
## DEU
## Detected following traits:
## YLD_ALL_BLUP
## CALLOW_2011
## FRANCE_2010
## FRANCE_2011
## LGE_2010
## LGE_2011
## NIAB_2011
## FT
## HT
## AWNS
## EARS_M2
## LODG
## MAT
## PROT
## SP_WT
## TGW
## TILLERS
## WINT_KILL
\end{verbatim}

Once again, we are treating inbred wheat lines as haploids here as also reflected in the numerically coded data, where we have 0 or 1 copy of an allele. Note that we had to declare the number of traits. The extra columns in the input file will be considered (correctly) as covariates in the analysis by GWASpoly. Finally we have declared the field separator to be a `,' as we are reading a csv file. For other options use \texttt{help}.

On reading in the data you will see a summary of the data structure, now all stored in \texttt{TGall}. Notice that the output says \texttt{Missing\ marker\ data\ imputed\ with\ population\ mode}. We had some missing marker data but GWASpoly requires complete data so has imputed the missing values on data input using ``population mode''. This substitutes missing data by the most frequent genotype class at each marker. This is not the best method. Ideally, we would impute missing data by a more accurate method prior to analysis (see the imputation tutorial).

We also need to read in the version of the data with just the markers required for kinship calculation.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#take out the "data/" if you copy}
\NormalTok{TGkin}\OtherTok{\textless{}{-}}\FunctionTok{read.GWASpoly}\NormalTok{(}\AttributeTok{ploidy=}\DecValTok{1}\NormalTok{,}\StringTok{"data/TGphens.csv"}\NormalTok{,}\StringTok{"data/TGkin.csv"}\NormalTok{,}\StringTok{"numeric"}\NormalTok{,}\AttributeTok{n.traits=}\DecValTok{18}\NormalTok{,}\AttributeTok{delim=}\StringTok{","}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Note, we have 1044 markers in the kinship dataset.

\hypertarget{the-exercise-2}{%
\section{The exercise}\label{the-exercise-2}}

The aim of the exercise is to familiarise yourselves with the package and compare the results obtained when:

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  Using different kinship adjustments, or without including kinship at all
\item
  Including known trait covariates (e.g.~Rht, Ppd loci)
\item
  Using different population structure adjustments, or without including population structure at all.
\end{enumerate}

\hypertarget{calculation-and-comparison-of-kinship}{%
\subsection{Calculation and comparison of kinship}\label{calculation-and-comparison-of-kinship}}

To estimate kinship in GWASpoly, we simply type:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TGall}\OtherTok{\textless{}{-}}\FunctionTok{set.K}\NormalTok{(TGall)  }
\NormalTok{TGkin}\OtherTok{\textless{}{-}}\FunctionTok{set.K}\NormalTok{(TGkin) }
\end{Highlighting}
\end{Shaded}

Each version of the data has now had a kinship matrix calculated from its associated marker set. We'll see later how to specify different kinship matrices to be used. There are multiple forms of kinship matrices. GWASpoly centres each marker (so the mean of each marker is zero and the variance is one). If the centred data are in a matrix M, the kinship matrix is then MMt. The kinship matrix is also scaled so that the mean of the diagonal elements are one. This is the VanRaden kinship matrix \citep{vanraden_efficient_2008}, which is in common use. Alternatively, you can supply your own matrix (we'll see how shortly). MtM/n would be the variance/covariance matrix among the n markers if M was centred, and would be the correlation matrix if each marker was initially adjusted to have a variance of one too.

The kinship matrix is stored in a `slot' in the GWASpoly object storing the data. Slots are accessed using \texttt{@} just as named vectors of data accessed using \texttt{\$} Thus:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TGall}\SpecialCharTok{@}\NormalTok{K[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##               AARDEN    AARDVARK       ABELE         ABO     ACCESS
## AARDEN    0.89425094  0.06820056 -0.08325033 -0.17401122  0.1904552
## AARDVARK  0.06820056  0.98608975  0.04025471 -0.03429426 -0.0102782
## ABELE    -0.08325033  0.04025471  1.16491365 -0.03983785  0.1856692
## ABO      -0.17401122 -0.03429426 -0.03983785  1.30763030 -0.1575344
## ACCESS    0.19045520 -0.01027820  0.18566924 -0.15753443  1.0314240
\end{verbatim}

This shows the top left hand corner of the matrix made from all markers. Compare to \texttt{TGkin}.

These kinship matrices are essentially measuring the genetic distance between varieties using different marker sets. So, as you now know from the Population Genetics exercise, we can now visualise the genetic distances in a PCoA -- principal coordinate analysis. The standard R function for spectral decomposition of a matrix is \texttt{eigen()}, which produces eigenvalues and vectors. We can plot the eigenvalues for both data sets:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(}\FunctionTok{eigen}\NormalTok{(TGall}\SpecialCharTok{@}\NormalTok{K)}\SpecialCharTok{$}\NormalTok{values)          }
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-351-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(}\FunctionTok{eigen}\NormalTok{(TGkin}\SpecialCharTok{@}\NormalTok{K)}\SpecialCharTok{$}\NormalTok{values)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-351-2.pdf}

You should see that in both cases, the first few eigenvalues are much larger than the remainder: they therefore account for most of the variation in their respective matrices. We can now plot the largest and second largest eigenvectors against each other for both data sets:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(}\FunctionTok{eigen}\NormalTok{(TGall}\SpecialCharTok{@}\NormalTok{K)}\SpecialCharTok{$}\NormalTok{vectors[,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{]) }
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-352-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(}\FunctionTok{eigen}\NormalTok{(TGkin}\SpecialCharTok{@}\NormalTok{K)}\SpecialCharTok{$}\NormalTok{vectors[,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{]) }
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-352-2.pdf}

\begin{rmdquiz}
\textbf{1}

The plots for all the data, and for the thinned markers alone are very different. What is the explanation for this? On the basis of this, which kinship matrix is most representative of relationships over the whole genome?
\end{rmdquiz}

We now want to make two data sets for the GWAS analysis. The GWAS analysis itself will be conducted with all the genotype data, but we want one data set with kinship calculated from all markers:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TGanalysisUnthinned}\OtherTok{\textless{}{-}}\NormalTok{TGall }
\end{Highlighting}
\end{Shaded}

\ldots{} and one data set with kinship calculated from only the thinned markers. This is remarkably easy to create: we just substitute in the kinship matrix calculate on the thinned markers:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TGanalysisThinned}\OtherTok{\textless{}{-}}\NormalTok{TGall}
\NormalTok{TGanalysisThinned}\SpecialCharTok{@}\NormalTok{K}\OtherTok{\textless{}{-}}\NormalTok{TGkin}\SpecialCharTok{@}\NormalTok{K }
\end{Highlighting}
\end{Shaded}

Then check:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TGanalysisUnthinned}\SpecialCharTok{@}\NormalTok{K[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{] }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##               AARDEN    AARDVARK       ABELE         ABO     ACCESS
## AARDEN    0.89425094  0.06820056 -0.08325033 -0.17401122  0.1904552
## AARDVARK  0.06820056  0.98608975  0.04025471 -0.03429426 -0.0102782
## ABELE    -0.08325033  0.04025471  1.16491365 -0.03983785  0.1856692
## ABO      -0.17401122 -0.03429426 -0.03983785  1.30763030 -0.1575344
## ACCESS    0.19045520 -0.01027820  0.18566924 -0.15753443  1.0314240
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TGanalysisThinned}\SpecialCharTok{@}\NormalTok{K[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{] }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##               AARDEN    AARDVARK       ABELE         ABO      ACCESS
## AARDEN    0.94938532  0.07464131 -0.07579459 -0.17446488  0.24676194
## AARDVARK  0.07464131  0.92078798  0.07606171 -0.03447680  0.03070368
## ABELE    -0.07579459  0.07606171  1.04717181  0.02871512  0.08796148
## ABO      -0.17446488 -0.03447680  0.02871512  1.30082327 -0.16499556
## ACCESS    0.24676194  0.03070368  0.08796148 -0.16499556  1.04546733
\end{verbatim}

\hypertarget{gwas}{%
\subsection{GWAS}\label{gwas}}

Strictly, in using the mixed model for GWAS, genetic variances and covariances should be estimated separately for every marker, since the estimates will vary depending on the magnitude of the SNP effect and the distribution of the marker alleles over individuals. However, given that most markers have very little effect, the variances change very little.
Therefore GWASpoly follows an approximate approach outlined by \citet{kang_variance_2010} and \citet{zhang_mixed_2010} in which the variances are estimated only once, in a model without any SNP effect. This greatly speeds up the analysis with little loss of accuracy or bias. To analyse all traits for the thinned data set:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TG.GWAS.Thinned}\OtherTok{\textless{}{-}}\FunctionTok{GWASpoly}\NormalTok{(TGanalysisThinned,}\AttributeTok{models=}\StringTok{"additive"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

A choice must be made of what model of marker effects to fit. For pseudo-haploid data, as here, the choice makes no difference. In general a simple additive model is best. Other options (see \texttt{help(GWASpoly()} ) are targeted at analyses with polyploids, where the multiple heterozygous classes can be pooled in various ways. \texttt{models\ ="general"} would fit separate additive and a dominance terms in a diploid species.

You can specify multiple models in the same analysis. More usefully for us, you can restrict the analysis to certain traits only. We will now focus on analysing only two traits, height and flowering time:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TG.HT.FT.Thinned}\OtherTok{\textless{}{-}}\FunctionTok{GWASpoly}\NormalTok{(TGanalysisThinned, }\AttributeTok{models=}\StringTok{"additive"}\NormalTok{, }\AttributeTok{traits=}\FunctionTok{c}\NormalTok{(}\StringTok{"HT"}\NormalTok{,}\StringTok{"FT"}\NormalTok{)) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Analyzing trait: HT 
## P3D approach: Estimating variance components...Completed 
## Testing markers for model: additive 
## Analyzing trait: FT 
## P3D approach: Estimating variance components...Completed 
## Testing markers for model: additive
\end{verbatim}

We now need to look at QQ plots:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qq.plot}\NormalTok{(TG.HT.FT.Thinned,}\AttributeTok{trait=}\StringTok{"FT"}\NormalTok{,}\AttributeTok{model=}\StringTok{"additive"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-359-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qq.plot}\NormalTok{(TG.HT.FT.Thinned,}\AttributeTok{trait=}\StringTok{"HT"}\NormalTok{,}\AttributeTok{model=}\StringTok{"additive"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-359-2.pdf}

These look OK. If the lower values did not fall close to the line, we would need to reassess our corrections for kinship and/or take into account other covariates (such as population structure, as below).

What QTL have we found? First we need to specify a significance threshold:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TG.HT.FT.Thinned.Bonf}\OtherTok{\textless{}{-}}\FunctionTok{set.threshold}\NormalTok{(TG.HT.FT.Thinned,}\AttributeTok{method=}\StringTok{"Bonferroni"}\NormalTok{,}\AttributeTok{level=}\FloatTok{0.05}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Thresholds
##    additive
## HT     4.71
## FT     4.71
\end{verbatim}

You have three choices for setting the threshold: \texttt{“Bonferroni”}, \texttt{“FDR”} (False Discovery Rate) and \texttt{“permute”} (permutation test). The permutation test is relatively quick but to do the default 1000 permutations would take too long for this exercise! I will show the results from this in the class exercise. For now, you can run the \texttt{"FDR"} threshold for comparison:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TG.HT.FT.Thinned.fdr}\OtherTok{\textless{}{-}}\FunctionTok{set.threshold}\NormalTok{(TG.HT.FT.Thinned,}\AttributeTok{method=}\StringTok{"FDR"}\NormalTok{,}\AttributeTok{level=}\FloatTok{0.05}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Thresholds
##    additive
## HT     3.47
## FT     3.73
\end{verbatim}

Having set the threshold, you will find it is plotted on any Manhattan plots you create provided there is something that exceeds the threshold. We can plot Manhattan plots from our analyses as:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{manhattan.plot}\NormalTok{(TG.HT.FT.Thinned.Bonf,}\AttributeTok{trait=}\FunctionTok{c}\NormalTok{(}\StringTok{"HT"}\NormalTok{),}\AttributeTok{model=}\StringTok{"additive"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-362-1.pdf}

You must specify a single trait and a single model.

We can examine the significant results directly:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{get.QTL}\NormalTok{(TG.HT.FT.Thinned.Bonf)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Trait    Model Threshold    Marker Chrom Position Ref Alt Score    Effect
## 1363    HT additive      4.71 DArT_1448    16  48.7860   0   1  5.72  4.87e+00
## 1364    HT additive      4.71 DArT_1449    16  48.7860   0   1  5.72  4.87e+00
## 1365    HT additive      4.71 DArT_1451    16  49.1736   0   1  5.27  4.70e+00
## 1366    HT additive      4.71 DArT_1452    16  49.1736   0   1  5.72  4.87e+00
## 1367    HT additive      4.71 DArT_1453    16  49.1736   0   1  5.72  4.87e+00
## 2042    HT additive      4.71   SNP_168    22 167.0000   0   1  6.03  3.32e+00
## 2150    HT additive      4.71   SNP_278    22 277.0000   0   1  5.00 -3.03e+00
## 2253    HT additive      4.71 DArT_2072    23  59.0000   0   1  5.41 -3.18e+00
## 2684    HT additive      4.71  Rht2_400    24   9.0000   0   1 12.72 -5.51e+00
## 654     FT additive      4.71  DArT_678     6  84.5399   0   1  5.18  1.66e+00
## 2065    FT additive      4.71   SNP_191    22 190.0000   0   1 12.42 -4.22e+00
## 2682    FT additive      4.71 PpdD1_297    24   7.0000   0   1 40.74 -4.81e+00
\end{verbatim}

``Score'' is the --log10(p-value) from the analyses, to be compared with the Threshold -- which is also a --log10(p-value). ``Ref and ``Alt'' give the reference allele and the alternative allele. ``Effect'' gives the gene effect for the alternative allele. In the example, The alternative allele (1) for PpdD1\_297 has an effect of -4.81 on flowering time. (i.e.~it reduces flowering time by nearly 5 days). We can confirm this by extracting and plotting individual trait -- marker combinations:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{boxplot}\NormalTok{(TG.HT.FT.Thinned.Bonf}\SpecialCharTok{@}\NormalTok{pheno}\SpecialCharTok{$}\NormalTok{FT}\SpecialCharTok{\textasciitilde{}}\FunctionTok{as.data.frame}\NormalTok{(TG.HT.FT.Thinned}\SpecialCharTok{@}\NormalTok{geno)}\SpecialCharTok{$}\NormalTok{PpdD1\_297,}\AttributeTok{varwidth=}\NormalTok{T) }
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-364-1.pdf}

Similarly, the top hit for height is the unmapped Rht2 gene (one of the `green revolution' dwarfing loci), where the reduced height allele has an effect size of about 5.5 cm relative to the reference allele.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{boxplot}\NormalTok{(TG.HT.FT.Thinned.Bonf}\SpecialCharTok{@}\NormalTok{pheno}\SpecialCharTok{$}\NormalTok{HT}\SpecialCharTok{\textasciitilde{}}\FunctionTok{as.data.frame}\NormalTok{(TG.HT.FT.Thinned}\SpecialCharTok{@}\NormalTok{geno)}\SpecialCharTok{$}\NormalTok{Rht2\_400,}\AttributeTok{varwidth=}\NormalTok{T)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-365-1.pdf}

\begin{rmdquiz}
\textbf{2}

a). Compare the results for ``HT'' and ``FT'' between the `Bonferroni' and `FDR', and, if possible, `permutation' thresholds. What do you conclude?

b). Rerun the analysis using the Non-thinned data. What differences might you expect to see? What do you observe and what are the implications?
\end{rmdquiz}

We will now run the analysis unadjusted for kinship. The unadjusted analysis is simply linear regression, or a t-test, on each marker in turn. We could do this by putting \texttt{lm()} or \texttt{t.test()} inside a loop in R. If you have time, this is a good exercise to try. However, we can get the same result directly from GWASpoly by analysing the data with a kinship matrix which has `1' down the leading diagonal and `0' everywhere else -- all the lines are unrelated. Strictly, it isn't that the lines are unrelated, it is that they have equal relationships: we should get the same answer if you used, say, 0.25, for the off- diagonal elements (another good exercise to try).

First take a copy of the data and then create the diagonal matrix:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TGdiag}\OtherTok{\textless{}{-}}\NormalTok{TGanalysisThinned}
\CommentTok{\#There is a command which will create diagonal matrices in R: diag().  }
\NormalTok{TGdiag}\SpecialCharTok{@}\NormalTok{K }\OtherTok{\textless{}{-}} \FunctionTok{diag}\NormalTok{(}\DecValTok{1}\NormalTok{,}\FunctionTok{length}\NormalTok{(}\FunctionTok{rownames}\NormalTok{(TGanalysisThinned}\SpecialCharTok{@}\NormalTok{geno)))}
\end{Highlighting}
\end{Shaded}

We've also picked up the required dimensions of the kinship matrix using \texttt{length(rownames\ (....))}. The new kinship matrix must also have row names and column names which match the names of the original matrix:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rownames}\NormalTok{(TGdiag}\SpecialCharTok{@}\NormalTok{K)}\OtherTok{\textless{}{-}}\NormalTok{(}\FunctionTok{rownames}\NormalTok{(TGanalysisThinned}\SpecialCharTok{@}\NormalTok{geno))}
\FunctionTok{colnames}\NormalTok{(TGdiag}\SpecialCharTok{@}\NormalTok{K)}\OtherTok{\textless{}{-}}\NormalTok{(}\FunctionTok{rownames}\NormalTok{(TGanalysisThinned}\SpecialCharTok{@}\NormalTok{geno))}
\end{Highlighting}
\end{Shaded}

Check all looks ok:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TGdiag}\SpecialCharTok{@}\NormalTok{K[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          AARDEN AARDVARK ABELE ABO ACCESS
## AARDEN        1        0     0   0      0
## AARDVARK      0        1     0   0      0
## ABELE         0        0     1   0      0
## ABO           0        0     0   1      0
## ACCESS        0        0     0   0      1
\end{verbatim}

And run the analyses, just as before:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TGdiag }\OtherTok{\textless{}{-}}\FunctionTok{GWASpoly}\NormalTok{(TGdiag,}\AttributeTok{models=}\StringTok{"additive"}\NormalTok{) }
\NormalTok{TGdiag.fdr }\OtherTok{\textless{}{-}}\FunctionTok{set.threshold}\NormalTok{(TGdiag,}\AttributeTok{method=}\StringTok{"FDR"}\NormalTok{,}\AttributeTok{level=}\FloatTok{0.05}\NormalTok{) }
\FunctionTok{get.QTL}\NormalTok{(TGdiag.fdr) }
\FunctionTok{manhattan.plot}\NormalTok{(TGdiag.fdr,}\AttributeTok{trait=}\FunctionTok{c}\NormalTok{(}\StringTok{"HT"}\NormalTok{),}\AttributeTok{model=}\StringTok{"additive"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{rmdquiz}
\textbf{3}

How do the results compare with the results of the mixed model which we ran previously? Are there many significant results arising from not considering kinship? Was the mixed model analysis necessary?
\end{rmdquiz}

\hypertarget{analysis-with-covariates}{%
\subsection{Analysis with covariates}\label{analysis-with-covariates}}

For height, in the unadjusted results, you should find Rht2 is the most significant marker and has the largest effect. It is the most important dwarfing gene employed in Europe, so this is not surprising. In the adjusted results, you will still find Rht2 on top, but the p-value is much reduced, as is the estimate of its effect. Height is a major contributor to population structure in wheat: German varieties tend to rely less on dwarfing genes for example and so are taller. Adjusting for kinship has the effect of leaving less variation to be accounted for by genuine marker relationships.

Perhaps Rht2 is dominating the results. If we include Rht2 as a covariate in the analysis, will other QTL for reduced height be revealed? For this we need to include Rht2 in the analysis as a covariate. Recall that the phenotypic data input included some additional columns, to be used as covariates. These have been stored in a slot: \texttt{@fixed}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(TGanalysisThinned}\SpecialCharTok{@}\NormalTok{fixed) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     Rht2_400        PpdD1_297           YEAR        COUNTRY         
##  Min.   :0.0000   Min.   :0.0000   Min.   :1946   Length:376        
##  1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:1988   Class :character  
##  Median :1.0000   Median :0.0000   Median :1997   Mode  :character  
##  Mean   :0.5455   Mean   :0.2253   Mean   :1994                     
##  3rd Qu.:1.0000   3rd Qu.:0.0000   3rd Qu.:2003                     
##  Max.   :1.0000   Max.   :1.0000   Max.   :2007                     
##       FRA              DEU        
##  Min.   :0.0000   Min.   :0.0000  
##  1st Qu.:0.0000   1st Qu.:0.0000  
##  Median :1.0000   Median :0.0000  
##  Mean   :0.5656   Mean   :0.2385  
##  3rd Qu.:1.0000   3rd Qu.:0.0000  
##  Max.   :1.0000   Max.   :1.0000
\end{verbatim}

I've included Rht2 as one of these covariates. To select other SNPs as covariates, I expect you could add or edit \texttt{TGanalysisThinned@fixed} to include the desired SNPs, but I've not tried this myself. It's another exercise in R and GWAS for you to try! To add a covariate to the mixed model, first provide their names and their data type. The covariates can be numeric or factors, such as country of origin.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Rht2}\OtherTok{\textless{}{-}} \FunctionTok{set.params}\NormalTok{(}\AttributeTok{fixed=}\FunctionTok{c}\NormalTok{(}\StringTok{"Rht2\_400"}\NormalTok{),}\AttributeTok{fixed.type=}\FunctionTok{c}\NormalTok{(}\StringTok{"numeric"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

We could include multiple covariates in the analysis (by using \texttt{c(cov1,cov2...)}. In the interests of speed, we'll just use Rht2:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TG.HT.Rht2}\OtherTok{\textless{}{-}}\FunctionTok{GWASpoly}\NormalTok{(TGanalysisThinned,}\AttributeTok{models=}\StringTok{"additive"}\NormalTok{,}\AttributeTok{trait=}\FunctionTok{c}\NormalTok{(}\StringTok{"HT"}\NormalTok{),}\AttributeTok{params=}\NormalTok{Rht2)}
\NormalTok{TG.HT.Rht2 }\OtherTok{\textless{}{-}}\FunctionTok{set.threshold}\NormalTok{(TG.HT.Rht2,}\AttributeTok{method=}\StringTok{"FDR"}\NormalTok{,}\AttributeTok{level=}\FloatTok{0.05}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now compare the results on your own PC:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{get.QTL}\NormalTok{(TG.HT.Rht2,}\AttributeTok{traits=}\FunctionTok{c}\NormalTok{(}\StringTok{"HT"}\NormalTok{))}
\FunctionTok{get.QTL}\NormalTok{(TG.HT.FT.Thinned.fdr,}\AttributeTok{traits=}\FunctionTok{c}\NormalTok{(}\StringTok{"HT"}\NormalTok{)) }
\end{Highlighting}
\end{Shaded}

\begin{rmdquiz}
\textbf{4}

What is the main difference of including a covariate? Do any hits vanish? If so, why do you think this might be? Do new hits appear? If so, why do you think they were not previously detected? (For those of you who work on wheat, PpdD1 and Rht1 should be familiar, in particular what do you think is going on with Rht1?
\end{rmdquiz}

\hypertarget{accounting-for-population-structure}{%
\subsection{Accounting for population structure}\label{accounting-for-population-structure}}

We could use as covariates the population membership as coefficients estimated in the program STRUCTURE. To do this, we would have to run STRUCTURE and import these as covariates with the rest of the trait and covariate data. Including these together with kinship coefficients is often called the ``Q + K'' analysis. Alternatively, as we have seen, for crop germplasm ``populations'' such as this one, principal coordinate analysis may be a more appropriate choice for estimating population structure. For this reason, the principal components from a PCoA may be the best choice to include as covariates. GWASpoly has options to do this automatically. Here we use the first 10 eigenvectors:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{params }\OtherTok{\textless{}{-}} \FunctionTok{set.params}\NormalTok{(}\AttributeTok{n.PC=}\DecValTok{10}\NormalTok{) }
\NormalTok{TG.PCO}\OtherTok{\textless{}{-}}\FunctionTok{GWASpoly}\NormalTok{(TGanalysisThinned,}\AttributeTok{models=}\StringTok{"additive"}\NormalTok{,}\AttributeTok{traits=}\FunctionTok{c}\NormalTok{(}\StringTok{"HT"}\NormalTok{,}\StringTok{"FT"}\NormalTok{),}\AttributeTok{params=}\NormalTok{params)}
\NormalTok{TG.PCO}\OtherTok{\textless{}{-}}\FunctionTok{set.threshold}\NormalTok{(TG.PCO,}\AttributeTok{method=}\StringTok{"Bonferroni"}\NormalTok{,}\AttributeTok{level=}\FloatTok{0.05}\NormalTok{)}
\FunctionTok{get.QTL}\NormalTok{(TG.PCO)}
\FunctionTok{get.QTL}\NormalTok{(TG.HT.FT.Thinned.fdr)}
\end{Highlighting}
\end{Shaded}

Then inspect he results:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qq.plot}\NormalTok{(TG.PCO,}\AttributeTok{trait=}\StringTok{"FT"}\NormalTok{,}\AttributeTok{model=}\StringTok{"additive"}\NormalTok{)}
\FunctionTok{qq.plot}\NormalTok{(TG.HT.FT.Thinned,}\AttributeTok{trait=}\StringTok{"FT"}\NormalTok{,}\AttributeTok{model=}\StringTok{"additive"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{rmdquiz}
\textbf{5}

Does this make any difference to the results (note the subtle difference in the qq plots as well as the qtls detected)? Do you think the results might differ for a different kind of mapping population? Do you think it is a problem that we are using the kinship matrix twice?
\end{rmdquiz}

Rather than estimate population structure from the kinship matrix, we can analyse the data with country of varietal origin as a factor. To do this,

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{country}\OtherTok{\textless{}{-}}\FunctionTok{set.params}\NormalTok{(}\AttributeTok{fixed =} \StringTok{"COUNTRY"}\NormalTok{,}\AttributeTok{fixed.type =} \StringTok{"factor"}\NormalTok{) }
\NormalTok{TG.country}\OtherTok{\textless{}{-}}\FunctionTok{GWASpoly}\NormalTok{(TGanalysisThinned,}\AttributeTok{models=}\StringTok{"additive"}\NormalTok{,}\AttributeTok{traits=}\FunctionTok{c}\NormalTok{(}\StringTok{"HT"}\NormalTok{,}\StringTok{"FT"}\NormalTok{), }\AttributeTok{params=}\NormalTok{country) }
\NormalTok{TG.country}\OtherTok{\textless{}{-}}\FunctionTok{set.threshold}\NormalTok{(TG.country,}\AttributeTok{method=}\StringTok{"FDR"}\NormalTok{,}\AttributeTok{level=}\FloatTok{0.05}\NormalTok{)}
\FunctionTok{get.QTL}\NormalTok{(TG.country) }
\FunctionTok{get.QTL}\NormalTok{(TG.HT.FT.Thinned.fdr)}
\end{Highlighting}
\end{Shaded}

For covariates to be included, there must be no missing data. A pseudo country has been added here for the three missing values. A better way of dealing with the unknown factor levels would be to include two columns for the three countries coded as FRA (1) and not-FRA (0) in the first column and DEU and notDEU in the second. If a country is not-FRA and not-DEU then it must be GBR: we have three countries but only two degrees of freedom so the two columns alone are adequate. Then the missing values in each column could be coded as 1/3: the average population membership over the three countries. This is an example of the equivalence of regression and the analysis of factors with multiple levels in the analysis of variance.

\begin{rmdquiz}
\textbf{6}

Did using countries as a covariate make any difference to the results?
\end{rmdquiz}

\hypertarget{further-analysis}{%
\subsection{Further analysis}\label{further-analysis}}

There are other things that can be done with the \texttt{set.params()} command: the minor allele frequency for inclusion in the analysis can be set. The default is 0.05. Also, the approximate method of carrying out the mixed analysis can be turned off. See \texttt{help(set.params)} for details.

We have seen that there remains some art in carrying out association analyses: what kinship matrix to use, what covariates to include and so on. The best approach is probably to simulate traits from the marker data for your data set and test analysis methods on those. In simulations on UK barley and UK wheat data we concluded that the best all-round method is the mixed model, with a kinship matrix similar to the one used here and no additional covariates to account for population structure. But this wasn't always best for all traits and datasets. If one has time, it is worth experimenting. And of course simulations are always needed to estimate the power of your analysis, and hence the data collection requirements, whichever approach you take.

\begin{rmdquiz}
\textbf{7}

How would we run simulations to estimate the power of different analysis approaches?
\end{rmdquiz}

\hypertarget{Mapping-Markers}{%
\chapter{Mapping Markers and Quantitative Trait Loci.}\label{Mapping-Markers}}

\emph{Mapping Markers and Quantitative Trait Loci with R/qtl (and ASMap)}

\hypertarget{introduction-2}{%
\section{Introduction}\label{introduction-2}}

There are several excellent mapping programmes available. A list of some, suitable for plant breeders is found on the R/qtl \href{http://www.rqtl.org/}{homepage.} Software for
creating maps of markers is less common than software for mapping QTL and most has
specific limitations in terms of speed, population types, handling of missing data and so
on. The gold standard is \href{https://www.kyazma.nl/}{Joinmap,} but this is expensive. In fact
it's so expensive that they don't put the price on their web site.

For this course we are going to use R/qtl. This is a popular, easy-to-use and well supported package and there is a useful book on QTL mapping associated with it: ``A
Guide to QTL Mapping with R/qtl.'' Details are available \href{http://www.rqtl.org/book/}{here.} There is also an R/qtl \href{http://www.rqtl.org/}{web site,} which
includes much useful information, including tutorials and there is a \href{http://groups.google.com/group/Rqtl-disc}{discussion group.} A quick tour of R/qtl can be found \href{http://www.rqtl.org/tutorials/rqtltour.pdf}{here}. R/qtl was originally written to map QTL and not to create linkage maps of markers. However, it will create marker maps, although it can be slow compared to alternative programs. However, in this tutorial we shall use R/qtl both to create a linkage map and then to map QTL onto that map.

A rewritten version, R/qtl2 is now available, which is faster and better able to handle
large data sets and complex cross-designs, including MAGIC. It also offers an automated
marker thinning option for kinship matrix estimation (hooray!) and includes options to
go beyond QTL detection to searching genomic databases for candidate SNP variants. It
is available from \href{http://kbroman.org/qtl2/}{here} but not yet on CRAN. We are currently trying it out in our work at NIAB.

Also relatively new, and available through CRAN, is the R package ASMap. This creates marker maps for basic biparental populations, but you would have to use R/qtl to map traits. The description of it is \href{https://www.jstatsoft.org/article/view/v079i06}{here}. The R manual for the package can be found \href{https://cran.r-project.org/web/packages/ASMap/ASMap.pdf}{here}. Its main benefit over R/qtl is in utilising a more advanced, much faster algorithm for clustering and marker ordering. We shall also have a brief look at this too.

\begin{rmdnote}
A R script for this tutorial has been provided with the data, either work from that or copy and paste the chunks you wish to run from the book.
\end{rmdnote}

\hypertarget{constructing-a-genetic-map}{%
\section{Constructing a genetic map}\label{constructing-a-genetic-map}}

Most (all?) statistical packages for mapping markers start with a square matrix of recombination fractions. From this two-dimensional matrix, markers are partitioned into linkage groups by some form of clustering. Hopefully each cluster or linkage group belongs uniquely to a single chromosome. However, stringent significant thresholds must be employed in this clustering to reduce the number of falsely linked markers. As a result, a numbers of markers are often left unlinked or in clusters with only a very small number of members. In practice, therefore, it is common to identify more linkage groups than chromosomes. With modern genomic resources, these linkage groups can often be subsequently linked to specific chromosomes.

Following clustering, for each linkage group in turn, markers are projected from the two-dimensional array of recombination fractions into a linear order. There are various methods for carrying out this linearization. Usually, as a final stage, the linear order is tested by some form of perturbation of the initial order to see if a new order can be found which is judged better: reducing the total number of recombinations required to create the observed set of chromosomes for example.

The data we are going to analyse are from a DH mapping population in UK winter wheat: Avalon x Cadenza. Data were produced as part of an experiment to map wheat soil borne mosaic virus.

\hypertarget{formatting-and-reading-in-the-data}{%
\section{Formatting and reading in the data}\label{formatting-and-reading-in-the-data}}

Data are in the file: ``AxC\_R.xlsx''. Note:

Row 1: in separate cells, column names for ID, phenotypes, markers.

Row 2: in the cells immediately under the marker names, the chromosome identifier for
that marker. Here that identifier is 50 for all markers (we could have used anything except x -- reserved for sex chromosomes. If we had a pre-existing map we wished to use, obviously this identifier would be for the chromosome or linkage group.

Row 3: is missing here as it is not required. But if we were using a pre-existing map, then on this line, under its marker name, would be the position in cM for that marker (see
the QTL mapping tutorial). Data are read in Haldane map units. You will see this after
we have created a map and exported our data with the new map to a file.

Coding for marker data follows that used by Mapmaker, one of the original pieces of
mapping software (slow by current standards and hard to use):

\begin{verbatim}
genotype    default coding  
AA            A               homozygous from parent A 
Aa            H               heterozygous 
aa            B               homozygous from parent B 
A_            D               for dominant markers, homozygous in parent A 
a_            C               for dominant markers, homozygous in parent B
\end{verbatim}

This is the basic coding. It has been expanded to deal with more complex cross types which R/qtl can also handle.

Should we drop any markers? Should we drop any individuals?

The dataset we are using has already been through a QC process which has removed really bad markers and individuals. This can be done without a map by looking at marker failure rates and segregation distortion -- over markers and over individuals. Some of these checks can be done by the mapping software, but they are easy to do in Excel too:

To calculate a chi-sq with 1 df to test for 1:1 segregation you can use the quick formula:

\[(A-B)^2/(A+B)\]

This is just a simplification of the usual \(Σ(O-E)^2/E\) (O= observed numbers, E = expected numbers) for a chi-sq test. It only applies when there are two classes and the expected are in the ratio 1:1. I prefer to do this prior to running any mapping programs but it is possible within R/qtl.

After you are happy with the data, then we read it into R. R cannot read these data directly from Excel or from the clipboard. First you must save the data as a csv file. This has already been done, to create the file AxC\_R.csv.

Within R Studio, set the directory and load the package \texttt{qtl}. You will need to install from CRAN if you don't have it already.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(qtl)}
\end{Highlighting}
\end{Shaded}

We can now read data in using the command \texttt{read.cross}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AxC}\OtherTok{\textless{}{-}}\FunctionTok{read.cross}\NormalTok{(}\StringTok{"csv"}\NormalTok{,}\AttributeTok{dir=}\StringTok{"data"}\NormalTok{,}\StringTok{"AxC\_R.csv"}\NormalTok{,}\AttributeTok{genotypes=}\FunctionTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{,}\StringTok{"B"}\NormalTok{),}\AttributeTok{estimate.map=}\NormalTok{F)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  --Read the following data:
##   78  individuals
##   186  markers
##   10  phenotypes
##  --Cross type: bc
\end{verbatim}

An option to note here is \texttt{estimate.map=F}. This is required to declare that we are not inputting a map with the data, You need the \texttt{dir="data"} in read.cross to declare the path to the file. However, you won't have the directory `data' on your computers. If you just want to call data from your working directory, like most of us do, just write this as \texttt{dir=""}. You will want to be working in the same directory as the data, as we have done for previous tutorials.

Take a look at the output from reading the file in. The last line indicates that the cross has been identified as a backcross from the data format: because only two marker classes were found at each locus. In fact, the cross type is a doubled haploid population. For marker and trait mapping purposes this makes absolutely no difference. Do you know why not? Nevertheless, we can change the cross to dh as below:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(AxC)[}\DecValTok{1}\NormalTok{]}\OtherTok{\textless{}{-}}\StringTok{"dh"} 
\end{Highlighting}
\end{Shaded}

If you read in a file which already has a map with some of the markers mapped to the same position you will get a warning message. To avoid problems in analysing the data, R/qtl uses the command \texttt{jittermap()} to add a tiny fraction to every interval on chromosomes where this occurs. We would need to overwrite the original data, or otherwise save the output (e.g.~\texttt{AxC\textless{}-jittermap(AxC)}) but there is no need to do that here.

\hypertarget{a-quick-map}{%
\section{A quick map!}\label{a-quick-map}}

Remember we have just read the data into the data table \texttt{AxC}. As with many R objects, the commands summary and plot give object specific details of the data contained within:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(AxC) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     Doubled haploids
## 
##     No. individuals:    78 
## 
##     No. phenotypes:     10 
##     Percent phenotyped: 100 94.9 100 100 100 100 100 100 100 100 
## 
##     No. chromosomes:    1 
##         Autosomes:      50 
## 
##     Total markers:      186 
##     No. markers:        186 
##     Percent genotyped:  96.1 
##     Genotypes (%):      AA:50.2  BB:49.8
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(AxC)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-387-1.pdf}

Before we go into detail about creating a map, we shall first do so very quickly to show that this can be done simply and with only a few commands:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{quick.map }\OtherTok{\textless{}{-}}\FunctionTok{est.rf}\NormalTok{(AxC) }
\NormalTok{quick.map }\OtherTok{\textless{}{-}}\FunctionTok{formLinkageGroups}\NormalTok{(quick.map,}\AttributeTok{reorgMarkers=}\NormalTok{T) }
\NormalTok{quick.map }\OtherTok{\textless{}{-}}\FunctionTok{orderMarkers}\NormalTok{(quick.map)}
\FunctionTok{plotMap}\NormalTok{(quick.map)  }
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-388-1.pdf}

This should produce the map shown above, which doesn't look bad, though we may wonder about the length of linkage group one, seemingly with some very large inter marker distances. While most mapping packages can probably produce maps as quickly and easily as this, it doesn't mean that they are correct. It is worth understanding how and why they work so that you can better judge the quality of the map and seek improvements to it.

\hypertarget{estimating-recombination-fractions}{%
\section{Estimating recombination fractions}\label{estimating-recombination-fractions}}

First delete quick.map

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rm}\NormalTok{(quick.map) }
\end{Highlighting}
\end{Shaded}

Now, the first thing to do is estimate the recombination frequencies among all pairs of loci:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AxC}\OtherTok{\textless{}{-}}\FunctionTok{est.rf}\NormalTok{(AxC) }
\end{Highlighting}
\end{Shaded}

Note that we have directed the output of \texttt{est.rf} back to \texttt{AxC} to which it is added.

We can inspect the matrix of recombination frequencies. However, this is a large table, so we'll just look at the top left hand corner:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pull.rf}\NormalTok{(AxC)[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{] }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##              Aflp100.175     AFLP24 AFLP24.320 AFLP43-160Bp Aflp57.165
## Aflp100.175           NA 0.44594595 0.46052632   0.01351351 0.46666667
## AFLP24        0.44594595         NA 0.01369863   0.45833333 0.00000000
## AFLP24.320    0.46052632 0.01369863         NA   0.47945205 0.01351351
## AFLP43-160Bp  0.01351351 0.45833333 0.47945205           NA 0.46575342
## Aflp57.165    0.46666667 0.00000000 0.01351351   0.46575342         NA
\end{verbatim}

Rather fortuitously, you can see some very low recombination fractions and some indicating clear lack of linkage. Since we are working in R, we have access to all its other features. For instance, we can look at the distribution of recombination fractions:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(}\FunctionTok{pull.rf}\NormalTok{(AxC),}\AttributeTok{breaks=}\DecValTok{40}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-392-1.pdf}

Does this help you put a threshold on recombination fraction at which you are willing to declare a pair of markers as linked? Generally, packages use a threshold on both recombination fraction and significance level. We can also extract the latter, as a LOD score:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(}\FunctionTok{pull.rf}\NormalTok{(AxC,}\AttributeTok{what=}\StringTok{"lod"}\NormalTok{)) }
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-393-1.pdf}

Most marker pairs show no evidence of linkage and have a low LOD score. If we truncate the y axis the plot is a little more clear:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(}\FunctionTok{pull.rf}\NormalTok{(AxC,}\AttributeTok{what=}\StringTok{"lod"}\NormalTok{),}\AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{200}\NormalTok{),}\AttributeTok{breaks=}\DecValTok{40}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-394-1.pdf}

Are the recombination fractions and LOD scores telling us the same thing? After a bit of trial and error, we can plot one against the other:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(}\FunctionTok{pull.rf}\NormalTok{(AxC,}\AttributeTok{what=}\StringTok{"rf"}\NormalTok{)[,],}\FunctionTok{pull.rf}\NormalTok{(AxC,}\AttributeTok{what=}\StringTok{"lod"}\NormalTok{)[,],}\AttributeTok{xlab=}\StringTok{"rf"}\NormalTok{,}\AttributeTok{ylab=}\StringTok{"lod"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-395-1.pdf}

\begin{rmdquiz}
\textbf{1}

What does this plot tell us? What do you think would be a good combination of cut-off parameters, and why?
\end{rmdquiz}

\hypertarget{forming-linkage-groups}{%
\section{Forming Linkage groups}\label{forming-linkage-groups}}

Next we want to allocate our markers to linkage groups. The command for this is \texttt{formLinkageGroups}. It has some options that we could experiment with:

\texttt{max.rf} = Maximum recombination fraction for placing two markers in the same linkage group. The default is 0.25.

\texttt{min.lod} = Minimum LOD score for placing two markers in the same linkage group. The default is 3.

\texttt{reorgMarkers} = If TRUE, the markers are organized into the inferred linkage groups. If FALSE, the output is a table indicating the initial chromosome assignments and the inferred linkage group partitions.

We want to add our chromosome assignments to \texttt{AxC} so \texttt{reorgMarkers} should be set to \texttt{T}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test.linkage.groups }\OtherTok{\textless{}{-}}
\FunctionTok{formLinkageGroups}\NormalTok{(AxC,}\AttributeTok{reorgMarkers=}\NormalTok{T,}\AttributeTok{verbose=}\NormalTok{T,}\AttributeTok{min.lod=}\DecValTok{3}\NormalTok{,}\AttributeTok{max.rf=}\FloatTok{0.25}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 10 of 186 
## 20 of 186 
## 30 of 186 
## 40 of 186 
## 50 of 186 
## 60 of 186 
## 70 of 186 
## 80 of 186 
## 90 of 186 
## 100 of 186 
## 110 of 186 
## 120 of 186 
## 130 of 186 
## 140 of 186 
## 150 of 186 
## 160 of 186 
## 170 of 186 
## 180 of 186
\end{verbatim}

An easy way to look at the linkage groups created in \texttt{test.linkage.groups} is to use the command

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plotRF}\NormalTok{(test.linkage.groups)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-398-1.pdf}

Compare this with the unordered markers set in \texttt{AxC}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plotRF}\NormalTok{(AxC) }
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-399-1.pdf}
\begin{rmdquiz}
\textbf{2}

Alter the min.lod and max.rf thresholds (try higher and lower) to see what difference this makes to the plots. Do the default values seem reasonable?
\end{rmdquiz}

Once you have decided on a threshold, you can save the linkage groups back to \texttt{AxC} using your own thresholds.

\texttt{AxC\ \textless{}-formLinkageGroups(AxC,reorgMarkers=T,\ verbose=T,min.lod=??,max.rf=??)}

\hypertarget{marker-ordering}{%
\section{Marker ordering}\label{marker-ordering}}

Next we must order markers within these linkage groups using the command orderMarkers(). This command also has some options we should experiment with:

\texttt{use.ripple} = If TRUE (the default), the initial order is refined by a call to the function ripple.

\texttt{window} = Indicates the number of markers to include in the sliding window of permuted markers for ripple (default is 7). Ripple can take a long time, especially with large number of markers in the window.

\texttt{error.prob} = Assumed genotyping error rate used in the final estimated map. Default is 0.0001. This can make a large difference: a double recombination between closely linked markers is more likely to result from genotyping errors.

First. Let's try without rippling;

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test}\OtherTok{\textless{}{-}}\FunctionTok{orderMarkers}\NormalTok{(AxC,}\AttributeTok{use.ripple=}\NormalTok{F)}
\end{Highlighting}
\end{Shaded}

Plot the recombination fractions of the ordered markers and compare with the unordered (in AxC).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plotRF}\NormalTok{(test)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-403-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plotRF}\NormalTok{(AxC)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-403-2.pdf}

We can also plot a simple cartoon of our mapped markers:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot.map}\NormalTok{(test)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-404-1.pdf}

\begin{rmdquiz}
\textbf{3}

What do you think of the chromosome lengths and numbers in this map?
\end{rmdquiz}

\hypertarget{improving-the-map}{%
\section{Improving the map}\label{improving-the-map}}

There are three things we can do to try to improve the map:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Ripple; where the marker order within a defined window is perturbed to see if improvements are possible.
\item
  Take into account the probability of genotype errors directly in the ordering process. This may be worthwhile for this data set, where we know the quality of the genotyping was poor.
\item
  Clean the data of probable errors -- eliminate improbable double recombinants.
\end{enumerate}

We'll have a look at ripple first. Only one chromosome at a time can be rippled, and the window size should not be too large since this can take a long time. Here are three different versions of the command:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{chr1.ripple4}\OtherTok{\textless{}{-}}\FunctionTok{ripple}\NormalTok{(AxC,}\AttributeTok{chr=}\DecValTok{1}\NormalTok{)     }\CommentTok{\# ripple chr 1 with defaults (window=4)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    276 total orders
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{chr1.ripple7}\OtherTok{\textless{}{-}}\FunctionTok{ripple}\NormalTok{(AxC,}\AttributeTok{chr=}\DecValTok{1}\NormalTok{,}\AttributeTok{window=}\DecValTok{7}\NormalTok{)    }\CommentTok{\# increase window size }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    52560 total orders
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{chr1.ripple4.like}\OtherTok{\textless{}{-}}\FunctionTok{ripple}\NormalTok{(AxC,}\AttributeTok{chr=}\DecValTok{1}\NormalTok{,}\AttributeTok{method=}\StringTok{"likelihood"}\NormalTok{) }\CommentTok{\# use likelihood}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    276 total orders
##     --Order 10 
##     --Order 20 
##     --Order 30 
##     --Order 40 
##     --Order 50 
##     --Order 60 
##     --Order 70 
##     --Order 80 
##     --Order 90 
##     --Order 100 
##     --Order 110 
##     --Order 120 
##     --Order 130 
##     --Order 140 
##     --Order 150 
##     --Order 160 
##     --Order 170 
##     --Order 180 
##     --Order 190 
##     --Order 200 
##     --Order 210 
##     --Order 220 
##     --Order 230 
##     --Order 240 
##     --Order 250 
##     --Order 260 
##     --Order 270
\end{verbatim}

The default method searches for orders which reduce the total number of recombinations. Using likelihood is better but takes longer.

You can examine the output directly but using summary is easier:

\texttt{summary(chr1.ripple4)}

\begin{rmdquiz}
\textbf{4}

Compare the three versions you have just made for chromosome 1. Has any version of ripple made an improvement? If so, which has worked best?
\end{rmdquiz}

Once we have identified an improved order, we can substitute it for the existing order using the command \texttt{switch.order}. Here is a worked example:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#  Existing map }
\FunctionTok{pull.map}\NormalTok{(AxC,}\AttributeTok{chr=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   BArc142_5B   BArc176_7B    BArc74_5B   dUpw115_5B   dUpw398_7B  eAACmCCG171 
##            0           10           20           30           40           50 
##  eAACmCCG184  eAACmCCG408  eAACmCGA270 eAACmeCCC249  eACGmCGG220  eACTmCGG200 
##           60           70           80           90          100          110 
##  eAGCmCTA340  eAGGmCTA120  eATTmCGG230       gwm213    gwm234_5B    wmc517_7B 
##          120          130          140          150          160          170
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# switch order (make sure you choose the correct row)}
\NormalTok{AxC.v2}\OtherTok{\textless{}{-}}\FunctionTok{switch.order}\NormalTok{(AxC,}\AttributeTok{chr=}\DecValTok{1}\NormalTok{,}\AttributeTok{order=}\NormalTok{chr1.ripple7[}\DecValTok{2}\NormalTok{,])}
\CommentTok{\#  Compare new order with the initial order}
\FunctionTok{pull.map}\NormalTok{(AxC.v2,}\AttributeTok{chr=}\DecValTok{1}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   BArc142_5B   BArc176_7B    BArc74_5B   dUpw115_5B   dUpw398_7B  eAACmCCG408 
##      0.00000     77.79435     80.52102    109.50827    183.34461    187.46286 
##  eAACmCCG171  eACGmCGG220 eAACmeCCC249  eAACmCGA270  eACTmCGG200  eAACmCCG184 
##    190.09164    226.04541    254.62921    264.18371    265.56842    282.12582 
##  eAGCmCTA340  eAGGmCTA120  eATTmCGG230       gwm213    gwm234_5B    wmc517_7B 
##    287.50267    322.18080    359.48948    394.33618    432.34606    487.79182
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#  Tidy up}
\FunctionTok{rm}\NormalTok{(AxC.v2)}
\end{Highlighting}
\end{Shaded}

We don't have to type in the new order, we can select the desired order as a row from the ripple output. We can also return the new order to our cross directly, which is what we want to do here:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test}\OtherTok{\textless{}{-}}\FunctionTok{switch.order}\NormalTok{(test,}\AttributeTok{chr=}\DecValTok{1}\NormalTok{,}\AttributeTok{order=}\FunctionTok{summary}\NormalTok{(chr1.ripple7)[}\DecValTok{2}\NormalTok{,]) }
\end{Highlighting}
\end{Shaded}

We could go through every linkage group in turn like this. However, it is also possible to use the option \texttt{use.ripple\ =T} directly within orderMarkers which would be more efficient in general use.

Now we can examine the effect of different assumed error rates on the data. The default in R/qtl is 0.0001 which is probably too low for most genotype platforms; at NIAB we achieved this only by extensive manual curation of SNP array data. 0.001 would be a good target to aim for, but 0.01 is probably more realistic for this data set, where it still represents an average of less than one error per marker over all lines. From your knowledge of genotyping, what do you think would be appropriate?

To examine the effect of a higher assumed error rate, we will reorder our markers in ``test'' using different error rates :

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{err.pr.order}\OtherTok{\textless{}{-}}\FunctionTok{orderMarkers}\NormalTok{(test,}\AttributeTok{use.ripple=}\NormalTok{F,}\AttributeTok{error.prob=}\FloatTok{0.01}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

There is an easy graphical way of comparing the original and new map:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plotMap}\NormalTok{(err.pr.order,test) }
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-411-1.pdf}

Though it is easier to examine a chromosome at a time.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plotMap}\NormalTok{(err.pr.order,test,}\AttributeTok{chr=}\DecValTok{1}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-412-1.pdf}

This has changed the order substantially and has also shrunk the chromosome length. However, there is a danger of over interpreting this map reduction. The elimination of genuine errors will generally reduce map length, but the assumption of too high an error rate will give a falsely short map and incorrect marker order.

\begin{rmdquiz}
\textbf{5}

Compare error rates of 0.0001, 0.01, 0.1 and 0.5. What do you notice about the inter-marker distances on the new map?
\end{rmdquiz}

Now, before doing further QC, we will make a basic map using the default rippling parameters (\texttt{window=4}) for all chromosomes. We will start from the first step again, so we know we are starting from a fresh data set, and as a reminder of the key steps required in making a map:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AxC}\OtherTok{\textless{}{-}}\FunctionTok{read.cross}\NormalTok{(}\StringTok{"csv"}\NormalTok{,}\AttributeTok{dir=}\StringTok{"data"}\NormalTok{,}\StringTok{"AxC\_R.csv"}\NormalTok{,}\AttributeTok{genotypes=}\FunctionTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{,}\StringTok{"B"}\NormalTok{),}\AttributeTok{estimate.map=}\NormalTok{F)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  --Read the following data:
##   78  individuals
##   186  markers
##   10  phenotypes
##  --Cross type: bc
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AxC}\OtherTok{\textless{}{-}}\FunctionTok{est.rf}\NormalTok{(AxC) }
\NormalTok{AxC}\OtherTok{\textless{}{-}}\FunctionTok{formLinkageGroups}\NormalTok{(AxC,}\AttributeTok{reorgMarkers=}\NormalTok{T) }
\NormalTok{AxC}\OtherTok{\textless{}{-}}\FunctionTok{orderMarkers}\NormalTok{(AxC,}\AttributeTok{use.ripple=}\NormalTok{T) }
\end{Highlighting}
\end{Shaded}

Note we've set \texttt{ripple\ =\ T}.

We can also export our data so as to save a copy for later work using \texttt{write.cross}, which has a simple to understand format:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write.cross}\NormalTok{(AxC,}\StringTok{"csv"}\NormalTok{,}\AttributeTok{filestem=}\StringTok{"AxCmapped"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

To highlight probable errors, R/qtl can calculate a LOD associated with p(single genotype is wrong). This is not an absolute test, but intended as a QC tool. It is particularly useful for highlighting improbable double recombinants. The advice is to use a LOD of 4 as a threshold. The results will be added to the cross (AxC). In this case, our data looks pretty good:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AxC}\OtherTok{\textless{}{-}}\FunctionTok{calc.errorlod}\NormalTok{(AxC)}
\FunctionTok{plotErrorlod}\NormalTok{(AxC)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-416-1.pdf}

But, from another data set, here is a non-so-clean version:

\includegraphics{images/dirty_data.png}

\begin{itemize}
\tightlist
\item
  LOD \textless= 2 in white;
\item
  2 \textless{} LOD \textless= 3 in grey;
\item
  3 \textless{} LOD \textless= 4.5 in pink;
\item
  LOD \textgreater{} 4.5 in purple.
\end{itemize}

You can restrict the plot to selected chromosomes if desired.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plotErrorlod}\NormalTok{(AxC,}\AttributeTok{chr=}\DecValTok{2}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-417-1.pdf}

And take a look at the high error LOD markers:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{top.errorlod}\NormalTok{(AxC,}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   chr id      marker errorlod
## 1   2 78 eACTmCAC172 3.105659
## 2   2 54 eAACmCCC176 2.667347
## 3   2  5 eACTmCAC320 2.576581
## 4   2  8 eACTmCAC148 2.431070
## 5   2 37 eACTmCAC148 2.431070
\end{verbatim}

With two markers given with the same map location (ignoring the effect of jitter) any observed recombination will generate a big LOD. Taking a look at the map:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pull.map}\NormalTok{(AxC,}\DecValTok{2}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## eAACmCCC272 B598mCGT390 eACTmCGG365 eAACmCCC176 eACTmCAC148 eACTmCAC320 
##     0.00000    28.03892    31.16497    33.74564    42.10218    46.55053 
## eACTmCAC172 eACTmCAC170 eAACmCCC270    gwm18_1B  BArc240_1B eATTmCGG110 
##    52.52118    53.83598    55.32079    55.32080    56.62002    57.91867 
## eACGmCTG185 eAGCmCTA234 eACTmCAC180 eAGGmCTA130  eAACmCCG96 eAGGmCTA290 
##    64.77927    73.09380    89.77822    97.47865   106.71102   136.45711 
## eACTmCAC115    wmc44_1B 
##   167.14099   178.87267
\end{verbatim}

That isn't a problem here, though you can see the high error markers are all very close to adjacent markers, and presumably have some recombination between them. We can see this in more detail with this very useful plot:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plotGeno}\NormalTok{(AxC,}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-420-1.pdf}

White and black circles indicate markers from each of the two parents. Blue crosses indicate cross-overs. Red squares (none here) highlight genotypes with LODs \textgreater{} the error LOD threshold (default 4). A single double recombination event can however clearly be seen around the highest error LOD marker eACTmCAC172 (around 60 or 125cM depending on the orientation of the chromosome) for the individual at the bottom of the plot, despite the gaps to the neighbouring markers being 0.8 and 1.2cM respectively. This maybe a genotype error. Multiple double recombinant individuals in such a narrow interval would be a sure sign of a mapping or genotype error, suggesting the marker should be dropped.

Other useful quality control tools are listed in the table below. The graphical tools are particularly helpful for detecting regions of the map that require further work. For example \texttt{plotRF(AxC)} plots pairwise recombination frequencies across all chromsomes or a restricted set: \texttt{plotRF(AxC,chr=1:5)}. It can clearly be seen that the ordering on Chromosome 1 can still be improved:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plotRF}\NormalTok{(AxC,}\AttributeTok{chr=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-421-1.pdf}

\begin{rmdquiz}
\textbf{6}

The block of markers 1:3 appear to be more closely linked to markers 11:14. We could swap their position as a block and see if it made an improvement. This anomaly may help explain some of the very long inter-marker distances we observed at the start of chromosome 1, that cause this chromosome to be so much longer than all the rest. What do you think might be going on here? (hint -- it might be worth recalling that bread wheat is hexaploid).
\end{rmdquiz}

This last observation indicates we still have some way to go in improving our map! In reality, several cycles of improvement and QC will be involved in creating a final map; this tutorial has given a quick overview of the main approaches. It can be seen that R/qtl has excellent tools for detecting and displaying potential genotype errors: a table of key Quality Control commands is given below. There is plenty of good documentation on how to map markers in r/qtl, including the book \citet{broman_guide_2009}, though it is not free and the discussion page mentioned at the start of the tutorial.

\hypertarget{quality-control-commands-for-marker-qc-and-mapping}{%
\section{Quality Control commands for marker QC and mapping}\label{quality-control-commands-for-marker-qc-and-mapping}}

\texttt{plot(AxC)} = provides a neat graphical summary of phenotypes and genotypes

\texttt{summary(AxC)}= worth running to check the data have been read in correctly

\texttt{summary.map(AxC)} = summarises chromosome lengths, marker nos. and spacing

\texttt{geno.table(AxC)} = summarises alleles and tests for segregation distortion

\texttt{plot(AxC,pheno.col=x)} = plots histogram for phenotype in column x or of a named phenotype - in which case the name must be in quotes. The phenotype data are stored in a data frame \texttt{AxC\$pheno} which can be accessed and manipulated by column number or ``name.'' You might want to transform the data. eg:

\begin{itemize}
\tightlist
\item
  \texttt{hist(AxC\$pheno{[},1{]})}
\item
  \texttt{AxC\$pheno{[},1{]}\textless{}-AxC\$pheno{[},1{]}\^{}0.5\ hist(AxC\$pheno{[},1{]})}
\item
  \texttt{AxC\$pheno{[},1{]}\textless{}-AxC\$pheno{[},1{]}\^{}2}
\end{itemize}

\texttt{plot.map(AxC,chr=1:2)} plots map for chrs 1 and 2. If the \texttt{chr} argument is missing,
then plots all chromosomes. Can also be used to plot comparisons between maps -- see \texttt{help(plot.map)}. This provides an easy way to look at map expansion/contraction between the supplied map and the estimated map in R/qtl (using the same map order). For example (using a different data set):

\texttt{plot.map(est.map(AxC),AxC)} = comparison of maps

\texttt{plotRF(AxC,chr=1)} = Plots pairwise recombination frequencies

\texttt{subset(AxC,chrs,inds)} = Selects a range of markers and individuals. eg\texttt{subset(AxC,5,2:10)}

\texttt{pull.map(AxC,chr=5)} = Extract the existing map

\texttt{est.map(subset(AxC,2))} = Estimate map distances but keeps the order the same. Useful to study map expansion / contraction. The example re-estimates for chr 2 only

\texttt{jittermap(AxC)} = Gets rid of the problem of having markers at the same location in the supplied map.

\texttt{AxC\textless{}-calc.errorlod(AxC)} = Calculates a LOD associated with p(single genotype is wrong). Add results to the cross (AxC here). Should be used to highlight probable errors, not as an absolute test. Advice is to use a LOD of 4 as a threshold.

\texttt{plotErrorlod(AxC)} = Plots possible genotype errors.

\hypertarget{asmap}{%
\section{ASMap}\label{asmap}}

Before using R/qtl to map QTL, we'll have a brief look at \texttt{ASMap} as an alternative for mapping markers, which definitely worth considering if you have a larger number of markers and R/qtl is too slow.

As ever, the problem with all computer packages is getting the data into them. Once that is achieved, then getting them to do something useful is generally straightforward. \texttt{ASmap} should work directly on pre-existing R/qtl objects, but after a lot of experimentation, this fails:

\texttt{test\textless{}-mstmap.cross(AxC)\ Error\ in\ mstmap.cross(AxC)} :

\emph{The unique identifier for the genotypes, ``Genotype'', cannot be found in the object}

\texttt{ASMap} will, however, work on dataframes, if correctly formatted. There is a text version of the AxC marker data in ASmap.txt. If you look at this, you will notice that; (1) there is no phenotype data; (2) missing data are represented by ``U'' and not ``NA''; (3) the data are transposed compared to the initial AxC data file, with markers running across rows and individuals down columns. This is better for populations with large numbers of markers.

Load the package, you will need to install if you don't have it on your computers:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ASMap)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#delete the "data/"}
\NormalTok{AxC.mst}\OtherTok{\textless{}{-}}\FunctionTok{read.table}\NormalTok{(}\StringTok{"data/AxC\_ASmap.txt"}\NormalTok{,}\AttributeTok{header=}\NormalTok{T,}\AttributeTok{colClasses =} \StringTok{"character"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

Note the added option \texttt{colClasses\ =\ "character"}. This is critical. Without it, the data might be read in as factors rather than text, and this is not an obvious error to pick up.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AxC.mst.map}\OtherTok{\textless{}{-}}\FunctionTok{mstmap.data.frame}\NormalTok{(AxC.mst)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Number of linkage groups: 45
## The size of the linkage groups are: 6    8   10  5   11  1   1   7   5   19  6   6   6   3   6   18  3   3   3   3   2   4   3   1   5   1   6   3   1   3   2   1   5   3   1   1   1   1   2   4   1   1   2   1   1   
## The number of bins in each linkage group: 6  5   10  3   10  1   1   7   5   18  4   4   5   3   5   18  2   3   3   3   2   4   3   1   5   1   6   3   1   3   2   1   5   3   1   1   1   1   2   4   1   1   2   1   1   
\end{verbatim}

As the name of the command suggests, the markers are mapped (very rapidly) by means of a minimum spanning tree: finding the shortest route between points defined by the recombination fractions between all marker pairs. A minimum spanning tree is a solution to what is called ``the travelling salesman problem'': what is the shortest route a salesman should take to visit all the towns in which he wishes to knock on doors? See the paper and documentation for more information.

We are advised that the clustering of markers into linkage groups is very sensitive to the p-value used. The default is 1e-06 and as more markers are added, this should be increased. Try this:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot.map}\NormalTok{(}\FunctionTok{mstmap.data.frame}\NormalTok{(AxC.mst,}\AttributeTok{p.value =} \FloatTok{1e{-}06}\NormalTok{)) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Number of linkage groups: 45
## The size of the linkage groups are: 6    8   10  5   11  1   1   7   5   19  6   6   6   3   6   18  3   3   3   3   2   4   3   1   5   1   6   3   1   3   2   1   5   3   1   1   1   1   2   4   1   1   2   1   1   
## The number of bins in each linkage group: 6  5   10  3   10  1   1   7   5   18  4   4   5   3   5   18  2   3   3   3   2   4   3   1   5   1   6   3   1   3   2   1   5   3   1   1   1   1   2   4   1   1   2   1   1   
\end{verbatim}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-426-1.pdf}

\begin{rmdquiz}
\textbf{7}

Now try altering the p.value up and down to see what effect this has on the map. What do you observe? (remember that the target number of chromosomes for wheat is 21). How might we decide what is best?
\end{rmdquiz}

Note that in all the ASMap maps, we still observe one very long chromosome, just as when we used R/qtl to create the map. As with R/qtl, this observation emphasizes that this is probably not a high quality data set (which we already knew). We will return now to using the default p.value.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AxC.mst.map}\OtherTok{\textless{}{-}}\FunctionTok{mstmap.data.frame}\NormalTok{(AxC.mst,}\AttributeTok{p.value =} \FloatTok{1e{-}06}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Number of linkage groups: 45
## The size of the linkage groups are: 6    8   10  5   11  1   1   7   5   19  6   6   6   3   6   18  3   3   3   3   2   4   3   1   5   1   6   3   1   3   2   1   5   3   1   1   1   1   2   4   1   1   2   1   1   
## The number of bins in each linkage group: 6  5   10  3   10  1   1   7   5   18  4   4   5   3   5   18  2   3   3   3   2   4   3   1   5   1   6   3   1   3   2   1   5   3   1   1   1   1   2   4   1   1   2   1   1   
\end{verbatim}

The commands available in R/qtl should all work on the output file we have created (I haven't tested this fully), but there are some useful additional commands available:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{heatMap}\NormalTok{(AxC.mst.map) }\CommentTok{\# LOD at the bottom rf at the top }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in heatMap(AxC.mst.map): Running est.rf.
\end{verbatim}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-429-1.pdf}

gives better looking output than

\texttt{plotRF(AxC.mst.map)}

Note that LG 16 (the long one) is still badly ordered in ASMap.

Also note these useful diagnostic commands

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{profileGen}\NormalTok{(AxC.mst.map,}\AttributeTok{chr=}\StringTok{"L3"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-430-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{profileMark}\NormalTok{(AxC.mst.map,}\AttributeTok{chr=}\StringTok{"L3"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-430-2.pdf}

There are other options to audit and edit the map too: see the package documentation for more information.

\hypertarget{Data_Visualization}{%
\chapter{Data Visualization.}\label{Data_Visualization}}

\hypertarget{flapjack}{%
\section{Flapjack}\label{flapjack}}

This is based on a tutorial presented by Pauline Bansept-Basler on this course in 2012
Flapjack is software for displaying graphical genotypes and visualizing haplotypes, developed
by the James Hutton institute. A link to the software online can be found \href{https://ics.hutton.ac.uk/flapjack/}{here}. It provides an easy way to store and explore visually your genotype, trait and QTL data. Breeders love it. It is great for displaying data for inbred individuals, less good for outbreeders.

To give you an overview we are first going to explore the genotype example provided by Flapjack. It consists of SNP data from a barley association mapping project.

Start Flapjack from the shortcut on your desktop or the Programs menu. In the bar menu
select \textbf{\emph{File Import data\ldots{}}} a popup windows will appear, click on the tab \textbf{\emph{Example data}} and load the \textbf{\emph{Barley (Sample)}} clicking on Open example button.
You will see the following display.

\begin{figure}
\centering
\includegraphics{images/flapjack_1.png}
\caption{Flapjack display}
\end{figure}

The genotype data are plotted as a grid, with lines displayed horizontally and the markers
vertically. From the combo box above the grid, you can select which chromosome you wish
to visualize. Using the sliding zoom at the bottom, you can select the resolution you wish. The horizontal and vertical scroll bars allows you to navigate the whole dataset for the selected chromosome.The default colouring system is 4 colours code for nucleotides.

Flapjack also provides other options to view data in the Visualization menu/Colour scheme.

\begin{itemize}
\tightlist
\item
  2 colour mode for A/B or 1/0 data such as DArT
\item
  Colour alleles by similarity to a given standard line or parent line (particularly useful)
\item
  Colour alleles by similarity to a comparison marker (particularly useful)
\item
  By frequency of rare alleles
\item
  Random
\item
  And many more including an 8-parent MAGIC option!
\end{itemize}

Another useful option is to highlight heterozygotes: \textbf{\emph{Visualization/Highlight Heterozygote}}

You can also move around the lines or markers to obtain a personal display:

\begin{itemize}
\tightlist
\item
  CTRL+click and drag up/down to move lines
\item
  CTRL+click and drag left/right to move markers
\item
  CTRL+double click to temporarily hide lines or markers.(You may need to switch to marker mode / Line mode using the buttons at the top.
\item
  Insert blank lines to help break the display into regions (Mouse right click , Split lines Insert line break)
\end{itemize}

To explore the sorting options available in flapjack and have a look at the file formats to import data; we are going to import DArT data for a collection of wheat varieties and a few
phenotypes.

Flapjack files formats are relatively simple.

\hypertarget{genotype-data-.geno-file.}{%
\subsection{Genotype data: ``.geno'' file.}\label{genotype-data-.geno-file.}}

You can choose how you represent your missing values (NA /-/?). The genotype file contains a list of varieties or lines, with allele data per marker for that line. Homozygote and heterozygote data can be loaded simultaneously using ``/'' to separate the heterozygous alleles. The format also requires a header line specifying the marker information for each column.

Example .geno file:

\begin{verbatim}
      Marker1 Marker2 Marker3 Marker4
Line1 A       G       G       T
Line2 A       NA      G       T
Line3 T       A/T     C       G
\end{verbatim}

\hypertarget{map-data-.map-file.}{%
\subsection{Map data : ``.MAP'' file.}\label{map-data-.map-file.}}

This contains the markers, the chromosome they are on, and their position within that chromosome. The markers don't need to be in a particular order.

Example .MAP file:

\begin{verbatim}
Marker1   1B    32.5
Marker2   1D    45.0
Marker3   2B    23.9
\end{verbatim}

\hypertarget{trait-data}{%
\subsection{Trait data}\label{trait-data}}

A flapjack data set can optionally store phenotype information associated with the lines. Trait information is imported from a tab delimited file, organized as follows:

The first row is a header row with the trait names. The second header row is optional and can include additional classifiers, for instance environments or years. Subsequent rows include a line/variety name and the phenotype data for the selected line. Trait data for a single trait can be either numerical or categorical. Missing data are left blank.

Example of trait data:

\begin{verbatim}
      Trait1      Trait2      Trait2
      Experiment1 Experiment1 Experiment2
Line1 5.6         High        High
Line2 2.3         High        Low
Line3             Low         Low
Line4 4.7         High        Low
\end{verbatim}

In addition, QTL positions can be loaded. For further information, click on `?' within flapjack.

To load the genotype and map data:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{\emph{File/Import data\ldots{}}} on the \textbf{\emph{Map and genotype}} tab
\item
  For the Map file, browse to ``Mapflapjack.MAP'' from your Flapjack tutorial data folder
\item
  For the Genotype file, browse to ``Genoflapjack.geno''
\item
  Click on \textbf{\emph{Advanced options\ldots{}}} button and check the missing data are set to NA. Some useful options for SNP data can found here: with very large numbers of SNPs, the option to import data with varieties in columns and SNPs in rows is useful.
\end{enumerate}

Click \textbf{\emph{OK}} and \textbf{\emph{Import Map/Genotypes}}

The Wheat DArT genotype data will appear on the main window.

Now to load the phenotype data:
Under \textbf{\emph{File/Import data\ldots{}}} on the \textbf{\emph{Phenotypes tab}}, browse to ``Phenoflapjack.txt'' for Phenotype file and click \textbf{\emph{Import Phenotype}} button.

A table with the phenotype data (yellow rust score, plant height and flowering time) will appear on the main windows. You can switch view between the phenotype data (Trait data folder) and genotype data (Default view folder) from the Data sets panel.

In the default view, in addition to the genotype visualization, you can now see a graphical representation of traits for each line (or heat-map), to the left of the variety name. If you don't see this go to \textbf{\emph{Data Select Traits}} and select some.

Hover over the heat map to obtain the name and value for the trait. To choose which trait is represented here go to \textbf{\emph{Data/Select Traits\ldots{}}}

For example, in \textbf{\emph{Data/Select Traits}} select strawlength2010 and strawlength2011, the two phenotypes will now be visible from the default view. As these data are a measure of the straw length in two successive years, the heat map for both phenotypes follows is similar.

Different sorting options are available in Flapjack. You can sort the lines:

\begin{itemize}
\tightlist
\item
  by their similarity to a given line
\item
  for a given marker, set of markers
\item
  by trait values
\end{itemize}

To sort lines by their similarity.

Got to \textbf{\emph{Analysis/Sort lines/By similarity.}} In the pop-up windows select your reference line. As an example set it to \textbf{\emph{Cadenza\_833}}. Then select the chromosomes which are going to be use for the similarity calculation. For the example select all and click \textbf{\emph{OK}}.

The varieties visible on the top of the list are now Cadenza, Cyber, Warlock, Xi19, Scorpion, Axona and Tonic. Warlock, Scorpion and Xi19 have Cadenza in their direct pedigree, Axona and Tonic are the parents of Cadenza. While the pedigree of Cyber is unknown, we can conclude that this variety has some parentage with Cadenza.

To sort for a given marker, first we have to select the marker. For some reason, to do this, you first deselect the marker you want, then invert the selected and unselected markers: On chromosome 4D, hover over the different markers to find RhTD1b and then click on it. Then use the command \textbf{\emph{Edit/select marker/invert selection}}. Note you have to be in the Markers mode to be able to select a Column. The mode can be modified using the Edit menu or using the short cut under the menu line.

The marker column for RhtB1b will now be the only one coloured, indicating that only this marker has been selected as a key for sorting.

Resort the data on similarity. All the lines are now sorted in order of the marker alleles for RhTD1b. At the top of the list the lines with allele 0, and at the bottom the lines with the allele1, followed by a few lines with no genotype for the selected marker.

Make sure all traits are being displayed. \textbf{\emph{Data/Select traits}} select them all.

To sort by trait value go to \textbf{\emph{Analysis/Sort lines by trait}} and select ``strawlength2010'' and ``strawlength2011''. The phenotype heat map in now sorted from green (low values) to red (high values) for the two selected phenotypes selected with the varieties are sorted in the same order. You should be able to see that allele 0 of RhtD1b is often associated with varieties with high straw length. This may be easier to see if you display only the two height phenotypes. RhtD1b is one of the major dwarfing genes in wheat.

\hypertarget{the-exercise-3}{%
\subsection{The Exercise}\label{the-exercise-3}}

\textbf{Use flapjack to examine the interval containing a QTL.}

\hypertarget{background}{%
\subsubsection{Background}\label{background}}

The data are yellow rust scores from a MAGIC wheat population and an association mapping panel. In this population, genome-wide analysis detected very strong associations between disease resistance and a cluster of SNPs on chromosome 2D. MAGIC populations are created from multiple founder lines, eight in this case. The resistant alleles at each SNP were carried by only two founders, Claire and Alchemy. Claire is a parent line of Alchemy. The other parents carried no resistant alleles at SNPs on chromosome 2D. The resistant alleles carried by Claire and Alchemy are therefore not merely the same (identical by state), they are identical by descent. This means that by aligning chromosomes for Claire and Alchemy and looking for a common haplotype covering the associated SNPs we can define a chromosome block (a haplotype) unique to the two parents which must contain the QTL. This has been done and defined a region of 54cM. MAGIC populations are highly recombined. Can we be more precise?

The two most associated SNPs are:

BobWhite\_c5756\_516: --log10(p-value) = 289.9
RAC875\_c50347\_258: --log10(p-value) = 300.0

These are 1.5cM apart. It is hard to distinguish which of these best predicts resistance. The MAGIC lines are not that inbred and there is quite a bit of residual heterozygosity. RAC875\_c50347\_258 happens to be a dominant SNP. The QTL is also dominant for susceptibility. The dominant allele at the marker is in-phase with the dominant allele at the QTL. This SNP may therefore have the highest score not because it is closest to the QTL but because the dominant alleles at the SNP and QTL match, giving a higher test statistic.

\hypertarget{method}{%
\subsubsection{Method}\label{method}}

We shall load the marker data from the 54 cM region for 720 MAGIC lines and the eight population founders into flapjack, and search for shortest haplotype which includes the QTL. The simplest way to do this is to select lines with very high levels of resistance, first confirm that these carry the resistance tagging alleles at the markers, and then construct the shortest haplotype we can.

The data you need are in three files:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  YR.MAGIC.MAP
\item
  YR.MAGIC.geno
\item
  YR.MAGIC.phen
\end{enumerate}

Load the data as before. Alternatively, load `MAGIC 2D.flapjack' directly, which has saved the above data, by selecting Open Project.

Low trait values are more resistant. Is it possible to discriminate between the two SNPs on trait values?

Compare the 10 most resistant with the 10 least resistant lines. Ignore any lines with lots of missing data. (Tag the lines, invert the selection, then use \textbf{\emph{Edit /Show/Hide lines.}}) Identify the largest interval among the 20 selected lines which discriminates the two groups.

What effect would map errors have on the process we have just done?
What effect would genotype errors have?
What effect would the size of the QTL effect have?

\hypertarget{selection-theory}{%
\chapter{Selection theory and breeding program design.}\label{selection-theory}}

\hypertarget{Genomic-prediction}{%
\chapter{Genomic prediction.}\label{Genomic-prediction}}

\hypertarget{ridge-regression-and-genomic-prediction-in-excel}{%
\section{Ridge regression and Genomic Prediction in Excel}\label{ridge-regression-and-genomic-prediction-in-excel}}

\hypertarget{introduction-3}{%
\subsection{Introduction}\label{introduction-3}}

Excel has functions for handling matrix operations, and using these is an easy way to get familiar with matrix algebra, as you can see what is happening. We're going to have a go at genomic prediction using a subset of data from the TriticeaeGenome wheat association mapping panel. We shall work with a set of ten lines and twelve marker loci, with three traits, height (HT), flowering time (FT) and yield. In our example the ten selected lines were all first listed in 1996. The 12 markers include a marker for PPD\_D1, which has a major effect on flowering time, and Rht2, which has a major effect on HT.

The data file we shall use is ``gs test class example.xlsx''. It is in the data folder for today's lecture.

We shall use the following matrix functions:

\begin{itemize}
\tightlist
\item
  TRANSPOSE(x)
\item
  MMULT(x,y)
\item
  MINVERSE(x)
\end{itemize}

Matrix functions in Excel are different to the usual functions. They apply to a block of cells, not just to one. To enter, first mark the block of cells for the results. For example, to transpose a matrix of n rows by m columns, you would first mark a block of m rows and n columns. Then enter the formula in the normal way, using the formula wizard if you wish, as part of which you will enter the block of cells, n rows by m columns which you wish to transpose. But instead of typing ENTER to finish, hold down CTRL and SHIFT simultaneously then press ENTER. This applies the formula to the whole block, although visually the only difference is that the formula in each cell reference is now encased in \{\}.

\hypertarget{the-exercise-4}{%
\subsection{The Exercise}\label{the-exercise-4}}

Open the Excel workbook. Start in the ``TG example data'' sheet.

In this tutorial, you are expected to enter the appropriate formulae in the highlighted cells, working through the tabs, from left to right, either following the instructions below, or following me.

\begin{rmdquiz}
\textbf{1}

a: Note there are no missing data. What would we have to do if there were some?

b: Note there are no monomorphic markers. What would their effect be on the prediction equation?

c: There are ten lines and twelve markers, Why won't an ordinary regression approach work?
\end{rmdquiz}

To fit a ridge regression of flowering time on the markers, we are going to solve the equation:

\[b = (X’X+Iλ)^{-1} X’Y\]

\begin{rmdquiz}
\textbf{2} What is X? Identify it in the spreadsheet. What is Y? What is b?
\end{rmdquiz}

We'll work on standardised marker data. We don't have to but it is common practice. There are arguments for and against this, which we can discuss.

Enter formulae for the mean and variance of each marker where indicated

Move over to the tab ``standardised''. We are going to standardise the marker data and create the \(X\) matrix here.

\begin{rmdquiz}
\textbf{3}
Insert values in the design matrix for the mean. Do you know what they should be?
\end{rmdquiz}

Standardise the data for each marker by subtracting the mean and dividing by the standard deviation. (Leave the column for the mean alone.)

\begin{rmdquiz}
\textbf{4}
How can you check you've done the standardisation correctly? Do this.
\end{rmdquiz}

In the next tab, ``solution'', we are going to estimate the mean and the marker effects.

At the top, for ease of working, there is a copy of \(X\). Below are \(X’\) and \(X’X\).

Work out \(X’X + Iλ\). (Use ``='' to take a copy of \(X’X\), then modify the formula to add \(λ\) to the diagonal.)

Set this up so \(λ\) can be altered for each diagonal element independently -- i.e.~have a column of \(λ\)'s rather than a single value.

\begin{rmdquiz}
\textbf{5} Are there any diagonal elements we should not penalise? What is the effect of the penalty?
\end{rmdquiz}

Set all \(λ\)'s to 12 initially (except for any you think should be left alone). I usually start off with \(λ\) equal to the number of markers. Invert \((X’X + Iλ)\) i.e.~find \((X’X + Iλ)^{-1}\)

Check the inversion has been carried out correctly.

\begin{rmdquiz}
\textbf{6}
Set all λ to zero. What is the effect on the inverse?
\end{rmdquiz}

Change \(λ\) back.

Calculate \(X’Y\)

Calculate the solution - \(b = (X’X+Iλ)^{-1} X’Y\)

\begin{rmdquiz}
\textbf{7}

See what effect changing λ has on the solution:

a: What happens when you set λ to very high values.
b: What happens when you set λ to different values for different markers.
c: How many values of λ can you set to zero and still get a solution? Any idea why?
\end{rmdquiz}

\begin{rmdquiz}
\textbf{8}

In the next tab ``pred in training pop'': For any value of λ, predict the performance of the ten lines and compare it to the observed performance. You can get these through matrix multiplication as Xb. What is the best prediction you can get?
\end{rmdquiz}

Comparing observed and predicted values with the same set of lines used to create the prediction equation gives an over optimistic view of accuracy. The predictions would be biased if we search for the value of \(λ\) which gives the best fit. Therefore we need to test our prediction in a separate set of lines. In practice (hard in Excel), we would use 90\% of the lines in the TG dataset to work out the prediction equation, and the remaining 10\% to test the prediction. This could be repeated this by leaving out 10 independent sets of 10\%. This is called ``ten-fold cross validation''. Five-fold cross validation also frequently used.

For this exercise, we will instead use the 10 lines in the training set to predict the remaining 307 lines in the TG panel. At the top of the sheet ``pred in test pop'' you will find marker and height data for the lines in the TG panel (including the ten in the training population. These ought to have been removed for a fair test.) We can use these to see how well the prediction equation from the training panel performs.

Underneath the data:

Standardise the marker scores. Remember that the marker data must be standardised using the mean and the variance of the ten lines in the test set so that the prediction equation is valid on these lines. Should we have standardised the training population using the mean and variance of the entire dataset? I don't know but we could alter the workbook to test this.

Again, use \(Xb\) to predict their performance.

The predictions will be much worse than those for the lines in the training population. Ten lines in the training population was a ludicrously small number used for demonstration purposes only.

\begin{rmdquiz}
\textbf{9}

a: Returning to the ``solution'' worksheet, try different values of λ for different SNPs. Two of the markers tag known major genes in wheat: Ppd for flowering time and Rht for height. You can remove the penalty for those loci, or set one value of λ for those and a different one for the remaining SNPs. Experiment. Can you improve the agreement between predicted and observed further?

b: For this small data set, you can also set a different value for λ for each SNP and search for the values which give the maximum correlation. Use Solver to try this. What is the highest correlation achieved for height
\end{rmdquiz}

It would be great if we could do this routinely in a training population with large numbers of markers and individuals, but that is not possible. Instead, other models are used in which an assumption is made about the nature of the marker effects and the genetic architecture of the trait. This prior knowledge allows different penalties to be fitted to each marker. The methods Bayes A and Bayes B do this for example. Bayes B estimates a proportion of the markers to have no effect on the trait. (Ridge regression and Bayes A give every marker an effect, no matter how small.) In practice, it has been found that there is usually little difference between methods, so using ridge regression doesn't go too far wrong. This is not guaranteed, however.

\begin{rmdquiz}
\textbf{10}
If there is time, you can now repeat the GS analysis for flowering time and yield by modifying the vlookup functions in column C in ``pred in test pop'' and column D in ``TG example data''. What is the best correlation you can achieve for each trait? Why might some traits be easier to predict than others?
\end{rmdquiz}

\hypertarget{genomic-prediction-with-rrblup}{%
\section{Genomic Prediction with rrBLUP}\label{genomic-prediction-with-rrblup}}

\hypertarget{introduction-4}{%
\subsection{Introduction}\label{introduction-4}}

Genomic Selection was first proposed in 2001 \citep{Meuwissen1819}. It looked ahead to the use of high density markers to cover the whole genome, selecting on a score accumulated over marker intervals. It is now much discussed in plant breeding (\citet{Bernardo2007}, \citet{Heffner2009},
\citet{Piepho2009}, \citet{Jannink2010}). The basic idea is that all markers or marker intervals are included in a model of phenotype-genotype relationships simultaneously. Because there is no selection of a subset of markers which are significant, there is no bias from the winner's curse. However, there are generally more markers than genotyped individuals available on which to estimate the marker effects and this causes statistical problems. The predicted
trait value is generally called the GEBV for genomic estimated breeding value. Strictly, our
predicted values aren't breeding values in the animal genetics sense, but we'll still use that terminology as shorthand. An excellent source of on-line information is the course notes of BJ Hayes, one of the authors of the original publication. \href{http://snp.toulouse.inra.fr/~alegarra/ben_hayes_course/toulouse_course_notes.pdf}{See this link.}

We're going to have a go at Genomic Selection using some DArT marker data on wheat
NL/RL varieties, using \texttt{rrBLUP} (ridge regression BLUP). A description of ridge regression is
given in the lecture notes. The intention is to demonstrate that genomic prediction is really
easy to do, but the answers you get depend critically on the composition of your data: the
skill is not in running the software but in interpreting the results.

The basic idea we shall follow is of ``cross validation.'' We shall partition the complete
dataset into test and training sets. In the training set, we calibrate markers against the
phenotype. We then use this calibration to predict phenotype from markers in the test set.
Finally we can compare observed and predicted phenotypes and see how we've done. In this way we can compare the efficiency of different methods of calibration and of relationships between the test and training sets. One can also partition the data into test and training sets in many different ways -- either at random or systematically to see what effect this has on predictions. Test and training sets don't have to be of equal size. Often, the test set is small -- say a fifth of the total. There are then five independent partitions of the data and one would look at all of them.

In ridge regression, we are solving the equations (\emph{see the lecture notes}):

\[b = (X’X+Iλ)^{-1} X’Y\]
\(b\) is the vector of regression coefficients: one for the mean plus one for each marker. \(X\) is a matrix of marker effects, one column for each marker with values 0 or 1 (or 0, 1 2 if we had heterozygotes). \(λ\) is a user supplied penalty to apply to each regression coefficient. \(Y\) is the vector of yields, and \(I\) is the identity matrix.

This is the usual least squares solution for multiple regression, except that a penalty, \(λ\), is added to the diagonal elements of \(X’X\). If \(λ\) is zero, the \(b\) are the usual least squares estimates.

Just as with BLUP, the regression coefficients are shrunk from their least squares values
towards zero. Because of this, ridge regression is usually carried out after \(Y\) is first adjusted to a mean of zero: it doesn't make sense to want to shrink the mean towards zero too. Equivalently, the penalty is not applied to the first entry of \(X’X\) so the mean estimate is not shrunk.

The covariates are often standardized to zero mean and unit variance to ensure that all
variables are on the same scale. With marker data it is arguable whether this is necessary or
not: the scale is 0\ldots1 for every marker. In practice, it doesn't seem to matter much whether you standardize or not with the datasets I've looked at so far.

More of a problem is to decide on the appropriate value for the penalty, \(λ\). According to
Hayes, if \(λ = σ^2e / σ^2g\) this is equivalent to BLUP. Here, \(σ^2g\) is the contribution to the genetic variance for each marker and not \(σ^2g\) for the whole genome. The package \texttt{rrBLUP} \citep{Endelman2011} follows this approach. Alternatively, one could select \(λ\) through cross validation.

We are going to use a lot of R code. Don't worry too much if you don't understand it all. Much of the data processing could equally be done in Excel. Aside from learning R, its advantage is that once you've done something once, you can easily save and modify the commands to run on other datasets. However, the most important point of this exercise is to understand something about the strengths and weaknesses of genomic selection.

Load R. Check that rrBLUP has been installed. If not, download it from the CRAN. Remember that after installing a package you must also load it into the current R session to be able to use it.

\hypertarget{the-dataset}{%
\subsection{The dataset}\label{the-dataset}}

Data are in ``NL\_RL\_GS.xls''. The sheet ``extended UK set'' contains the raw data. There are 159
varieties of winter wheat with yield data, and 217 DArT markers, largely unmapped. Ideally,
we would need more markers and more individuals. Note also that the year in which the
variety was first tested in UK National List trials is given. These data are a subset of those analysed in \citet{Mackay2011} in which you will find more information about the origins of the data.

\hypertarget{loading-and-preparing-the-data}{%
\subsection{Loading and preparing the data}\label{loading-and-preparing-the-data}}

The data are in the spreadsheet NL\_RL\_GS.xls. You'll see that there are some missing data.
These have been imputed using ``softImpute'' and are also in the spreadsheet. These data have been saved as a csv file for ease of input.

Read in the data :

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#delete the "data/" if you copy this}
\NormalTok{NLRL}\OtherTok{\textless{}{-}}\FunctionTok{read.csv}\NormalTok{(}\StringTok{"data/NL\_RL\_GS.csv"}\NormalTok{,}\AttributeTok{header=}\ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Check the size of the dataset:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dim}\NormalTok{(NLRL) }\CommentTok{\# Markers start in column four.}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 159 221
\end{verbatim}

\hypertarget{examining-the-data-structure}{%
\subsection{Examining the data structure}\label{examining-the-data-structure}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{attach}\NormalTok{(NLRL)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## The following object is masked from TG_data_for_day_1:
## 
##     yield
\end{verbatim}

Before moving on to the ridge regression itself, we'll look at the dominant feature of these data: the relationship between yield and age of varieties, given here by the column ``year\_in'' which is the first year the variety entered this trial series.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(NLRL}\SpecialCharTok{$}\NormalTok{year\_in,NLRL}\SpecialCharTok{$}\NormalTok{yield)}
\CommentTok{\#Just because we can, add a trend line:}
\FunctionTok{abline}\NormalTok{(}\FunctionTok{lm}\NormalTok{(yield}\SpecialCharTok{\textasciitilde{}}\NormalTok{year\_in))}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-444-1.pdf}

Finally, try this:

\texttt{identify(year\_in,yield,labels=Name)} - You'll need to mouse-over the graph and click. (In Rstudio, nothing happens until you ``Finish'' in the graphics window. This works better in native R. Any comments on the graph?

We'll look at one other feature of the data before we move on: how genetic relationships
among varieties have changed over time. The relevance of this will be seen later. To do a thorough job, we would analyse the data with packages designed for this task. Here, we're going to do a cheap and cheerful analysis using some standard R functions. For ease of
display, we'll pool varieties into decade groups and display the relationships between
decades.

Don't worry about the R code here: with a bit of experience, searching and by asking
colleagues, your knowledge begins to grow. Or you stumble across something in someone else's code. I only discovered ``identify'' a couple of years ago.

We're going to calculate the mean allele frequency for each marker, and save the results.
First, we define a matrix to keep the results. There are seven decades in the dataset and 217
markers:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{decade\_means}\OtherTok{\textless{}{-}}\FunctionTok{matrix}\NormalTok{(}\AttributeTok{nrow=}\DecValTok{7}\NormalTok{,}\AttributeTok{ncol=}\DecValTok{217}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For ease of interpretation, I want to name the rows of this matrix by their decades.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rownames}\NormalTok{(decade\_means)}\OtherTok{\textless{}{-}}\FunctionTok{c}\NormalTok{(}\DecValTok{1940}\NormalTok{,}\DecValTok{1950}\NormalTok{,}\DecValTok{1960}\NormalTok{,}\DecValTok{1970}\NormalTok{,}\DecValTok{1980}\NormalTok{,}\DecValTok{1990}\NormalTok{,}\DecValTok{2000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The next bit of code defines the means:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{decade}\OtherTok{\textless{}{-}}\DecValTok{10}\SpecialCharTok{*}\NormalTok{(year\_in}\DecValTok{{-}1940}\NormalTok{)}\SpecialCharTok{\%/\%}\DecValTok{10}\SpecialCharTok{+}\DecValTok{1940}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{5}\SpecialCharTok{:}\DecValTok{217}\NormalTok{) decade\_means[,(i}\DecValTok{{-}4}\NormalTok{)]}\OtherTok{\textless{}{-}}\NormalTok{(}\FunctionTok{tapply}\NormalTok{(NLRL[,i],}\AttributeTok{INDEX=}\NormalTok{decade,}\AttributeTok{FUN=}\NormalTok{mean))}
\end{Highlighting}
\end{Shaded}

The first line creates a new variable with the decade in which each variety entered trial. The \texttt{\%/\%} structure is new. It is for integer division, in which 41/10 = 4, 49/10 = 4 and 50/10 = 5. So you can see the line takes the year each variety entered trial and returns the decade of entry.

The second line takes the mean allele frequency for each marker in each decade and saves
it. \texttt{tapply()} is a command which applies an R function (given in \texttt{FUN=mean}) to each column
of the data (in \texttt{NLRL{[},i{]}}) for each value of the grouping criterion (given in \texttt{INDEX=decade}). You can write a loop to do exactly the same thing, but \texttt{tapply} is quicker. I only learned about this from a course I gave in which there were several R gurus among the participants.

Have a look at the first few rows of \texttt{decade\_means} to check that it has worked.

Now we estimate a genetic distance between our mean allele frequencies using the R command dist. This has a number of options (see help(dist)for what distance measure to use. The default is Euclidian which is reasonable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{decade\_dist}\OtherTok{\textless{}{-}}\FunctionTok{dist}\NormalTok{(decade\_means,}\AttributeTok{diag=}\ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

decade\_dist should be 7x7 matrix of distances between decades. Have a look at it. Can you
see the pattern? You can visualize it:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{image}\NormalTok{(}\DecValTok{1930}\SpecialCharTok{+}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{7}\NormalTok{)}\SpecialCharTok{*}\DecValTok{10}\NormalTok{,}\DecValTok{1930}\SpecialCharTok{+}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{7}\NormalTok{)}\SpecialCharTok{*}\DecValTok{10}\NormalTok{,}\FunctionTok{as.matrix}\NormalTok{(decade\_dist))}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-449-1.pdf}

A more sophisticated view, which includes a tree is given by:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{heatmap}\NormalTok{(}\FunctionTok{as.matrix}\NormalTok{(decade\_dist))}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-450-1.pdf}

\hypertarget{ordinary-least-squares}{%
\subsection{Ordinary least squares}\label{ordinary-least-squares}}

We are now ready to have a go at getting some GEBVs. First, let's demonstrate that having
more markers (217) than varieties (159) is a problem. Use ordinary linear regression to fit
three markers, then fit all the markers. To fit three markers:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# regress yield on the first three markers}
\FunctionTok{anova}\NormalTok{(}\FunctionTok{lm}\NormalTok{(yield}\SpecialCharTok{\textasciitilde{}}\NormalTok{NLRL[,}\DecValTok{5}\NormalTok{]}\SpecialCharTok{+}\NormalTok{NLRL[,}\DecValTok{6}\NormalTok{] }\SpecialCharTok{+}\NormalTok{NLRL[,}\DecValTok{7}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Analysis of Variance Table
## 
## Response: yield
##            Df Sum Sq Mean Sq F value  Pr(>F)  
## NLRL[, 5]   1  13.66 13.6567  6.0150 0.01529 *
## NLRL[, 6]   1   1.98  1.9846  0.8741 0.35128  
## NLRL[, 7]   1   0.01  0.0068  0.0030 0.95650  
## Residuals 155 351.92  2.2704                  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

To fit all the markers, we don't want to enter \texttt{NLRL{[},5{]}\ +\ NLRL{[},7{]}\ +\ ...\ dim(NLRL+\ NLRL{[},221{]}))} by hand. We have to tell R to treat the data we have read in as a matrix, and then the \texttt{lm} command will let us reference the columns of the matrix:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# fit all markers}
\FunctionTok{anova}\NormalTok{(}\FunctionTok{lm}\NormalTok{(yield}\SpecialCharTok{\textasciitilde{}}\FunctionTok{as.matrix}\NormalTok{(NLRL[,}\DecValTok{5}\SpecialCharTok{:}\DecValTok{221}\NormalTok{])))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning in anova.lm(lm(yield ~ as.matrix(NLRL[, 5:221]))): ANOVA F-tests on an
## essentially perfect fit are unreliable
\end{verbatim}

\begin{verbatim}
## Analysis of Variance Table
## 
## Response: yield
##                           Df Sum Sq Mean Sq F value Pr(>F)
## as.matrix(NLRL[, 5:221]) 158 367.57  2.3264               
## Residuals                  0   0.00
\end{verbatim}

How many markers can you fit? The anova tells you.

\hypertarget{rrblup}{%
\subsection{rrBLUP}\label{rrblup}}

We'll now use \texttt{rrBLUP} for genomic prediction . \texttt{rrBLUP} predicts individual performances
using a genomic relationship matrix, but can transform results from this into weightings for
individual markers. There is no search for an optimum value for \(λ\); this comes from the
estimates of genetic and environmental variances among lines.

\begin{rmdnote}
Don't use \texttt{rrBLUP} for GWAS, although the manual says you can. Use the additional
package from the same source; \texttt{GWASpoly}, which we used earlier in the week.
\end{rmdnote}

Install the package if this has not been done. Then load it for use:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rrBLUP)}
\end{Highlighting}
\end{Shaded}

\texttt{rrBLUP} requires that marker data are coded as -1, 0 +1 rather than 0,1,2. This allows the transformed marker data to be used to create a relationship matrix which takes into account differential inbreeding among parents and produce valid estimates (BLUPs) of the individuals under consideration. Data are currently coded 0 and 1, so to transform these values -1 and +1, subtract a half then multiply by two:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{NLRL[,}\DecValTok{5}\SpecialCharTok{:}\DecValTok{221}\NormalTok{]}\OtherTok{\textless{}{-}}\NormalTok{(NLRL[,}\DecValTok{5}\SpecialCharTok{:}\DecValTok{221}\NormalTok{]}\SpecialCharTok{{-}}\FloatTok{0.5}\NormalTok{)}\SpecialCharTok{*}\DecValTok{2}
\end{Highlighting}
\end{Shaded}

There are no missing data in this dataset, but \texttt{rrBLUP} will handle missing data automatically. Let's first see if prediction using a genomic relationship matrix and prediction from ridge regression give the same answer.

\texttt{mixed.solve} is the command for fitting the mixed model to either a relationship matrix
among lines or by regression on markers. First, by use of a relationship matrix:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{BLUP }\OtherTok{\textless{}{-}} \FunctionTok{mixed.solve}\NormalTok{(NLRL}\SpecialCharTok{$}\NormalTok{yield,}\AttributeTok{K=}\FunctionTok{A.mat}\NormalTok{(NLRL[,}\DecValTok{5}\SpecialCharTok{:}\DecValTok{221}\NormalTok{]))}
\NormalTok{pred.BLUP}\OtherTok{\textless{}{-}}\FunctionTok{as.numeric}\NormalTok{(BLUP}\SpecialCharTok{$}\NormalTok{beta)}\SpecialCharTok{+}\NormalTok{BLUP}\SpecialCharTok{$}\NormalTok{u}
\FunctionTok{plot}\NormalTok{(pred.BLUP,NLRL}\SpecialCharTok{$}\NormalTok{yield)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-456-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(pred.BLUP,NLRL}\SpecialCharTok{$}\NormalTok{yield)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.958079
\end{verbatim}

\texttt{A.mat} uses rrBLUP's internal definition of kinship, and therefore of the additive relationship matrix. Alternative relationship matrices can be provided too. Next, by ridge regression:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred.RR}\OtherTok{\textless{}{-}}\FunctionTok{mixed.solve}\NormalTok{(NLRL}\SpecialCharTok{$}\NormalTok{yield,NLRL[,}\DecValTok{5}\SpecialCharTok{:}\DecValTok{221}\NormalTok{], }\AttributeTok{method=}\StringTok{"REML"}\NormalTok{,}\AttributeTok{bounds=}\FunctionTok{c}\NormalTok{(}\FloatTok{1e{-}09}\NormalTok{,}
\FloatTok{1e+09}\NormalTok{), }\AttributeTok{SE=}\ConstantTok{FALSE}\NormalTok{, }\AttributeTok{return.Hinv=}\ConstantTok{FALSE}\NormalTok{)}

\NormalTok{pred.RR.all}\OtherTok{\textless{}{-}}\FunctionTok{as.matrix}\NormalTok{(NLRL[,}\DecValTok{5}\SpecialCharTok{:}\DecValTok{221}\NormalTok{]) }\SpecialCharTok{\%*\%}\NormalTok{ pred.RR}\SpecialCharTok{$}\NormalTok{u }\SpecialCharTok{+}
\FunctionTok{as.numeric}\NormalTok{(pred.RR}\SpecialCharTok{$}\NormalTok{beta)}

\FunctionTok{plot}\NormalTok{(pred.RR.all,NLRL}\SpecialCharTok{$}\NormalTok{yield)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-457-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(pred.RR.all,NLRL}\SpecialCharTok{$}\NormalTok{yield)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           [,1]
## [1,] 0.9580788
\end{verbatim}

Now compare the results from the two:

\texttt{plot(pred.RR.all,pred.BLUP)}- How do the results compare? We can see what the maximum possible predicted yield is:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sum}\NormalTok{(}\FunctionTok{abs}\NormalTok{(pred.RR}\SpecialCharTok{$}\NormalTok{u))}\SpecialCharTok{+}\NormalTok{pred.RR}\SpecialCharTok{$}\NormalTok{beta}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 14.17547
\end{verbatim}

Is this realistic? To use the kinship matrix to predict the performance of individuals with no phenotype, include those individuals in the computations, but with their phenotype set to NA:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{samp.temp}\OtherTok{\textless{}{-}}\FunctionTok{sort}\NormalTok{(}\FunctionTok{sample}\NormalTok{(}\DecValTok{159}\NormalTok{,}\DecValTok{50}\NormalTok{,}\AttributeTok{replace=}\NormalTok{F))}
\NormalTok{temp }\OtherTok{\textless{}{-}}\NormalTok{NLRL[,}\DecValTok{3}\NormalTok{]}
\NormalTok{temp[samp.temp] }\OtherTok{\textless{}{-}} \ConstantTok{NA}
\NormalTok{BLUP }\OtherTok{\textless{}{-}} \FunctionTok{mixed.solve}\NormalTok{(temp,}\AttributeTok{K=}\FunctionTok{A.mat}\NormalTok{(NLRL[,}\DecValTok{5}\SpecialCharTok{:}\DecValTok{221}\NormalTok{]))}
\NormalTok{pred.BLUP}\OtherTok{\textless{}{-}}\FunctionTok{as.numeric}\NormalTok{(BLUP}\SpecialCharTok{$}\NormalTok{beta)}\SpecialCharTok{+}\NormalTok{BLUP}\SpecialCharTok{$}\NormalTok{u}
\FunctionTok{cor}\NormalTok{(pred.BLUP[samp.temp],NLRL[samp.temp,}\DecValTok{3}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.8300724
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(pred.BLUP[samp.temp],NLRL[samp.temp,}\DecValTok{3}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-459-1.pdf}

How does this accuracy compare with predictions for those with missing data?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(pred.BLUP[}\SpecialCharTok{{-}}\NormalTok{samp.temp],NLRL[}\SpecialCharTok{{-}}\NormalTok{samp.temp,}\DecValTok{3}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-460-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(pred.BLUP[}\SpecialCharTok{{-}}\NormalTok{samp.temp],NLRL[}\SpecialCharTok{{-}}\NormalTok{samp.temp,}\DecValTok{3}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9638327
\end{verbatim}

\hypertarget{prediction-forward-in-time}{%
\subsection{Prediction forward in time}\label{prediction-forward-in-time}}

The results so far are positive. Given a new variety, we can predict its yield well without running any variety trials. The predictions are dependent on the quality of the phenotype data, so unless heritability is 100\%, we'll never get a perfect correlation. In fact, the upper limit for the correlation is h, the square root of the phenotypic heritability.

For breeding purposes, we would be predicting forward in time using historical data: we
don't want to predict the performance of old varieties which are most likely already
outclassed. To test this, we can split the data in half on age, using the old varieties as the training set and the new varieties as a test set. We'll manipulate the data in R.

First find a suitable point to split the data:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{median}\NormalTok{(NLRL[,}\StringTok{"year\_in"}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1989
\end{verbatim}

You can check the split by counting the number of varieties lying above and below your cutoff.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sum}\NormalTok{(NLRL[,}\StringTok{"year\_in"}\NormalTok{]}\SpecialCharTok{\textless{}}\DecValTok{1950}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 13
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sum}\NormalTok{(NLRL[,}\StringTok{"year\_in"}\NormalTok{]}\SpecialCharTok{\textgreater{}=}\DecValTok{1950}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 146
\end{verbatim}

Now set the most recent half of varieties to missing values for their phenotype:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{temp }\OtherTok{\textless{}{-}}\NormalTok{NLRL[,}\DecValTok{3}\NormalTok{]}
\NormalTok{temp[NLRL[,}\DecValTok{4}\NormalTok{]}\SpecialCharTok{\textgreater{}=}\DecValTok{1989}\NormalTok{]}\OtherTok{\textless{}{-}}\ConstantTok{NA}
\end{Highlighting}
\end{Shaded}

Fit BLUPs to the modified dataset, both for cases where phenotypes are present or missing:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{BLUP }\OtherTok{\textless{}{-}} \FunctionTok{mixed.solve}\NormalTok{(temp,}\AttributeTok{K=}\FunctionTok{A.mat}\NormalTok{(NLRL[,}\DecValTok{5}\SpecialCharTok{:}\DecValTok{221}\NormalTok{]))}
\NormalTok{pred.BLUP}\OtherTok{\textless{}{-}}\FunctionTok{as.numeric}\NormalTok{(BLUP}\SpecialCharTok{$}\NormalTok{beta)}\SpecialCharTok{+}\NormalTok{BLUP}\SpecialCharTok{$}\NormalTok{u}
\end{Highlighting}
\end{Shaded}

How good a fit for the lines with missing phenotype:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(pred.BLUP[NLRL[,}\DecValTok{4}\NormalTok{]}\SpecialCharTok{\textgreater{}=}\DecValTok{1989}\NormalTok{],NLRL[NLRL[,}\DecValTok{4}\NormalTok{]}\SpecialCharTok{\textgreater{}=}\DecValTok{1989}\NormalTok{,}\DecValTok{3}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.2056729
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(pred.BLUP[NLRL[,}\DecValTok{4}\NormalTok{]}\SpecialCharTok{\textgreater{}=}\DecValTok{1989}\NormalTok{],NLRL[NLRL[,}\DecValTok{4}\NormalTok{]}\SpecialCharTok{\textgreater{}=}\DecValTok{1989}\NormalTok{,}\DecValTok{3}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-465-1.pdf}

How good a fit for the lines with phenotype:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(pred.BLUP[NLRL[,}\DecValTok{4}\NormalTok{]}\SpecialCharTok{\textless{}}\DecValTok{1989}\NormalTok{],NLRL[NLRL[,}\DecValTok{4}\NormalTok{]}\SpecialCharTok{\textless{}}\DecValTok{1989}\NormalTok{,}\DecValTok{3}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9411743
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(pred.BLUP[NLRL[,}\DecValTok{4}\NormalTok{]}\SpecialCharTok{\textless{}}\DecValTok{1989}\NormalTok{],NLRL[NLRL[,}\DecValTok{4}\NormalTok{]}\SpecialCharTok{\textless{}}\DecValTok{1989}\NormalTok{,}\DecValTok{3}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-466-1.pdf}

Try plotting over a specific range. This makes it easier to compare accuracy with age
difference from training population. For example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{upper}\OtherTok{\textless{}{-}}\DecValTok{20000}
\NormalTok{lower}\OtherTok{\textless{}{-}}\DecValTok{2000}

\FunctionTok{cor}\NormalTok{(pred.BLUP[NLRL[,}\DecValTok{4}\NormalTok{]}\SpecialCharTok{\textless{}}\NormalTok{upper }\SpecialCharTok{\&}\NormalTok{ NLRL[,}\DecValTok{4}\NormalTok{]}\SpecialCharTok{\textgreater{}}\NormalTok{lower],NLRL[NLRL[,}\DecValTok{4}\NormalTok{]}\SpecialCharTok{\textless{}}\NormalTok{upper }\SpecialCharTok{\&}
\NormalTok{NLRL[,}\DecValTok{4}\NormalTok{]}\SpecialCharTok{\textgreater{}}\NormalTok{lower,}\DecValTok{3}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.1606659
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(pred.BLUP[NLRL[,}\DecValTok{4}\NormalTok{]}\SpecialCharTok{\textless{}}\NormalTok{upper }\SpecialCharTok{\&}\NormalTok{ NLRL[,}\DecValTok{4}\NormalTok{]}\SpecialCharTok{\textgreater{}}\NormalTok{lower],NLRL[NLRL[,}\DecValTok{4}\NormalTok{]}\SpecialCharTok{\textless{}}\NormalTok{upper }\SpecialCharTok{\&}
\NormalTok{NLRL[,}\DecValTok{4}\NormalTok{]}\SpecialCharTok{\textgreater{}}\NormalTok{lower,}\DecValTok{3}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-467-1.pdf}

Alter \texttt{upper} and \texttt{lower} to compare accuracies in different ranges of the test population.
Can you explain the results? We shall discuss them!

\hypertarget{Breeding-simulations}{%
\chapter{Breeding simulations.}\label{Breeding-simulations}}

\backmatter

  \bibliography{book.bib,packages.bib,journals.bib}

\printindex

\end{document}
